{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aca613c",
   "metadata": {},
   "source": [
    "#### Monday, May 27, 2024\n",
    "\n",
    "mamba activate mistral\n",
    "\n",
    "https://github.com/mistralai/mistral-inference/blob/main/tutorials/classifier.ipynb\n",
    "\n",
    "Yes, this notebook was renamed here to 'Train_a_classifier_with_Mistral_7Bclassifier.ipynb'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eca8ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f47fc4b-7cdc-48ce-b42c-e0b73d902ece",
   "metadata": {},
   "source": [
    "# Train a classifier with Mistral 7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd6bdfb-c8cc-4fb6-8969-153a97dfb576",
   "metadata": {},
   "source": [
    "In this tutorial, we will see how we can leverage Mistral 7B to train a classifier. We need:\n",
    "- The mistral codebase: `https://github.com/mistralai/mistral-inference`\n",
    "- The pretrained model and its tokenizer: `https://docs.mistral.ai/llm/mistral-v0.1`\n",
    "- A dataset to train our classifier\n",
    "\n",
    "We will train and evaluate the classifier on `Symptom2Disease`, a public classification dataset from Kaggle provided in a CSV format: https://www.kaggle.com/datasets/niyarrbarman/symptom2disease/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f88da-96ab-4262-88f8-8f1d6d5224bd",
   "metadata": {},
   "source": [
    "## Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebe4bc9b-d0f7-42a6-9e41-bb3d042e0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# code_path = \"/codebase_path/mistral-inference\"  # codebase\n",
    "# data_path = Path(\"/dataset_path/Symptom2Disease.csv\")  # dataset downloaded from Kaggle\n",
    "# model_path = Path(\"/model_path/\")  # model and tokenizer location\n",
    "\n",
    "code_path = \"/home/rob/Data/Documents/Github/mistralai/mistral-inference\"  # codebase\n",
    "data_path = Path(\"data/Symptom2Disease.csv\")  # dataset downloaded from Kaggle\n",
    "model_path = Path(\"/home/rob/mistral_models/7B-Instruct-v0.3/\")  # model and tokenizer location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcc420b-2ac7-42e1-a3b1-8a80f87ccc10",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e8227fe-d2fb-4a2f-a136-96ee2d32287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(code_path)  # append the path where mistral-inference was cloned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86a3e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mistral.model import Transformer ... why was this wrong??\n",
    "from mistral_inference.model import Transformer\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9c8408-1bc1-4e79-8b6d-391e07d78eb4",
   "metadata": {},
   "source": [
    "## Load the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd5c2705",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer.from_folder(model_path, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "806ff308-5909-46f7-b10e-8b1f1c7b7edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mistral_tokenizer = MistralTokenizer.from_file(str(model_path / \"tokenizer.model\"))\n",
    "mistral_tokenizer = MistralTokenizer.from_file(str(model_path / \"tokenizer.model.v3\"))\n",
    "tokenizer = mistral_tokenizer.instruct_tokenizer.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9aa34b-cbe3-49a6-948b-17f6c84d93c6",
   "metadata": {},
   "source": [
    "The tokenizer is a critical component of the model that segments input text into word-pieces.\n",
    "\n",
    "For instance, the sentence `\"Roasted barramundi fish\"` is encoded as `['▁Ro', 'asted', '▁barr', 'am', 'und', 'i', '▁fish']`.\n",
    "\n",
    "The number of subwords in the model is set to `32000`, and each word is decomposed into known word pieces to avoid unknown words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0e03ef-439f-4078-a848-d69bf4e339ec",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d1f696-138d-467d-bb53-032e08d47bf1",
   "metadata": {},
   "source": [
    "We reload the Kaggle dataset from the disk. Each sample is composed of a sentence and a label. The dataset contains 24 different labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc2bf18b-d99c-4c31-9988-1675d019e04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded 1200 samples with 24 labels.\n"
     ]
    }
   ],
   "source": [
    "data = []  # list of (text, label)\n",
    "with open(data_path, newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i == 0:  # skip csv header\n",
    "            continue\n",
    "        data.append((row[2], row[1]))\n",
    "\n",
    "# label text to label ID\n",
    "labels = sorted({x[1] for x in data})\n",
    "txt_to_label = {x: i for i, x in enumerate(labels)}\n",
    "print(f\"Reloaded {len(data)} samples with {len(labels)} labels.\")\n",
    "\n",
    "# integer class for each datapoint\n",
    "data_class = [txt_to_label[x[1]] for x in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5434265-fad8-4daa-b54b-438d420bcb4d",
   "metadata": {},
   "source": [
    "The task is to classify a symptom, for instance `\"I have a dry cough that never stops.\"` to one of the 24 disease labels (`Acne, Arthritis, Bronchial Asthma, Cervical spondylosis, Chicken pox, Common Cold, etc.`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1540b537-c3cc-4851-80e3-91a9b9e8cc7c",
   "metadata": {},
   "source": [
    "## Embed data points in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73d5d75-55eb-4210-8ffe-d0db3e9f8735",
   "metadata": {},
   "source": [
    "We will now learn a linear classifier on frozen features provided by Mistral 7B.\n",
    "In particular, each sentence in the dataset will be tokenized, and provided to the model.\n",
    "\n",
    "If the input sentence is composed of `N` tokens, the model output will be a list of `N` vectors of dimension `d=4096`, where `d` is the dimensionality of the model.\n",
    "These vectors are then averaged along the dimension 0 to get a vector of size `d`.\n",
    "\n",
    "Finally, the vectors of all sentences in the dataset are concatenated into a matrix of shape `(D, d)` where `D` is the number of samples in the dataset (in particular, D=1200)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78019021-01d7-499f-94ea-a3c88bbcb42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1200it [00:28, 42.68it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    featurized_x = []\n",
    "    # compute an embedding for each sentence\n",
    "    for i, (x, y) in tqdm.tqdm(enumerate(data)):\n",
    "        # tokens = tokenizer.encode(x, bos=True)\n",
    "        tokens = tokenizer.encode(x, bos=True, eos=True)\n",
    "        tensor = torch.tensor(tokens).to(model.device)\n",
    "        features = model.forward_partial(tensor, [len(tokens)])  # (n_tokens, model_dim)\n",
    "        featurized_x.append(features.float().mean(0).cpu().detach().numpy())\n",
    "\n",
    "# concatenate sentence embeddings\n",
    "X = np.concatenate([x[None] for x in featurized_x], axis=0)  # (n_points, model_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a997101-7844-4928-bdf4-d7a3051eca75",
   "metadata": {},
   "source": [
    "## Plot t-SNE embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bbc6b4-dfc5-42e0-b99b-1ed281d929cb",
   "metadata": {},
   "source": [
    "Now that we have one vector/embedding for each sample in our dataset, we can visualize them using t-SNE.\n",
    "t-SNE is a powerful tool for reducing the dimensionality of high-dimensional data, while preserving the underlying structure and relationships between the data points, which can help uncover patterns and insights that would be difficult to discern in the high-dimensional space.\n",
    "\n",
    "In the graph below, we assign a different color to each class in the dataset. It is apparent that several distinct clusters are visible in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e3cd894-ac30-4f34-a4c8-ef34293b587d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rainbow\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# This is a sanity check that our data clusters nicely\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# map our feature space to 2D, plot it and color it by its labels\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.cm import rainbow\n",
    "\n",
    "# This is a sanity check that our data clusters nicely\n",
    "# map our feature space to 2D, plot it and color it by its labels\n",
    "\n",
    "reduced = sklearn.manifold.TSNE(n_components=2, random_state=0).fit_transform(X)\n",
    "plt.scatter(reduced[:, 0], reduced[:, 1], c=data_class, cmap='rainbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a6b699-b006-4ea5-8bcd-91dd69405b1b",
   "metadata": {},
   "source": [
    "## Create a train / test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdd2624-0f99-450a-93e5-55c009c7e535",
   "metadata": {},
   "source": [
    "For the sake of demonstration, we will shuffle the dataset and split it into two train and test splits composed of 80% and 20% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a9b3f4-a564-49f4-ba8a-fde55c636fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set : 960 samples\n",
      "Test set  : 240 samples\n"
     ]
    }
   ],
   "source": [
    "# make things reproducible\n",
    "rng = np.random.default_rng(seed=0)\n",
    "\n",
    "# shuffle the data\n",
    "permuted = rng.permutation(len(X))\n",
    "shuffled_x, shuffled_class = X[permuted], np.array(data_class)[permuted]\n",
    "\n",
    "# create a train / test split\n",
    "train_prop = 0.8\n",
    "n_train = int(len(shuffled_x) * 0.8)\n",
    "train_x, train_y = shuffled_x[:n_train], shuffled_class[:n_train]\n",
    "test_x, test_y = shuffled_x[n_train:], shuffled_class[n_train:]\n",
    "\n",
    "# summary\n",
    "print(f\"Train set : {len(train_x)} samples\")\n",
    "print(f\"Test set  : {len(test_x)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25720eb-e499-43b6-a19a-b82b100af32b",
   "metadata": {},
   "source": [
    "## Normalize features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecee2d7-5f23-49df-8c6a-066c38ef01bf",
   "metadata": {},
   "source": [
    "It is usually recommended to normalize features to improve the performance and stability of the classifier.\n",
    "In particular, we ensure that each feature in the training set has a mean of 0 and a standard deviation of 1.\n",
    "This can be done with [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) of Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a28bbe9b-0be4-4e68-b9ed-977fb20f29e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_x = scaler.fit_transform(train_x)\n",
    "test_x = scaler.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91116a77-b188-4c1c-95ac-e6e8872308fb",
   "metadata": {},
   "source": [
    "## Train a classifier and compute the test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad69a0bd-e57d-45e6-80da-2a49a64fe9a3",
   "metadata": {},
   "source": [
    "We can now train the classifier. We will use the [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) object of Scikit-learn.\n",
    "\n",
    "The default learning alborithm is `lbfgs`, which should give an accuracy of 98.33% for this particular train/test split.\n",
    "\n",
    "You can try different algorithms, and hyper-parameters. In a real-life scenario, it is possible to obtain better results by using k-fold cross-validation methods to validate accuracy and choice of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b496a585-018b-471e-8b26-4bd1fc0cd0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 98.33%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# For a real problem, C should be properly cross validated and the confusion matrix analyzed\n",
    "clf = LogisticRegression(random_state=0, C=1.0, max_iter=500).fit(train_x, train_y)  # 98.33%\n",
    "\n",
    "# you can also try the sag algorithm:\n",
    "# clf = LogisticRegression(random_state=0, C=1.0, max_iter=1000, solver='sag').fit(train_x, train_y)\n",
    "\n",
    "print(f\"Precision: {100*np.mean(clf.predict(test_x) == test_y):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9a2ab7-c354-4273-8f07-76687b10f7e2",
   "metadata": {},
   "source": [
    "## Classify a single example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c0c8ed-e56a-46fb-9754-b48bba907914",
   "metadata": {},
   "source": [
    "Below is an example showing how to classify new samples, and how to print the probabilities of the top-5 predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5e2cb04-2326-46d9-b080-20b464d6ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def classify(s: str):\n",
    "    tokens = tokenizer.encode(s, bos=True)\n",
    "    tensor = torch.tensor(tokens).to(model.device)\n",
    "    features = model.forward_partial(tensor, [len(tokens)])  # (n_tokens, model_dim)\n",
    "    embedding = features.float().mean(0).cpu().detach().numpy()\n",
    "    probas = clf.predict_proba(embedding[None])\n",
    "    assert probas.shape == (1, len(labels))\n",
    "    return probas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41246dbf-ea83-4140-bb57-510c958c26df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99.65%  Migraine\n",
      "  0.20%  Dengue\n",
      "  0.08%  Cervical spondylosis\n",
      "  0.05%  Jaundice\n",
      "  0.02%  allergy\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I have been feeling excessively hungry, even after eating, and have had a stiff neck.\"\n",
    "probas = classify(sentence)\n",
    "\n",
    "for i in np.argsort(probas)[::-1][:5]:\n",
    "    print(f\"{100*probas[i]:6.2f}%  {labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51284fe5-da9b-4176-9468-72d5041e0ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52e440c1-d377-4580-bd6b-84ee1e5686f1",
   "metadata": {},
   "source": [
    "## Second implementation: Zero shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a454b5ae-f68e-4f0d-9274-7f401420a882",
   "metadata": {},
   "source": [
    "Below is another method to do classification which does not require any training set.\n",
    "It also does not require to train a classifier on top of Mistral.\n",
    "However, it is much slower to predict, and it is not expected to work as well as the previous method in the case where you have a training dataset available.\n",
    "\n",
    "The method consists in prompting the model with the following text:\n",
    "\n",
    "```\n",
    "Symptoms: {symptom}\n",
    "Disease:\n",
    "```\n",
    "\n",
    "and to evaluate the probability that the model predicts each of the possible labels.\n",
    "The method is quite slower, because you need to compute the probability of all labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12546a21-6b0c-4df2-800b-43cb0b753e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240it [00:14, 16.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 30.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.args.max_batch_size = len(labels)\n",
    "\n",
    "test_data = [data[i] for i in permuted[n_train:]]\n",
    "\n",
    "good = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for i, (symptom, disease) in tqdm.tqdm(enumerate(test_data)):\n",
    "    \n",
    "        # given an input, predict the probability of each output (one output corresponds to one label)\n",
    "        input_ids, seq_lengths, output_lengths = [], [], []\n",
    "        for label in labels:\n",
    "            prefix = f\"Symptoms: {symptom}\\nDisease:\"\n",
    "            tokens = tokenizer.encode(f\"{prefix} {label}\", bos=True)\n",
    "            input_ids.extend(tokens)\n",
    "            seq_lengths.append(len(tokens))\n",
    "            output_lengths.append(len(tokens) - len(tokenizer.encode(prefix, bos=True)))\n",
    "        tensor = torch.tensor(input_ids).to(model.device)\n",
    "        logprobs = torch.log_softmax(model.forward(tensor, seq_lengths), dim=-1)  # sum_slens * vocab_size\n",
    "        \n",
    "        # select logprobs for each output\n",
    "        offset = 0\n",
    "        avg_logprobs = []\n",
    "        for slen, output_len in zip(seq_lengths, output_lengths):\n",
    "            output_tokens = input_ids[offset + slen - output_len:offset + slen]\n",
    "            output_logprobs = torch.gather(\n",
    "                logprobs[offset + slen - output_len - 1:offset + slen - 1],\n",
    "                dim=1,\n",
    "                index=torch.tensor(output_tokens).to(model.device)[:, None],\n",
    "            ).mean().item()\n",
    "            avg_logprobs.append(output_logprobs)\n",
    "            offset += slen\n",
    "\n",
    "        good.append(np.argmax(avg_logprobs) == txt_to_label[disease])\n",
    "    \n",
    "print(f\"Accuracy: {100 * np.mean(good):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4157a9-236d-4ef5-a02d-0731ae8e89eb",
   "metadata": {},
   "source": [
    "Even though this approach did not require any training set, the performance is only of 30.4%, compared to 98.3% for the previous, faster one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a42ece8-c879-414c-a21a-25843f7d2bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
