{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuesday, November 5, 2024\n",
    "\n",
    "mamba activate langchain-chroma\n",
    "\n",
    "[Build a Local RAG Application](https://python.langchain.com/docs/tutorials/local_rag/)\n",
    "\n",
    "This notebook uses Ollama to serve up Llama 3.1 8b model.\n",
    "\n",
    "This all runs in one pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "local_embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=local_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    # model=\"llama3.1:8b\",\n",
    "       model=\"llama3.1:latest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**The scene is set: a dark, crowded comedy club. The crowd is rowdy and ready for the ultimate showdown in satirical wit. In the blue corner, we have Stephen Colbert, aka \"The O'Reilly Factor's\" own \"Truthy\" Truth-teller. In the red corner, John Oliver, the lovable and scathing host of Last Week Tonight. The battle begins!**\n",
      "\n",
      "**Round 1: Stephen Colbert**\n",
      "\n",
      "(Stephen takes the stage, adjusting his tie)\n",
      "\n",
      "Yo, I'm Stephen Colbert, and I'm here to say\n",
      "My facts are straight, in a world gone astray\n",
      "I report on the news, with a grin so wide\n",
      "While you're over here, sipping tea, and letting satire slide\n",
      "\n",
      "(My apologies, John, but your show's just not as bright\n",
      "As mine, where we fact-check, day and night)\n",
      "\n",
      "**Round 1: John Oliver**\n",
      "\n",
      "(John storms onto the stage, mic in hand)\n",
      "\n",
      "Hold up, Stephen, let me set you straight\n",
      "Your 'facts' are cherry-picked, with a partisan weight\n",
      "You're a master of spin, but I'm not impressed\n",
      "My show's got substance, and your facts are depressed\n",
      "\n",
      "(I fact-check too, with a fire that's true blue)\n",
      "While you're busy making up numbers, I'll stick to what I do: telling truth to power, in all its guile and might, boo!\n",
      "\n",
      "**Round 2: Stephen Colbert**\n",
      "\n",
      "(Stephen smirks, enjoying the exchange)\n",
      "\n",
      "Oh, John, you think you're bold\n",
      "But your jokes are stale, like a beer that's old\n",
      "My show's got humor, with a twist of glee\n",
      "While you're over here, trying to be serious, but it's just not me\n",
      "\n",
      "(I'm the king of satirical spin, you know the deal)\n",
      "You may have facts on your side, but I've got style and steel\n",
      "\n",
      "**Round 2: John Oliver**\n",
      "\n",
      "(John grins, unfazed)\n",
      "\n",
      "Style? Steel? You mean like a rusty old gate?\n",
      "My show's got heart, and a will to participate\n",
      "In the conversation, where ideas truly collide\n",
      "While you're stuck in your bubble, with an echo chamber inside\n",
      "\n",
      "(I'm not just funny, I'm smart, and that's a fact)\n",
      "You may have ratings, but I've got respect from the pack!\n",
      "\n",
      "**Round 3: Stephen Colbert**\n",
      "\n",
      "(Stephen raises his voice)\n",
      "\n",
      "Respect? Ha! You think you deserve it?\n",
      "From the likes of me, who paved the satirical merit?\n",
      "I was here first, with a truth-teller's grin\n",
      "While you were still drinking tea, and wondering how to fit in!\n",
      "\n",
      "**Round 3: John Oliver**\n",
      "\n",
      "(John takes a step forward)\n",
      "\n",
      "You may have been first, but I'm the one who's next\n",
      "In a world that needs satire, with a voice that's unpossessed\n",
      "I'll keep pushing boundaries, with jokes that sting\n",
      "While you're stuck in your comfort zone, with an audience that clings!\n",
      "\n",
      "**The crowd goes wild! The judges deliberate. In the end... it's a tie!**\n",
      "\n",
      "(The two comedians shake hands, grinning at each other)\n",
      "\n",
      "Stephen Colbert: \"You win this battle, John, but I'll be back!\"\n",
      "\n",
      "John Oliver: \"Game on, Stephen. It's going to be a long war!\"\n",
      "\n",
      "(Both walk off stage, ready for the next round in the epic satirical showdown)\n"
     ]
    }
   ],
   "source": [
    "response_message = model.invoke(\n",
    "    \"Simulate a rap battle between Stephen Colbert and John Oliver\"\n",
    ")\n",
    "\n",
    "print(response_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using in a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main themes in these retrieved documents are:\\n\\n1. **Task Decomposition**: Breaking down complex tasks into smaller, manageable subgoals using:\\n\\t* Large Language Models (LLMs) with simple prompting\\n\\t* Task-specific instructions\\n\\t* Human inputs\\n2. **Planning and Execution**: A system for autonomous agents that involves:\\n\\t* Planning: decomposing tasks into subgoals and planning ahead\\n\\t* Task execution: expert models executing specific tasks and logging results\\n3. **Autonomous Agent System**: A system overview (Fig. 1) that includes components such as:\\n\\t* Component One: Planning, which involves task decomposition and reflection/refinement\\n\\nThese themes are related to the development of autonomous agents that can perform complex tasks efficiently and learn from their experiences.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    ")\n",
    "\n",
    "\n",
    "# Convert loaded documents into strings by concatenating their content\n",
    "# and ignoring metadata\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "chain = {\"docs\": format_docs} | prompt | model | StrOutputParser()\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "\n",
    "chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are three approaches to task decomposition: (1) using simple prompting in a Large Language Model (LLM), (2) employing task-specific instructions, and (3) utilizing human inputs. This breakdown enables efficient handling of complex tasks by breaking them down into smaller, manageable subgoals. It facilitates planning ahead and improves the quality of final results through self-reflection and refinement.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(context=lambda input: format_docs(input[\"context\"]))\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "\n",
    "# Run\n",
    "chain.invoke({\"context\": docs, \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q&A with retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition can be done in three ways: (1) by Large Language Model (LLM) with simple prompting, (2) using task-specific instructions, and (3) with human inputs. This process breaks down large tasks into smaller, manageable subgoals for efficient handling of complex tasks. It enables an agent to plan ahead and handle tasks more effectively.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "qa_chain.invoke(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-chroma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
