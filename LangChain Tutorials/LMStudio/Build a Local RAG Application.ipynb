{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuesday, November 5, 2024\n",
    "\n",
    "mamba activate langchain-chroma\n",
    "\n",
    "[Build a Local RAG Application](https://python.langchain.com/docs/tutorials/local_rag/)\n",
    "\n",
    "Gonna try using LMStudio and HuggingFace embeddings.\n",
    "\n",
    "Nice! This all runs in one pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/miniforge3/envs/langchain-chroma/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "# model_kwargs = {'device': 'cpu'}\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hfEmbeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# local_embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# vectorstore = Chroma.from_documents(documents=all_splits, embedding=local_embeddings)\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=hfEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_ollama import ChatOllama\n",
    "\n",
    "# model = ChatOllama(\n",
    "#     # model=\"llama3.1:8b\",\n",
    "#        model=\"llama3.1:latest\",\n",
    "#        #model=\"llama3.2:latest\",\n",
    "# )\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(base_url=\"http://localhost:1234/v1\", \n",
    "                   # model = \"hermes-3-llama-3.1-8b\",  # do not pass in an unrecognized model name ... \n",
    "                   api_key=\"lm-studio\", \n",
    "                   temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Stephen Colbert steps up to the mic, grinning mischievously*\n",
      "\n",
      "Yo John, what's up? Stephen Colbert in the house, \n",
      "You think you're funny, but your jokes are a mouse,\n",
      "Of course you've got that British charm,\n",
      "But I'm here to show you how this game is played from the start!\n",
      "\n",
      "*John Oliver takes his turn, smirking confidently*\n",
      "\n",
      "Stephen, my man, you've got some skills,\n",
      "But I've been around the block and know how to kill,\n",
      "Your humor's clever, but it's all a bit too precious,\n",
      "I'll take that microphone and show you what real is!\n",
      "\n",
      "*Colbert comes back even stronger, not missing a beat*\n",
      "\n",
      "Oh John, you think you can best me?\n",
      "You're just another talking head, as far as I see,\n",
      "But I'll give you this - your wit's sharp like a knife,\n",
      "Now let's see if you can keep up with my rhymes and life!\n",
      "\n",
      "*Oliver grins, clearly enjoying the back-and-forth*\n",
      "\n",
      "Stephen, you're quick, but I'm quicker,\n",
      "I've got that British bite that will make you shudder,\n",
      "You may have Late Night, but I've got Last Week Tonight,\n",
      "And I'll use it to show you why your jokes are just second-rate!\n",
      "\n",
      "*Colbert's grin widens as he sees Oliver is holding his own*\n",
      "\n",
      "John, you're putting up a good fight,\n",
      "But I'm the king of satire, and that's my right,\n",
      "I may be a puppet, but my strings are strong,\n",
      "And I'll use them to make you realize your show's just not as wrong!\n",
      "\n",
      "*Oliver chuckles, clearly impressed by Colbert's persistence*\n",
      "\n",
      "Stephen, you're persistent, I'll give you that,\n",
      "But in the end, it's all about who can deliver that satirical fact,\n",
      "I may be new to the game, but I'm ready to play,\n",
      "And I'll show you why John Oliver is here to stay!\n",
      "\n",
      "*The crowd erupts as both men bow, laughing and clapping*\n",
      "\"Thank you ladies and gentlemen! That was fun!\"\n"
     ]
    }
   ],
   "source": [
    "response_message = model.invoke(\n",
    "    \"Simulate a rap battle between Stephen Colbert and John Oliver\"\n",
    ")\n",
    "\n",
    "print(response_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using in a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main themes in these retrieved documents are:\\n\\n1. Task decomposition methods:\\n   - Using simple prompting like \"Steps for XYZ.\\\\n1.\"\\n   - Asking \"What are the subgoals for achieving XYZ?\"\\n   - Employing task-specific instructions, such as \"Write a story outline.\" for writing a novel\\n   - Incorporating human inputs\\n\\n2. Challenges in long-term planning and task decomposition:\\n   - Planning over a lengthy history is difficult\\n   - Effectively exploring the solution space remains challenging\\n   - LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error\\n\\n3. Planning process:\\n   - Breaking down large tasks into smaller, manageable subgoals for efficient handling of complex tasks\\n   - Engaging in self-criticism and self-reflection over past actions\\n   - Learning from mistakes and refining them for future steps to improve the quality of final results\\n\\n4. Memory and task execution:\\n   - Expert models execute specific tasks and log results\\n   - Instructions are provided for task execution'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    ")\n",
    "\n",
    "\n",
    "# Convert loaded documents into strings by concatenating their content\n",
    "# and ignoring metadata\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "chain = {\"docs\": format_docs} | prompt | model | StrOutputParser()\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "\n",
    "chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(context=lambda input: format_docs(input[\"context\"]))\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "\n",
    "# Run\n",
    "chain.invoke({\"context\": docs, \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q&A with retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "qa_chain.invoke(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-chroma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
