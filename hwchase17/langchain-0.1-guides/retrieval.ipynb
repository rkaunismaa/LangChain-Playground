{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9084d15",
   "metadata": {},
   "source": [
    "#### Friday, April 19, 2024\n",
    "\n",
    "mamba activate langchain3\n",
    "\n",
    "Local model will be [lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF](https://huggingface.co/lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF)\n",
    "\n",
    "Switched to [QuantFactory/Meta-Llama-3-8B-Instruct-GGUF](https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF)\n",
    "\n",
    "This all runs locally. Ran each invoke method 3 times and saved the output to show the differences in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "347dca51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"I'm a chatbot, oh so fine,\\nCreated to assist, all the time!\\nMy name is LLaMA, nice to meet you too,\\nI'll respond in rhymes, that's what I love to do!\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Example: reuse your existing OpenAI setup\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f0e4a",
   "metadata": {},
   "source": [
    "Another LLama 3 8B Instruct quantized model. [QuantFactory/Meta-Llama-3-8B-Instruct-GGUF](https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5550614b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Nice to meet you, don't be shy,\\nI'm an AI assistant, flying high!\\nI'll help with tasks and answers provide,\\nIn rhyming form, I'll be your guide!\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Example: reuse your existing OpenAI setup\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"QuantFactory/Meta-Llama-3-8B-Instruct-GGUF\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c865a-9f50-417d-a0cf-4c430571d104",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "This notebook covers basic walkthrough of retrieval functionality in LangChain. For more information, see:\n",
    "\n",
    "- [Retrieval Documentation](https://python.langchain.com/docs/modules/data_connection/)\n",
    "\n",
    "- [Advanced Retrieval Types](https://python.langchain.com/docs/modules/data_connection/retrievers/)\n",
    "\n",
    "- [QA with RAG Use Case Documentation](https://python.langchain.com/docs/use_cases/question_answering/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b594de",
   "metadata": {},
   "source": [
    "## Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16bda46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb65747c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aece40e",
   "metadata": {},
   "source": [
    "## Split documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e749d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "006a257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27c64930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82a1f0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Getting started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'Introduction', 'language': 'en'})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792485da",
   "metadata": {},
   "source": [
    "## Index Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e357193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceb827d",
   "metadata": {},
   "source": [
    "We want to use [HuggingFace embeddings](https://python.langchain.com/docs/integrations/text_embedding/huggingfacehub/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32b29daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d85d1045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = OpenAIEmbeddings()\n",
    "#embeddings = OpenAIEmbeddings(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "289e53d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7d2450",
   "metadata": {},
   "source": [
    "## Query Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "231c74bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ef3d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOpenAI()\n",
    "llm = ChatOpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85a66449",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ff74f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "588f5818",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93cfd3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, LangSmith can help with testing by providing a platform for building production-grade LLM applications and closely monitoring and evaluating them. This allows developers to ship their applications quickly and with confidence.\n",
      "\n",
      "LangSmith provides features such as tracing, evaluation, and prompt management, which can be used to test and grade the performance of an AI system. For example, LangSmith's built-in accuracy evaluator can be used to evaluate the results of an AI system against a set of test cases, providing feedback on its performance.\n",
      "\n",
      "Additionally, LangSmith allows developers to create custom evaluators using JavaScript or Python code, which can be used to implement complex testing scenarios and evaluation criteria.\n"
     ]
    }
   ],
   "source": [
    "# the call to the llm is made with the invoke method.\n",
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])\n",
    "\n",
    "# *** Run 1 ***\n",
    "# Based on the provided context, LangSmith helps with testing by providing a platform for building production-grade LLM applications and allowing you to closely monitor and evaluate your application. It offers features such as tracing, evaluation, and prompt management.\n",
    "\n",
    "# In particular, LangSmith provides a built-in accuracy evaluator that can be used to grade the results of an AI system. This is demonstrated in the code examples provided, where an `exact_match` evaluator is defined to score the output of the AI system against expected outputs.\n",
    "\n",
    "# Additionally, LangSmith allows you to create datasets and examples, which can be used as test cases for evaluating your AI system's performance. The `runOnDataset` function can be used to run your AI system on a dataset and generate results, which can then be evaluated using custom evaluators like the one defined in the example.\n",
    "\n",
    "# Overall, LangSmith provides a range of tools and features that can help with testing and evaluating LLM applications, making it easier to develop and deploy high-quality AI systems.\n",
    "\n",
    "\n",
    "# *** Run 2 ***\n",
    "# Based on the provided context, LangSmith seems to provide features for building production-grade LLM applications and closely monitoring and evaluating them. Specifically, it appears that LangSmith helps with testing by:\n",
    "\n",
    "# 1. **Evaluating results**: LangSmith provides a built-in accuracy evaluator, as shown in the Python example, which allows you to test and grade the output of your AI system.\n",
    "# 2. **Creating datasets**: You can create sample datasets in LangSmith, which are used for testing and evaluating your AI system's performance.\n",
    "# 3. **Running experiments**: LangSmith allows you to run experiments with your AI system, using a dataset and evaluators to score the results.\n",
    "\n",
    "# Overall, LangSmith seems to provide a platform for building and testing LLM applications, making it easier to develop and deploy AI-powered solutions.\n",
    "\n",
    "\n",
    "# *** Run 3 ***\n",
    "# According to the provided context, LangSmith can help with testing by providing a platform for building production-grade LLM applications and closely monitoring and evaluating them. This allows developers to ship their applications quickly and with confidence.\n",
    "\n",
    "# LangSmith provides features such as tracing, evaluation, and prompt management, which can be used to test and grade the performance of an AI system. For example, LangSmith's built-in accuracy evaluator can be used to evaluate the results of an AI system against a set of test cases, providing feedback on its performance.\n",
    "\n",
    "# Additionally, LangSmith allows developers to create custom evaluators using JavaScript or Python code, which can be used to implement complex testing scenarios and evaluation criteria.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9ba679",
   "metadata": {},
   "source": [
    "## Advanced Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52d89b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import MultiQueryRetriever\n",
    "\n",
    "advanced_retriever = MultiQueryRetriever.from_llm(retriever=retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67347a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = create_retrieval_chain(advanced_retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "685ccfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, LangSmith can help with testing by providing a platform to closely monitor and evaluate LLM applications. It allows you to define datasets, create examples, and use evaluators to grade the results. This enables you to test your AI system and get feedback on its performance.\n",
      "\n",
      "In the example code, it is shown how to create a dataset, create examples, define an evaluator (in this case, `exact_match`), and then run the experiment using the `runOnDataset` function. The `evaluate` function is used to score the results based on the defined evaluators.\n",
      "\n",
      "LangSmith also provides features such as tracing, which allows you to log traces of your application's execution, and prompt hub, a tool for managing prompts in your LLM applications. These features can help with testing by providing insights into how your AI system is performing and identifying areas that need improvement.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])\n",
    "\n",
    "# *** Run 1 ***\n",
    "# Based on the provided context, LangSmith can help with testing by allowing you to:\n",
    "\n",
    "# 1. **Create a dataset**: You can create a sample dataset in LangSmith and define test cases for your AI system.\n",
    "# 2. **Evaluate results**: LangSmith provides built-in evaluators, such as accuracy evaluator, or allows you to define custom evaluators to grade the results of your AI system.\n",
    "# 3. **Run experiments**: You can run experiments on your AI system using LangSmith's `runOnDataset` method, which takes in a dataset and an AI system function as inputs.\n",
    "# 4. **Monitor and evaluate performance**: LangSmith provides tracing capabilities to monitor the performance of your AI system and evaluate its results.\n",
    "\n",
    "# In the provided example, you can see how to create a dataset, define a custom evaluator (`exact_match`), and run an experiment using `runOnDataset`. The result of the experiment is then evaluated using the defined evaluator.\n",
    "\n",
    "\n",
    "\n",
    "# *** Run 2 ***\n",
    "# According to the provided context, LangSmith helps with testing by providing a platform for building production-grade LLM applications and closely monitoring and evaluating them. It allows you to create datasets, define test cases (examples), and optionally use evaluators to grade the results.\n",
    "\n",
    "# For example, in the code snippet provided, it demonstrates how to:\n",
    "\n",
    "# 1. Create a dataset and add examples to it.\n",
    "# 2. Define an evaluator function (`exact_match`) that checks if the output of the AI system matches the expected output.\n",
    "# 3. Use the `evaluate` function to run the AI system on the dataset and score its performance using the defined evaluator.\n",
    "\n",
    "# This allows you to test and evaluate your LLM application's performance and make improvements before deploying it in production.\n",
    "\n",
    "\n",
    "\n",
    "# *** Run 3 ***\n",
    "# Based on the provided context, LangSmith can help with testing by providing a platform to closely monitor and evaluate LLM applications. It allows you to define datasets, create examples, and use evaluators to grade the results. This enables you to test your AI system and get feedback on its performance.\n",
    "\n",
    "# In the example code, it is shown how to create a dataset, create examples, define an evaluator (in this case, `exact_match`), and then run the experiment using the `runOnDataset` function. The `evaluate` function is used to score the results based on the defined evaluators.\n",
    "\n",
    "# LangSmith also provides features such as tracing, which allows you to log traces of your application's execution, and prompt hub, a tool for managing prompts in your LLM applications. These features can help with testing by providing insights into how your AI system is performing and identifying areas that need improvement.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e992b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
