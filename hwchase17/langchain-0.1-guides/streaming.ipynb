{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a938a4f8",
   "metadata": {},
   "source": [
    "#### Tuesday, April 16, 2024\n",
    "\n",
    "mamba activate langchain3\n",
    "\n",
    "This all runs in one pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85aa8e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"I'm a chatbot. My name is Emily, and I was created by Meta AI to help people communicate with each other. I can answer questions, provide information, and even engage in conversation. Is there anything specific you would like me to help you with?\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Example: reuse your existing OpenAI setup\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"TheBloke/NexusRaven-V2-13B-GGUF\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71512a6-fb0b-47b0-944c-7372a4808180",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "This notebook covers functionality related to streaming.\n",
    "\n",
    "For more information, see:\n",
    "\n",
    "- [Streaming with LCEL](https://python.langchain.com/docs/expression_language/interface#stream)\n",
    "\n",
    "- [Streaming for RAG](https://python.langchain.com/docs/use_cases/question_answering/streaming)\n",
    "\n",
    "- [Streaming for Agents](https://python.langchain.com/docs/modules/agents/how_to/streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e60b927",
   "metadata": {},
   "source": [
    "## Basic Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65dbf4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7c19988",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11039b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = ChatOpenAI()\n",
    "model = ChatOpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18d4063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b75c535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "'\n",
      "s\n",
      " a\n",
      " jo\n",
      "ke\n",
      " for\n",
      " you\n",
      ":\n",
      " Why\n",
      " did\n",
      " the\n",
      " bear\n",
      " go\n",
      " to\n",
      " the\n",
      " doctor\n",
      "?\n",
      " Because\n",
      " he\n",
      " was\n",
      " feeling\n",
      " r\n",
      "uff\n",
      "!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"topic\": \"bears\"}):\n",
    "    print(s)\n",
    "\n",
    "# 5.8s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdd3f60",
   "metadata": {},
   "source": [
    "## Streaming with RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3f8d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb387c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ChatOpenAI()\n",
    "model = ChatOpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c0067ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()\n",
    "chain1 = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9563d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Write me a poem about {topic}\")\n",
    "# model = ChatOpenAI()\n",
    "model = ChatOpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "output_parser = StrOutputParser()\n",
    "chain2 = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6520dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_chain = RunnableParallel({\n",
    "    \"joke\": chain1,\n",
    "    \"poem\": chain2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3322349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'poem': 'I'}\n",
      "{'poem': ' am'}\n",
      "{'poem': ' unable'}\n",
      "{'poem': ' to'}\n",
      "{'poem': ' generate'}\n",
      "{'poem': ' a'}\n",
      "{'poem': ' poem'}\n",
      "{'poem': ' about'}\n",
      "{'poem': ' be'}\n",
      "{'poem': 'ars'}\n",
      "{'poem': ' as'}\n",
      "{'poem': ' it'}\n",
      "{'poem': ' is'}\n",
      "{'poem': ' not'}\n",
      "{'poem': ' within'}\n",
      "{'poem': ' my'}\n",
      "{'poem': ' capabilities'}\n",
      "{'poem': '.'}\n",
      "{'poem': ' However'}\n",
      "{'poem': ','}\n",
      "{'poem': ' I'}\n",
      "{'poem': ' can'}\n",
      "{'poem': ' provide'}\n",
      "{'poem': ' you'}\n",
      "{'poem': ' with'}\n",
      "{'poem': ' information'}\n",
      "{'poem': ' on'}\n",
      "{'poem': ' how'}\n",
      "{'poem': ' to'}\n",
      "{'poem': ' write'}\n",
      "{'poem': ' a'}\n",
      "{'poem': ' poem'}\n",
      "{'poem': ' about'}\n",
      "{'poem': ' be'}\n",
      "{'poem': 'ars'}\n",
      "{'poem': ' if'}\n",
      "{'poem': ' you'}\n",
      "{'poem': ' would'}\n",
      "{'poem': ' like'}\n",
      "{'poem': '.<'}\n",
      "{'poem': '|'}\n",
      "{'poem': 'im'}\n",
      "{'poem': '_'}\n",
      "{'poem': 'end'}\n",
      "{'poem': '|'}\n",
      "{'poem': '>'}\n",
      "{'poem': ''}\n",
      "{'joke': 'Here'}\n",
      "{'joke': \"'\"}\n",
      "{'joke': 's'}\n",
      "{'joke': ' a'}\n",
      "{'joke': ' jo'}\n",
      "{'joke': 'ke'}\n",
      "{'joke': ' for'}\n",
      "{'joke': ' you'}\n",
      "{'joke': ':'}\n",
      "{'joke': ' Why'}\n",
      "{'joke': ' did'}\n",
      "{'joke': ' the'}\n",
      "{'joke': ' bear'}\n",
      "{'joke': ' go'}\n",
      "{'joke': ' to'}\n",
      "{'joke': ' the'}\n",
      "{'joke': ' doctor'}\n",
      "{'joke': '?'}\n",
      "{'joke': ' Because'}\n",
      "{'joke': ' it'}\n",
      "{'joke': ' had'}\n",
      "{'joke': ' a'}\n",
      "{'joke': ' hair'}\n",
      "{'joke': '-'}\n",
      "{'joke': 'ra'}\n",
      "{'joke': 'ising'}\n",
      "{'joke': ' accident'}\n",
      "{'joke': '!'}\n",
      "{'joke': ''}\n"
     ]
    }
   ],
   "source": [
    "for s in parallel_chain.stream({\"topic\": \"bears\"}):\n",
    "    print(s)\n",
    "\n",
    "# 8.4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d0a2fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joke': 'Ok'}\n",
      "{'joke': 'Okay'}\n",
      "{'joke': 'Okay,'}\n",
      "{'joke': 'Okay, here'}\n",
      "{'joke': \"Okay, here'\"}\n",
      "{'joke': \"Okay, here's\"}\n",
      "{'joke': \"Okay, here's a\"}\n",
      "{'joke': \"Okay, here's a jo\"}\n",
      "{'joke': \"Okay, here's a joke\"}\n",
      "{'joke': \"Okay, here's a joke for\"}\n",
      "{'joke': \"Okay, here's a joke for you\"}\n",
      "{'joke': \"Okay, here's a joke for you:\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor?\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling r\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': 'Here'}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here'\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about be\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\n\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nB\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears,\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh be\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears,\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\n\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nThe\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft,\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their ro\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\n\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat h\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\n\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with g\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\n\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they grow\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl,\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\n\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they'\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\n\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\nThey\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\nThey'\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\nThey'll\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\nThey'll attack\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\nThey'll attack if\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\nThey'll attack if you\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\nThey'll attack if you don\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\nThey'll attack if you don'\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\nThey'll attack if you don't\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\nThey'll attack if you don't show\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\nThey'll attack if you don't show respect\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\nThey'll attack if you don't show respect\\n\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\nThey'll attack if you don't show respect\\nSo\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\nThey'll attack if you don't show respect\\nSo be\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\nThey'll attack if you don't show respect\\nSo be careful\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\nThey'll attack if you don't show respect\\nSo be careful,\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\nThey'll attack if you don't show respect\\nSo be careful, be\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\nThey'll attack if you don't show respect\\nSo be careful, bears\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\nThey'll attack if you don't show respect\\nSo be careful, bears are\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\nThey'll attack if you don't show respect\\nSo be careful, bears are a\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\nThey'll attack if you don't show respect\\nSo be careful, bears are a threat\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\nThey'll attack if you don't show respect\\nSo be careful, bears are a threat\\n\"}\n",
      "{'joke': \"Okay, here's a joke for you: Why did the bear go to the doctor? Because he was feeling ruff!\", 'poem': \"Here's a poem about bears:\\n\\nBears, oh bears, so big and strong\\nTheir fur is soft, their roar is long\\nThey like to eat honey from the trees\\nAnd play in the forest with glee\\n\\nBut when they growl, their eyes turn red\\nAnd they're not afraid of being fed\\nThey'll attack if you don't show respect\\nSo be careful, bears are a threat\\n\"}\n"
     ]
    }
   ],
   "source": [
    "result = {}\n",
    "for s in parallel_chain.stream({\"topic\": \"bears\"}):\n",
    "    for k,v in s.items():\n",
    "        if k not in result:\n",
    "            result[k] = \"\"\n",
    "        result[k] += v\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4c4685",
   "metadata": {},
   "source": [
    "## Stream Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "643eb17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers.tavily_search_api import TavilySearchAPIRetriever\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6d472a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your api key\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "TAVILY_API_KEY = getpass(\"Enter your API key: \")\n",
    "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f021ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever= TavilySearchAPIRetriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based only on the context provided:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\"\"\")\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "retrieval_chain = RunnablePassthrough.assign(\n",
    "    context=(lambda x: x[\"question\"]) | retriever.with_config(run_name=\"Docs\")\n",
    ") | chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a672dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Call: load_tools(tool_apis=['serpapi'], llm='llm-math')<bot_end> \n",
      "Thought: The function call `load_tools(tool_apis=['serpapi'], llm='llm-math')` answers the question \"What is LangSmith?\" because it loads the tool API 'serpapi' and the LLM 'llm-math'.\n",
      "\n",
      "The docstring for the function explains that it initializes a chat model, loads specific tools, and creates an agent that can generate responses based on descriptions. The `tool_apis` parameter is used to specify which tool APIs should be loaded, and in this case, we are loading the 'serpapi' tool API.\n",
      "\n",
      "The `llm` parameter is used to specify which LLM should be loaded, and in this case, we are loading the 'llm-math' LLM. The docstring for the function also explains that it takes an llm instance as a parameter, which means that the function will use the specified LLM when generating responses.\n",
      "\n",
      "Therefore, the function call `load_tools(tool_apis=['serpapi'], llm='llm-math')` answers the question \"What is LangSmith?\" by loading the 'serpapi' tool API and the 'llm-math' LLM, which are both components of the LangSmith platform."
     ]
    }
   ],
   "source": [
    "for s in retrieval_chain.stream({\"question\": \"what is langsmith\"}):\n",
    "    print(s, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a8a4163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '',\n",
      "  'value': {'final_output': None,\n",
      "            'id': 'c6d5454f-aab5-4879-bae5-b2afa95bf180',\n",
      "            'logs': {},\n",
      "            'name': 'RunnableSequence',\n",
      "            'streamed_output': [],\n",
      "            'type': 'chain'}})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableAssign<context>',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': 'ad271571-8cc3-43b6-9265-0a0183c45ea0',\n",
      "            'metadata': {},\n",
      "            'name': 'RunnableAssign<context>',\n",
      "            'start_time': '2024-04-16T15:56:22.921+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['seq:step:1'],\n",
      "            'type': 'chain'}})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableAssign<context>/streamed_output/-',\n",
      "  'value': {'question': 'what is langsmith'}})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableParallel<context>',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': '3ea43d6d-04d6-4463-bfbf-88aa84777269',\n",
      "            'metadata': {},\n",
      "            'name': 'RunnableParallel<context>',\n",
      "            'start_time': '2024-04-16T15:56:22.924+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': [],\n",
      "            'type': 'chain'}})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableSequence',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': '9e0baf00-d61a-4cba-8974-2839c87a25ce',\n",
      "            'metadata': {},\n",
      "            'name': 'RunnableSequence',\n",
      "            'start_time': '2024-04-16T15:56:22.925+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['map:key:context'],\n",
      "            'type': 'chain'}})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableLambda',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': '7f890060-acd2-4403-8805-f032224918d2',\n",
      "            'metadata': {},\n",
      "            'name': 'RunnableLambda',\n",
      "            'start_time': '2024-04-16T15:56:22.926+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['seq:step:1'],\n",
      "            'type': 'chain'}})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableLambda/streamed_output/-',\n",
      "  'value': 'what is langsmith'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableLambda/final_output',\n",
      "  'value': {'output': 'what is langsmith'}},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/RunnableLambda/end_time',\n",
      "  'value': '2024-04-16T15:56:22.927+00:00'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/Docs',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': '0dca8f26-234b-479e-a9b5-5a393309f355',\n",
      "            'metadata': {},\n",
      "            'name': 'Docs',\n",
      "            'start_time': '2024-04-16T15:56:22.927+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['seq:step:2'],\n",
      "            'type': 'retriever'}})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/Docs/final_output',\n",
      "  'value': {'documents': [Document(page_content='How it Works, Use Cases, Alternatives & More\\nRichie Cotton\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAdel Nehme\\n32 min\\nAn Introductory Guide to Fine-Tuning LLMs\\nJosep Ferrer\\n12 min\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nBex Tuychiev\\n15 min\\nGrow your data skills with DataCamp for Mobile\\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\\n For labeled datasets like the CSV dataset we uploaded, LangSmith offers more comprehensive evaluators for measuring the correctness of the response to a prompt:\\nLet’s try the last one on our examples:\\nCoTQA criterion returns a score called Contextual accuracy, as depicted in the GIF below (also in the UI):\\nPlease visit the LangChain evaluators section of LangSmith docs to learn much more about evaluators.\\n Once the upload finishes, the dataset will appear in the UI:\\nLet’s run our custom criterion from the previous section on this dataset as well:\\nIf you go to the dataset page and check out the run, we can see the average scores for each custom criteria:\\nEvaluating Labeled Datasets\\nBuilt-in and custom evaluators written in natural language are mostly for unlabeled datasets. We will use the create_dataset function of the client:\\nNow, let’s add three inputs that each ask the LLM to create a single flashcard:\\nIf you go over the dataset tab of the UI, you will see each prompt listed with NULL output:\\nNow, let’s run all the prompts in a single line of code using run_on_dataset function:\\n How it Works, Use Cases, Alternatives & More\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAn Introductory Guide to Fine-Tuning LLMs\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nStart Your Langchain Journey Today\\nCourse\\nDeveloping LLM Applications with LangChain\\nCourse\\nLarge Language Models (LLMs)', metadata={'title': 'An Introduction to Debugging And Testing LLMs in LangSmith', 'source': 'https://www.datacamp.com/tutorial/introduction-to-langsmith', 'score': 0.96075, 'images': None}),\n",
      "                          Document(page_content='This function helps to load the specific language models and tools required for the task as shown in the code snippet below:\\nAs a next step, initialize an agent by calling the initialize_agent function with several parameters like tools, llms, and agent:\\nThe verbose parameter is set to false, indicating that the agent will not provide verbose or detailed output.\\n You can accomplish this by following the shell commands provided below:\\nCreating a LangSmith client\\nNext, create a LangSmith client to interact with the API:\\nIf you’re using Python, run the following commands to import the module:\\n This code also handles exceptions that may occur during the agent execution:\\nIt’s also important to call the wait_for_all_tracers function from the langchain.callbacks.tracers.langchain module as shown in the code snippet below:\\nCalling the wait_for_all_tracers function helps ensure that logs and traces are submitted in full before the program proceeds. The temperature parameter will be set to 0, implying that the generated response will be more deterministic as shown in the code snippet below:\\nNow, let’s call the load_tools function with a list of tool APIs, such as serpapi and llm-math, and also take the llm instance as a parameter. It initializes a chat model, loads specific tools, and creates an agent that can generate responses based on descriptions:\\nInput processing with exception handling\\nThe code below defines a list of input examples using the asyncio library to asynchronously run the agent on each input and gather the results for further processing.', metadata={'title': 'Using LangSmith to test LLMs and AI applications', 'source': 'https://blog.logrocket.com/langsmith-test-llms-ai-applications/', 'score': 0.95984, 'images': None}),\n",
      "                          Document(page_content=\"BETA Sign Up\\nContact Sales\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter 🦜🔗 LangChain\\nLangSmith\\nLangServe\\nAgents\\nRetrieval\\nEvaluation\\nBlog\\nDocs\\n🦜🔗 LangChain\\nBuild and deploy LLM apps with confidence\\nAn all-in-one developer platform for every step of the application lifecycle.\\n Prompt playground\\nCross-team collaboration\\nCatalog of ranging models & tasks\\nProven prompting strategies\\nExplore LangChain Hub\\nTurn the magic of LLM applications into enterprise-ready products\\nNative collaboration\\nBring your team together in LangSmith to craft prompts, debug, and capture feedback.\\n Application-level usage stats\\nFeedback collection\\nFilter traces\\nCost measurement\\nPerformance comparison\\nGo To Docs\\nManage Prompts\\nPrompts power your team's chains and agents, and LangSmith allows you to refine, test, and version them in one place. Dataset curation\\nEvaluate chain performance\\nAI-assisted evaluation\\nEasy benchmarking\\nGo To Docs\\nMonitor\\nGiven the stochastic nature of LLMs, it can be hard to answer the simple question: “what’s happening with my application?”\", metadata={'title': 'LangSmith', 'source': 'https://www.langchain.com/langsmith', 'score': 0.94446, 'images': None}),\n",
      "                          Document(page_content='We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like.\\n Over the last few months, we’ve been working directly with some early design partners and testing it on our own internal workflows, and we’ve found LangSmith helps teams in 5 core ways:\\nDebugging\\nLangSmith gives you full visibility into model inputs and output of every step in the chain of events. As a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones.\\n A unified platform\\nWhile each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.”\\n', metadata={'title': 'Announcing LangSmith, a unified platform for debugging, testing ...', 'source': 'https://blog.langchain.dev/announcing-langsmith/', 'score': 0.93552, 'images': None}),\n",
      "                          Document(page_content=\"This was such an important unlock for us as we build in tactical functionality to our AI.\\nAdvice for Product Teams Considering LangSmith\\nWhen you're in the thick of product development, the whirlwind of AI and LLMs can be overwhelming. ou can now clearly see the citations coming through with the responses in the traces, and we’re good to ship the changes to the prompt to prod.\\n We do this primarily to legitimize the LLMs response and give our HelpHub customer’s peace of mind that their end users are in fact getting to the help resource they need (instead of an LLM just hallucinating and giving any response it wants.)\\n Take it from us, as we integrated AI more heavily and widely across our products, we’ve been more conscious than ever that the quality of the outputs matches the quality and trust that our customers have in our product. We updated our prompt to be more firm when asking for the sources:\\nWe then tested everything using LangSmith evals, to make sure that it fixes the issue before pushing to production.\\n\", metadata={'title': 'Peering Into the Soul of AI Decision-Making with LangSmith', 'source': 'https://blog.langchain.dev/peering-into-the-soul-of-ai-decision-making-with-langsmith/', 'score': 0.90045, 'images': None}),\n",
      "                          Document(page_content=\"LangSmith Cookbook: Real-world Lang Smith Examples\\nThe LangSmith Cookbook is not just a compilation of code snippets; it's a goldmine of hands-on examples designed to inspire and assist you in your projects. On This Page\\nLangSmith: Best Way to Test LLMs and AI Application\\nPublished on 12/17/2023\\nIf you're in the world of Language Learning Models (LLMs), you've probably heard of LangSmith. How to Download Feedback and Examples (opens in a new tab): Export predictions, evaluation results, and other information to add to your reports programmatically.\\n This article is your one-stop guide to understanding LangSmith, a platform that offers a plethora of features for debugging, testing, evaluating, and monitoring LLM applications.\\n How do I get access to LangSmith?\\nTo get access to LangSmith, you'll need to sign up for an account on their website.\", metadata={'title': 'LangSmith: Best Way to Test LLMs and AI Application', 'source': 'https://cheatsheet.md/langchain-tutorials/langsmith.en', 'score': 0.89128, 'images': None}),\n",
      "                          Document(page_content='A deep dive into the latest product from the creators of LangChain 🦜\\n--\\n3\\nCobus Greyling\\nLangSmith\\nI was fortunate to get early access to the LangSmith platform and in this article you will find practical code examples and demonstration…\\n--\\nLists\\nNatural Language Processing\\nPredictive Modeling w/ Python\\nAI Regulation\\nPractical Guides to Machine Learning\\nYule Wang, PhD\\nA Complete Guide to LLMs-based Autonomous Agents (Part I):\\n—\\u200a—\\u200aChain of Thought, Plan and Solve/Execute, Self-Ask, ReAct, Reflexion, Self-Consistency, Tree of Thoughts and Graph of Thoughts\\n--\\n1\\nRyan Nguyen\\nin\\nBetter Programming\\nLlamaIndex: How To Evaluate Your RAG (Retrieval Augmented Generation) Applications\\nClassic 10k reports with Australian real estate market reports to demonstrate the end-to-end evaluation process\\n--\\n2\\nKelvin Lu\\nTracing How LangChain Works Behind the Scenes\\nIntroduction of LLM Tracing technique with both LangChain built-in tracing funciton and LangChain Visualizer.\\n --\\nCameron R. Wolfe, Ph.D.\\nin\\nTowards Data Science\\nChain of Thought Prompting for LLMs\\nA practical and simple approach for “reasoning” with LLMs\\n--\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams A Step-by-Step Guide to Deploying ChromaDB in the Cloud\\n--\\nBenoit Ruiz\\nin\\nBetter Programming\\nAdvice From a Software Engineer With 8 Years of Experience\\nPractical tips for those who want to advance in their careers\\n--\\n233\\nVinita\\nin\\nBetter Programming\\n4 Strategies to Give Effective Feedback to a Difficult Person\\nRespect is like air. But if you take it away, it’s all that people can think about.\\n--\\n21\\nRicardo Ledan\\nin\\nBetter Programming\\nBuilding Context-Aware Question-Answering Systems With LLMs\\nA step-by-step guide to using embeddings, vector search, and prompt engineering for building context-aware question-answering systems\\n--\\n1\\nRecommended from Medium\\nLogan Kilpatrick\\nWhat is LangSmith and why should I care as a developer?\\n Ricardo Ledan\\nFollow\\nBetter Programming\\n--\\nShare\\nWhat is LangSmith?\\n', metadata={'title': 'The Art of LangSmithing - Better Programming', 'source': 'https://betterprogramming.pub/the-art-of-langsmithing-42dcd191a220', 'score': 0.84778, 'images': None}),\n",
      "                          Document(page_content='But what are the production challenges that need to be solved which were not as relevant in prototyping?\\nReliability — it is deceptively easy to build something that works well for a simple constrained example but actually still quite hard today to build LLM applications with the consistency that most companies would want.\\n for tools\\nKim Seon Woo - Nov 6\\nComo deixar o Swagger com tema dark mode usando NestJS\\nWiliam V. Joaquim - Nov 5\\nData visualizing\\nBIRINGANINE BASEME Destin\\n- Oct 7\\nFree Face-Swap AI 🎭 with Source Code for Web and Mobile App.\\nshadee22 - As you build more complex workflows, it can be hard to understand exactly what queries are moving through different flows so a simple UI to see this and log the historical data is going to be a value add from day one.\\n A huge part of the value add for LangSmith is being able to do all of these things from a simple and intuitive UI which significantly reduces the barrier to entry for those without a software background.\\n [Quick note: I am writing this article to reflect my personal views as I explore the LLM ecosystem and this is not intended to represent the views of my employer, hence being on my personal blog]\\nWhat is LangSmith?', metadata={'title': 'What is LangSmith and why should I care as a developer?', 'source': 'https://dev.to/logankilpatrick/what-is-langsmith-and-why-should-i-care-as-a-developer-19k', 'score': 0.84204, 'images': None}),\n",
      "                          Document(page_content=\"LangSmith lets you evaluate any LLM, chain, agent, or even a custom function. Conversational agents are stateful (they have memory); to ensure that this state isn't shared between dataset runs, we will pass in a chain_factory (aka a constructor) function to initialize for each call.\", metadata={'title': 'LangSmith Walkthrough - Google Colab', 'source': 'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', 'score': 0.8413, 'images': None}),\n",
      "                          Document(page_content='The Run tab for the top-level chain shows the human input, the parsed output, the latency (under a second), and the tokens used, as well as the clock time and call status.\\n The cookbook covers tracing your code without LangChain (using the @traceable decorator); using the LangChain Hub to discover, share, and version control prompts; testing and benchmarking your LLM systems in Python and TypeScript or JavaScript; using user feedback to improve, monitor, and personalize your applications; exporting data for fine-tuning; and exporting your run data for exploratory data analysis.\\n In addition to the input variables (prompt), an LLM call uses a template and often auxiliary functions; for example, retrieval of information from the web, uploaded files, and system prompts that set the context for the LLM.\\n After I set up my LangSmith account, created my API key, updated my LangChain installation with pip, and set up my shell environment variables, I tried to run the Python quickstart application:\\nBefore we discuss the results, take a look at the LangSmith hub:\\nFigure 1. Here are the evaluation statistics, which were printed in the terminal during the run:\\nSomebody had fun creating the rap battle prompts, as shown in the dataset below:\\nFigure 9.', metadata={'title': 'What is LangSmith? Tracing and debugging for LLMs | InfoWorld', 'source': 'https://www.infoworld.com/article/3708628/what-is-langsmith-tracing-and-debugging-for-llms.html', 'score': 0.80079, 'images': None})]}},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/Docs/end_time',\n",
      "  'value': '2024-04-16T15:56:23.898+00:00'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableSequence/streamed_output/-',\n",
      "  'value': [Document(page_content='How it Works, Use Cases, Alternatives & More\\nRichie Cotton\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAdel Nehme\\n32 min\\nAn Introductory Guide to Fine-Tuning LLMs\\nJosep Ferrer\\n12 min\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nBex Tuychiev\\n15 min\\nGrow your data skills with DataCamp for Mobile\\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\\n For labeled datasets like the CSV dataset we uploaded, LangSmith offers more comprehensive evaluators for measuring the correctness of the response to a prompt:\\nLet’s try the last one on our examples:\\nCoTQA criterion returns a score called Contextual accuracy, as depicted in the GIF below (also in the UI):\\nPlease visit the LangChain evaluators section of LangSmith docs to learn much more about evaluators.\\n Once the upload finishes, the dataset will appear in the UI:\\nLet’s run our custom criterion from the previous section on this dataset as well:\\nIf you go to the dataset page and check out the run, we can see the average scores for each custom criteria:\\nEvaluating Labeled Datasets\\nBuilt-in and custom evaluators written in natural language are mostly for unlabeled datasets. We will use the create_dataset function of the client:\\nNow, let’s add three inputs that each ask the LLM to create a single flashcard:\\nIf you go over the dataset tab of the UI, you will see each prompt listed with NULL output:\\nNow, let’s run all the prompts in a single line of code using run_on_dataset function:\\n How it Works, Use Cases, Alternatives & More\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAn Introductory Guide to Fine-Tuning LLMs\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nStart Your Langchain Journey Today\\nCourse\\nDeveloping LLM Applications with LangChain\\nCourse\\nLarge Language Models (LLMs)', metadata={'title': 'An Introduction to Debugging And Testing LLMs in LangSmith', 'source': 'https://www.datacamp.com/tutorial/introduction-to-langsmith', 'score': 0.96075, 'images': None}),\n",
      "            Document(page_content='This function helps to load the specific language models and tools required for the task as shown in the code snippet below:\\nAs a next step, initialize an agent by calling the initialize_agent function with several parameters like tools, llms, and agent:\\nThe verbose parameter is set to false, indicating that the agent will not provide verbose or detailed output.\\n You can accomplish this by following the shell commands provided below:\\nCreating a LangSmith client\\nNext, create a LangSmith client to interact with the API:\\nIf you’re using Python, run the following commands to import the module:\\n This code also handles exceptions that may occur during the agent execution:\\nIt’s also important to call the wait_for_all_tracers function from the langchain.callbacks.tracers.langchain module as shown in the code snippet below:\\nCalling the wait_for_all_tracers function helps ensure that logs and traces are submitted in full before the program proceeds. The temperature parameter will be set to 0, implying that the generated response will be more deterministic as shown in the code snippet below:\\nNow, let’s call the load_tools function with a list of tool APIs, such as serpapi and llm-math, and also take the llm instance as a parameter. It initializes a chat model, loads specific tools, and creates an agent that can generate responses based on descriptions:\\nInput processing with exception handling\\nThe code below defines a list of input examples using the asyncio library to asynchronously run the agent on each input and gather the results for further processing.', metadata={'title': 'Using LangSmith to test LLMs and AI applications', 'source': 'https://blog.logrocket.com/langsmith-test-llms-ai-applications/', 'score': 0.95984, 'images': None}),\n",
      "            Document(page_content=\"BETA Sign Up\\nContact Sales\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter 🦜🔗 LangChain\\nLangSmith\\nLangServe\\nAgents\\nRetrieval\\nEvaluation\\nBlog\\nDocs\\n🦜🔗 LangChain\\nBuild and deploy LLM apps with confidence\\nAn all-in-one developer platform for every step of the application lifecycle.\\n Prompt playground\\nCross-team collaboration\\nCatalog of ranging models & tasks\\nProven prompting strategies\\nExplore LangChain Hub\\nTurn the magic of LLM applications into enterprise-ready products\\nNative collaboration\\nBring your team together in LangSmith to craft prompts, debug, and capture feedback.\\n Application-level usage stats\\nFeedback collection\\nFilter traces\\nCost measurement\\nPerformance comparison\\nGo To Docs\\nManage Prompts\\nPrompts power your team's chains and agents, and LangSmith allows you to refine, test, and version them in one place. Dataset curation\\nEvaluate chain performance\\nAI-assisted evaluation\\nEasy benchmarking\\nGo To Docs\\nMonitor\\nGiven the stochastic nature of LLMs, it can be hard to answer the simple question: “what’s happening with my application?”\", metadata={'title': 'LangSmith', 'source': 'https://www.langchain.com/langsmith', 'score': 0.94446, 'images': None}),\n",
      "            Document(page_content='We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like.\\n Over the last few months, we’ve been working directly with some early design partners and testing it on our own internal workflows, and we’ve found LangSmith helps teams in 5 core ways:\\nDebugging\\nLangSmith gives you full visibility into model inputs and output of every step in the chain of events. As a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones.\\n A unified platform\\nWhile each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.”\\n', metadata={'title': 'Announcing LangSmith, a unified platform for debugging, testing ...', 'source': 'https://blog.langchain.dev/announcing-langsmith/', 'score': 0.93552, 'images': None}),\n",
      "            Document(page_content=\"This was such an important unlock for us as we build in tactical functionality to our AI.\\nAdvice for Product Teams Considering LangSmith\\nWhen you're in the thick of product development, the whirlwind of AI and LLMs can be overwhelming. ou can now clearly see the citations coming through with the responses in the traces, and we’re good to ship the changes to the prompt to prod.\\n We do this primarily to legitimize the LLMs response and give our HelpHub customer’s peace of mind that their end users are in fact getting to the help resource they need (instead of an LLM just hallucinating and giving any response it wants.)\\n Take it from us, as we integrated AI more heavily and widely across our products, we’ve been more conscious than ever that the quality of the outputs matches the quality and trust that our customers have in our product. We updated our prompt to be more firm when asking for the sources:\\nWe then tested everything using LangSmith evals, to make sure that it fixes the issue before pushing to production.\\n\", metadata={'title': 'Peering Into the Soul of AI Decision-Making with LangSmith', 'source': 'https://blog.langchain.dev/peering-into-the-soul-of-ai-decision-making-with-langsmith/', 'score': 0.90045, 'images': None}),\n",
      "            Document(page_content=\"LangSmith Cookbook: Real-world Lang Smith Examples\\nThe LangSmith Cookbook is not just a compilation of code snippets; it's a goldmine of hands-on examples designed to inspire and assist you in your projects. On This Page\\nLangSmith: Best Way to Test LLMs and AI Application\\nPublished on 12/17/2023\\nIf you're in the world of Language Learning Models (LLMs), you've probably heard of LangSmith. How to Download Feedback and Examples (opens in a new tab): Export predictions, evaluation results, and other information to add to your reports programmatically.\\n This article is your one-stop guide to understanding LangSmith, a platform that offers a plethora of features for debugging, testing, evaluating, and monitoring LLM applications.\\n How do I get access to LangSmith?\\nTo get access to LangSmith, you'll need to sign up for an account on their website.\", metadata={'title': 'LangSmith: Best Way to Test LLMs and AI Application', 'source': 'https://cheatsheet.md/langchain-tutorials/langsmith.en', 'score': 0.89128, 'images': None}),\n",
      "            Document(page_content='A deep dive into the latest product from the creators of LangChain 🦜\\n--\\n3\\nCobus Greyling\\nLangSmith\\nI was fortunate to get early access to the LangSmith platform and in this article you will find practical code examples and demonstration…\\n--\\nLists\\nNatural Language Processing\\nPredictive Modeling w/ Python\\nAI Regulation\\nPractical Guides to Machine Learning\\nYule Wang, PhD\\nA Complete Guide to LLMs-based Autonomous Agents (Part I):\\n—\\u200a—\\u200aChain of Thought, Plan and Solve/Execute, Self-Ask, ReAct, Reflexion, Self-Consistency, Tree of Thoughts and Graph of Thoughts\\n--\\n1\\nRyan Nguyen\\nin\\nBetter Programming\\nLlamaIndex: How To Evaluate Your RAG (Retrieval Augmented Generation) Applications\\nClassic 10k reports with Australian real estate market reports to demonstrate the end-to-end evaluation process\\n--\\n2\\nKelvin Lu\\nTracing How LangChain Works Behind the Scenes\\nIntroduction of LLM Tracing technique with both LangChain built-in tracing funciton and LangChain Visualizer.\\n --\\nCameron R. Wolfe, Ph.D.\\nin\\nTowards Data Science\\nChain of Thought Prompting for LLMs\\nA practical and simple approach for “reasoning” with LLMs\\n--\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams A Step-by-Step Guide to Deploying ChromaDB in the Cloud\\n--\\nBenoit Ruiz\\nin\\nBetter Programming\\nAdvice From a Software Engineer With 8 Years of Experience\\nPractical tips for those who want to advance in their careers\\n--\\n233\\nVinita\\nin\\nBetter Programming\\n4 Strategies to Give Effective Feedback to a Difficult Person\\nRespect is like air. But if you take it away, it’s all that people can think about.\\n--\\n21\\nRicardo Ledan\\nin\\nBetter Programming\\nBuilding Context-Aware Question-Answering Systems With LLMs\\nA step-by-step guide to using embeddings, vector search, and prompt engineering for building context-aware question-answering systems\\n--\\n1\\nRecommended from Medium\\nLogan Kilpatrick\\nWhat is LangSmith and why should I care as a developer?\\n Ricardo Ledan\\nFollow\\nBetter Programming\\n--\\nShare\\nWhat is LangSmith?\\n', metadata={'title': 'The Art of LangSmithing - Better Programming', 'source': 'https://betterprogramming.pub/the-art-of-langsmithing-42dcd191a220', 'score': 0.84778, 'images': None}),\n",
      "            Document(page_content='But what are the production challenges that need to be solved which were not as relevant in prototyping?\\nReliability — it is deceptively easy to build something that works well for a simple constrained example but actually still quite hard today to build LLM applications with the consistency that most companies would want.\\n for tools\\nKim Seon Woo - Nov 6\\nComo deixar o Swagger com tema dark mode usando NestJS\\nWiliam V. Joaquim - Nov 5\\nData visualizing\\nBIRINGANINE BASEME Destin\\n- Oct 7\\nFree Face-Swap AI 🎭 with Source Code for Web and Mobile App.\\nshadee22 - As you build more complex workflows, it can be hard to understand exactly what queries are moving through different flows so a simple UI to see this and log the historical data is going to be a value add from day one.\\n A huge part of the value add for LangSmith is being able to do all of these things from a simple and intuitive UI which significantly reduces the barrier to entry for those without a software background.\\n [Quick note: I am writing this article to reflect my personal views as I explore the LLM ecosystem and this is not intended to represent the views of my employer, hence being on my personal blog]\\nWhat is LangSmith?', metadata={'title': 'What is LangSmith and why should I care as a developer?', 'source': 'https://dev.to/logankilpatrick/what-is-langsmith-and-why-should-i-care-as-a-developer-19k', 'score': 0.84204, 'images': None}),\n",
      "            Document(page_content=\"LangSmith lets you evaluate any LLM, chain, agent, or even a custom function. Conversational agents are stateful (they have memory); to ensure that this state isn't shared between dataset runs, we will pass in a chain_factory (aka a constructor) function to initialize for each call.\", metadata={'title': 'LangSmith Walkthrough - Google Colab', 'source': 'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', 'score': 0.8413, 'images': None}),\n",
      "            Document(page_content='The Run tab for the top-level chain shows the human input, the parsed output, the latency (under a second), and the tokens used, as well as the clock time and call status.\\n The cookbook covers tracing your code without LangChain (using the @traceable decorator); using the LangChain Hub to discover, share, and version control prompts; testing and benchmarking your LLM systems in Python and TypeScript or JavaScript; using user feedback to improve, monitor, and personalize your applications; exporting data for fine-tuning; and exporting your run data for exploratory data analysis.\\n In addition to the input variables (prompt), an LLM call uses a template and often auxiliary functions; for example, retrieval of information from the web, uploaded files, and system prompts that set the context for the LLM.\\n After I set up my LangSmith account, created my API key, updated my LangChain installation with pip, and set up my shell environment variables, I tried to run the Python quickstart application:\\nBefore we discuss the results, take a look at the LangSmith hub:\\nFigure 1. Here are the evaluation statistics, which were printed in the terminal during the run:\\nSomebody had fun creating the rap battle prompts, as shown in the dataset below:\\nFigure 9.', metadata={'title': 'What is LangSmith? Tracing and debugging for LLMs | InfoWorld', 'source': 'https://www.infoworld.com/article/3708628/what-is-langsmith-tracing-and-debugging-for-llms.html', 'score': 0.80079, 'images': None})]})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableParallel<context>/streamed_output/-',\n",
      "  'value': {'context': [Document(page_content='How it Works, Use Cases, Alternatives & More\\nRichie Cotton\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAdel Nehme\\n32 min\\nAn Introductory Guide to Fine-Tuning LLMs\\nJosep Ferrer\\n12 min\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nBex Tuychiev\\n15 min\\nGrow your data skills with DataCamp for Mobile\\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\\n For labeled datasets like the CSV dataset we uploaded, LangSmith offers more comprehensive evaluators for measuring the correctness of the response to a prompt:\\nLet’s try the last one on our examples:\\nCoTQA criterion returns a score called Contextual accuracy, as depicted in the GIF below (also in the UI):\\nPlease visit the LangChain evaluators section of LangSmith docs to learn much more about evaluators.\\n Once the upload finishes, the dataset will appear in the UI:\\nLet’s run our custom criterion from the previous section on this dataset as well:\\nIf you go to the dataset page and check out the run, we can see the average scores for each custom criteria:\\nEvaluating Labeled Datasets\\nBuilt-in and custom evaluators written in natural language are mostly for unlabeled datasets. We will use the create_dataset function of the client:\\nNow, let’s add three inputs that each ask the LLM to create a single flashcard:\\nIf you go over the dataset tab of the UI, you will see each prompt listed with NULL output:\\nNow, let’s run all the prompts in a single line of code using run_on_dataset function:\\n How it Works, Use Cases, Alternatives & More\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAn Introductory Guide to Fine-Tuning LLMs\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nStart Your Langchain Journey Today\\nCourse\\nDeveloping LLM Applications with LangChain\\nCourse\\nLarge Language Models (LLMs)', metadata={'title': 'An Introduction to Debugging And Testing LLMs in LangSmith', 'source': 'https://www.datacamp.com/tutorial/introduction-to-langsmith', 'score': 0.96075, 'images': None}),\n",
      "                        Document(page_content='This function helps to load the specific language models and tools required for the task as shown in the code snippet below:\\nAs a next step, initialize an agent by calling the initialize_agent function with several parameters like tools, llms, and agent:\\nThe verbose parameter is set to false, indicating that the agent will not provide verbose or detailed output.\\n You can accomplish this by following the shell commands provided below:\\nCreating a LangSmith client\\nNext, create a LangSmith client to interact with the API:\\nIf you’re using Python, run the following commands to import the module:\\n This code also handles exceptions that may occur during the agent execution:\\nIt’s also important to call the wait_for_all_tracers function from the langchain.callbacks.tracers.langchain module as shown in the code snippet below:\\nCalling the wait_for_all_tracers function helps ensure that logs and traces are submitted in full before the program proceeds. The temperature parameter will be set to 0, implying that the generated response will be more deterministic as shown in the code snippet below:\\nNow, let’s call the load_tools function with a list of tool APIs, such as serpapi and llm-math, and also take the llm instance as a parameter. It initializes a chat model, loads specific tools, and creates an agent that can generate responses based on descriptions:\\nInput processing with exception handling\\nThe code below defines a list of input examples using the asyncio library to asynchronously run the agent on each input and gather the results for further processing.', metadata={'title': 'Using LangSmith to test LLMs and AI applications', 'source': 'https://blog.logrocket.com/langsmith-test-llms-ai-applications/', 'score': 0.95984, 'images': None}),\n",
      "                        Document(page_content=\"BETA Sign Up\\nContact Sales\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter 🦜🔗 LangChain\\nLangSmith\\nLangServe\\nAgents\\nRetrieval\\nEvaluation\\nBlog\\nDocs\\n🦜🔗 LangChain\\nBuild and deploy LLM apps with confidence\\nAn all-in-one developer platform for every step of the application lifecycle.\\n Prompt playground\\nCross-team collaboration\\nCatalog of ranging models & tasks\\nProven prompting strategies\\nExplore LangChain Hub\\nTurn the magic of LLM applications into enterprise-ready products\\nNative collaboration\\nBring your team together in LangSmith to craft prompts, debug, and capture feedback.\\n Application-level usage stats\\nFeedback collection\\nFilter traces\\nCost measurement\\nPerformance comparison\\nGo To Docs\\nManage Prompts\\nPrompts power your team's chains and agents, and LangSmith allows you to refine, test, and version them in one place. Dataset curation\\nEvaluate chain performance\\nAI-assisted evaluation\\nEasy benchmarking\\nGo To Docs\\nMonitor\\nGiven the stochastic nature of LLMs, it can be hard to answer the simple question: “what’s happening with my application?”\", metadata={'title': 'LangSmith', 'source': 'https://www.langchain.com/langsmith', 'score': 0.94446, 'images': None}),\n",
      "                        Document(page_content='We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like.\\n Over the last few months, we’ve been working directly with some early design partners and testing it on our own internal workflows, and we’ve found LangSmith helps teams in 5 core ways:\\nDebugging\\nLangSmith gives you full visibility into model inputs and output of every step in the chain of events. As a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones.\\n A unified platform\\nWhile each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.”\\n', metadata={'title': 'Announcing LangSmith, a unified platform for debugging, testing ...', 'source': 'https://blog.langchain.dev/announcing-langsmith/', 'score': 0.93552, 'images': None}),\n",
      "                        Document(page_content=\"This was such an important unlock for us as we build in tactical functionality to our AI.\\nAdvice for Product Teams Considering LangSmith\\nWhen you're in the thick of product development, the whirlwind of AI and LLMs can be overwhelming. ou can now clearly see the citations coming through with the responses in the traces, and we’re good to ship the changes to the prompt to prod.\\n We do this primarily to legitimize the LLMs response and give our HelpHub customer’s peace of mind that their end users are in fact getting to the help resource they need (instead of an LLM just hallucinating and giving any response it wants.)\\n Take it from us, as we integrated AI more heavily and widely across our products, we’ve been more conscious than ever that the quality of the outputs matches the quality and trust that our customers have in our product. We updated our prompt to be more firm when asking for the sources:\\nWe then tested everything using LangSmith evals, to make sure that it fixes the issue before pushing to production.\\n\", metadata={'title': 'Peering Into the Soul of AI Decision-Making with LangSmith', 'source': 'https://blog.langchain.dev/peering-into-the-soul-of-ai-decision-making-with-langsmith/', 'score': 0.90045, 'images': None}),\n",
      "                        Document(page_content=\"LangSmith Cookbook: Real-world Lang Smith Examples\\nThe LangSmith Cookbook is not just a compilation of code snippets; it's a goldmine of hands-on examples designed to inspire and assist you in your projects. On This Page\\nLangSmith: Best Way to Test LLMs and AI Application\\nPublished on 12/17/2023\\nIf you're in the world of Language Learning Models (LLMs), you've probably heard of LangSmith. How to Download Feedback and Examples (opens in a new tab): Export predictions, evaluation results, and other information to add to your reports programmatically.\\n This article is your one-stop guide to understanding LangSmith, a platform that offers a plethora of features for debugging, testing, evaluating, and monitoring LLM applications.\\n How do I get access to LangSmith?\\nTo get access to LangSmith, you'll need to sign up for an account on their website.\", metadata={'title': 'LangSmith: Best Way to Test LLMs and AI Application', 'source': 'https://cheatsheet.md/langchain-tutorials/langsmith.en', 'score': 0.89128, 'images': None}),\n",
      "                        Document(page_content='A deep dive into the latest product from the creators of LangChain 🦜\\n--\\n3\\nCobus Greyling\\nLangSmith\\nI was fortunate to get early access to the LangSmith platform and in this article you will find practical code examples and demonstration…\\n--\\nLists\\nNatural Language Processing\\nPredictive Modeling w/ Python\\nAI Regulation\\nPractical Guides to Machine Learning\\nYule Wang, PhD\\nA Complete Guide to LLMs-based Autonomous Agents (Part I):\\n—\\u200a—\\u200aChain of Thought, Plan and Solve/Execute, Self-Ask, ReAct, Reflexion, Self-Consistency, Tree of Thoughts and Graph of Thoughts\\n--\\n1\\nRyan Nguyen\\nin\\nBetter Programming\\nLlamaIndex: How To Evaluate Your RAG (Retrieval Augmented Generation) Applications\\nClassic 10k reports with Australian real estate market reports to demonstrate the end-to-end evaluation process\\n--\\n2\\nKelvin Lu\\nTracing How LangChain Works Behind the Scenes\\nIntroduction of LLM Tracing technique with both LangChain built-in tracing funciton and LangChain Visualizer.\\n --\\nCameron R. Wolfe, Ph.D.\\nin\\nTowards Data Science\\nChain of Thought Prompting for LLMs\\nA practical and simple approach for “reasoning” with LLMs\\n--\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams A Step-by-Step Guide to Deploying ChromaDB in the Cloud\\n--\\nBenoit Ruiz\\nin\\nBetter Programming\\nAdvice From a Software Engineer With 8 Years of Experience\\nPractical tips for those who want to advance in their careers\\n--\\n233\\nVinita\\nin\\nBetter Programming\\n4 Strategies to Give Effective Feedback to a Difficult Person\\nRespect is like air. But if you take it away, it’s all that people can think about.\\n--\\n21\\nRicardo Ledan\\nin\\nBetter Programming\\nBuilding Context-Aware Question-Answering Systems With LLMs\\nA step-by-step guide to using embeddings, vector search, and prompt engineering for building context-aware question-answering systems\\n--\\n1\\nRecommended from Medium\\nLogan Kilpatrick\\nWhat is LangSmith and why should I care as a developer?\\n Ricardo Ledan\\nFollow\\nBetter Programming\\n--\\nShare\\nWhat is LangSmith?\\n', metadata={'title': 'The Art of LangSmithing - Better Programming', 'source': 'https://betterprogramming.pub/the-art-of-langsmithing-42dcd191a220', 'score': 0.84778, 'images': None}),\n",
      "                        Document(page_content='But what are the production challenges that need to be solved which were not as relevant in prototyping?\\nReliability — it is deceptively easy to build something that works well for a simple constrained example but actually still quite hard today to build LLM applications with the consistency that most companies would want.\\n for tools\\nKim Seon Woo - Nov 6\\nComo deixar o Swagger com tema dark mode usando NestJS\\nWiliam V. Joaquim - Nov 5\\nData visualizing\\nBIRINGANINE BASEME Destin\\n- Oct 7\\nFree Face-Swap AI 🎭 with Source Code for Web and Mobile App.\\nshadee22 - As you build more complex workflows, it can be hard to understand exactly what queries are moving through different flows so a simple UI to see this and log the historical data is going to be a value add from day one.\\n A huge part of the value add for LangSmith is being able to do all of these things from a simple and intuitive UI which significantly reduces the barrier to entry for those without a software background.\\n [Quick note: I am writing this article to reflect my personal views as I explore the LLM ecosystem and this is not intended to represent the views of my employer, hence being on my personal blog]\\nWhat is LangSmith?', metadata={'title': 'What is LangSmith and why should I care as a developer?', 'source': 'https://dev.to/logankilpatrick/what-is-langsmith-and-why-should-i-care-as-a-developer-19k', 'score': 0.84204, 'images': None}),\n",
      "                        Document(page_content=\"LangSmith lets you evaluate any LLM, chain, agent, or even a custom function. Conversational agents are stateful (they have memory); to ensure that this state isn't shared between dataset runs, we will pass in a chain_factory (aka a constructor) function to initialize for each call.\", metadata={'title': 'LangSmith Walkthrough - Google Colab', 'source': 'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', 'score': 0.8413, 'images': None}),\n",
      "                        Document(page_content='The Run tab for the top-level chain shows the human input, the parsed output, the latency (under a second), and the tokens used, as well as the clock time and call status.\\n The cookbook covers tracing your code without LangChain (using the @traceable decorator); using the LangChain Hub to discover, share, and version control prompts; testing and benchmarking your LLM systems in Python and TypeScript or JavaScript; using user feedback to improve, monitor, and personalize your applications; exporting data for fine-tuning; and exporting your run data for exploratory data analysis.\\n In addition to the input variables (prompt), an LLM call uses a template and often auxiliary functions; for example, retrieval of information from the web, uploaded files, and system prompts that set the context for the LLM.\\n After I set up my LangSmith account, created my API key, updated my LangChain installation with pip, and set up my shell environment variables, I tried to run the Python quickstart application:\\nBefore we discuss the results, take a look at the LangSmith hub:\\nFigure 1. Here are the evaluation statistics, which were printed in the terminal during the run:\\nSomebody had fun creating the rap battle prompts, as shown in the dataset below:\\nFigure 9.', metadata={'title': 'What is LangSmith? Tracing and debugging for LLMs | InfoWorld', 'source': 'https://www.infoworld.com/article/3708628/what-is-langsmith-tracing-and-debugging-for-llms.html', 'score': 0.80079, 'images': None})]}})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableAssign<context>/streamed_output/-',\n",
      "  'value': {'context': [Document(page_content='How it Works, Use Cases, Alternatives & More\\nRichie Cotton\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAdel Nehme\\n32 min\\nAn Introductory Guide to Fine-Tuning LLMs\\nJosep Ferrer\\n12 min\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nBex Tuychiev\\n15 min\\nGrow your data skills with DataCamp for Mobile\\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\\n For labeled datasets like the CSV dataset we uploaded, LangSmith offers more comprehensive evaluators for measuring the correctness of the response to a prompt:\\nLet’s try the last one on our examples:\\nCoTQA criterion returns a score called Contextual accuracy, as depicted in the GIF below (also in the UI):\\nPlease visit the LangChain evaluators section of LangSmith docs to learn much more about evaluators.\\n Once the upload finishes, the dataset will appear in the UI:\\nLet’s run our custom criterion from the previous section on this dataset as well:\\nIf you go to the dataset page and check out the run, we can see the average scores for each custom criteria:\\nEvaluating Labeled Datasets\\nBuilt-in and custom evaluators written in natural language are mostly for unlabeled datasets. We will use the create_dataset function of the client:\\nNow, let’s add three inputs that each ask the LLM to create a single flashcard:\\nIf you go over the dataset tab of the UI, you will see each prompt listed with NULL output:\\nNow, let’s run all the prompts in a single line of code using run_on_dataset function:\\n How it Works, Use Cases, Alternatives & More\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAn Introductory Guide to Fine-Tuning LLMs\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nStart Your Langchain Journey Today\\nCourse\\nDeveloping LLM Applications with LangChain\\nCourse\\nLarge Language Models (LLMs)', metadata={'title': 'An Introduction to Debugging And Testing LLMs in LangSmith', 'source': 'https://www.datacamp.com/tutorial/introduction-to-langsmith', 'score': 0.96075, 'images': None}),\n",
      "                        Document(page_content='This function helps to load the specific language models and tools required for the task as shown in the code snippet below:\\nAs a next step, initialize an agent by calling the initialize_agent function with several parameters like tools, llms, and agent:\\nThe verbose parameter is set to false, indicating that the agent will not provide verbose or detailed output.\\n You can accomplish this by following the shell commands provided below:\\nCreating a LangSmith client\\nNext, create a LangSmith client to interact with the API:\\nIf you’re using Python, run the following commands to import the module:\\n This code also handles exceptions that may occur during the agent execution:\\nIt’s also important to call the wait_for_all_tracers function from the langchain.callbacks.tracers.langchain module as shown in the code snippet below:\\nCalling the wait_for_all_tracers function helps ensure that logs and traces are submitted in full before the program proceeds. The temperature parameter will be set to 0, implying that the generated response will be more deterministic as shown in the code snippet below:\\nNow, let’s call the load_tools function with a list of tool APIs, such as serpapi and llm-math, and also take the llm instance as a parameter. It initializes a chat model, loads specific tools, and creates an agent that can generate responses based on descriptions:\\nInput processing with exception handling\\nThe code below defines a list of input examples using the asyncio library to asynchronously run the agent on each input and gather the results for further processing.', metadata={'title': 'Using LangSmith to test LLMs and AI applications', 'source': 'https://blog.logrocket.com/langsmith-test-llms-ai-applications/', 'score': 0.95984, 'images': None}),\n",
      "                        Document(page_content=\"BETA Sign Up\\nContact Sales\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter 🦜🔗 LangChain\\nLangSmith\\nLangServe\\nAgents\\nRetrieval\\nEvaluation\\nBlog\\nDocs\\n🦜🔗 LangChain\\nBuild and deploy LLM apps with confidence\\nAn all-in-one developer platform for every step of the application lifecycle.\\n Prompt playground\\nCross-team collaboration\\nCatalog of ranging models & tasks\\nProven prompting strategies\\nExplore LangChain Hub\\nTurn the magic of LLM applications into enterprise-ready products\\nNative collaboration\\nBring your team together in LangSmith to craft prompts, debug, and capture feedback.\\n Application-level usage stats\\nFeedback collection\\nFilter traces\\nCost measurement\\nPerformance comparison\\nGo To Docs\\nManage Prompts\\nPrompts power your team's chains and agents, and LangSmith allows you to refine, test, and version them in one place. Dataset curation\\nEvaluate chain performance\\nAI-assisted evaluation\\nEasy benchmarking\\nGo To Docs\\nMonitor\\nGiven the stochastic nature of LLMs, it can be hard to answer the simple question: “what’s happening with my application?”\", metadata={'title': 'LangSmith', 'source': 'https://www.langchain.com/langsmith', 'score': 0.94446, 'images': None}),\n",
      "                        Document(page_content='We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like.\\n Over the last few months, we’ve been working directly with some early design partners and testing it on our own internal workflows, and we’ve found LangSmith helps teams in 5 core ways:\\nDebugging\\nLangSmith gives you full visibility into model inputs and output of every step in the chain of events. As a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones.\\n A unified platform\\nWhile each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.”\\n', metadata={'title': 'Announcing LangSmith, a unified platform for debugging, testing ...', 'source': 'https://blog.langchain.dev/announcing-langsmith/', 'score': 0.93552, 'images': None}),\n",
      "                        Document(page_content=\"This was such an important unlock for us as we build in tactical functionality to our AI.\\nAdvice for Product Teams Considering LangSmith\\nWhen you're in the thick of product development, the whirlwind of AI and LLMs can be overwhelming. ou can now clearly see the citations coming through with the responses in the traces, and we’re good to ship the changes to the prompt to prod.\\n We do this primarily to legitimize the LLMs response and give our HelpHub customer’s peace of mind that their end users are in fact getting to the help resource they need (instead of an LLM just hallucinating and giving any response it wants.)\\n Take it from us, as we integrated AI more heavily and widely across our products, we’ve been more conscious than ever that the quality of the outputs matches the quality and trust that our customers have in our product. We updated our prompt to be more firm when asking for the sources:\\nWe then tested everything using LangSmith evals, to make sure that it fixes the issue before pushing to production.\\n\", metadata={'title': 'Peering Into the Soul of AI Decision-Making with LangSmith', 'source': 'https://blog.langchain.dev/peering-into-the-soul-of-ai-decision-making-with-langsmith/', 'score': 0.90045, 'images': None}),\n",
      "                        Document(page_content=\"LangSmith Cookbook: Real-world Lang Smith Examples\\nThe LangSmith Cookbook is not just a compilation of code snippets; it's a goldmine of hands-on examples designed to inspire and assist you in your projects. On This Page\\nLangSmith: Best Way to Test LLMs and AI Application\\nPublished on 12/17/2023\\nIf you're in the world of Language Learning Models (LLMs), you've probably heard of LangSmith. How to Download Feedback and Examples (opens in a new tab): Export predictions, evaluation results, and other information to add to your reports programmatically.\\n This article is your one-stop guide to understanding LangSmith, a platform that offers a plethora of features for debugging, testing, evaluating, and monitoring LLM applications.\\n How do I get access to LangSmith?\\nTo get access to LangSmith, you'll need to sign up for an account on their website.\", metadata={'title': 'LangSmith: Best Way to Test LLMs and AI Application', 'source': 'https://cheatsheet.md/langchain-tutorials/langsmith.en', 'score': 0.89128, 'images': None}),\n",
      "                        Document(page_content='A deep dive into the latest product from the creators of LangChain 🦜\\n--\\n3\\nCobus Greyling\\nLangSmith\\nI was fortunate to get early access to the LangSmith platform and in this article you will find practical code examples and demonstration…\\n--\\nLists\\nNatural Language Processing\\nPredictive Modeling w/ Python\\nAI Regulation\\nPractical Guides to Machine Learning\\nYule Wang, PhD\\nA Complete Guide to LLMs-based Autonomous Agents (Part I):\\n—\\u200a—\\u200aChain of Thought, Plan and Solve/Execute, Self-Ask, ReAct, Reflexion, Self-Consistency, Tree of Thoughts and Graph of Thoughts\\n--\\n1\\nRyan Nguyen\\nin\\nBetter Programming\\nLlamaIndex: How To Evaluate Your RAG (Retrieval Augmented Generation) Applications\\nClassic 10k reports with Australian real estate market reports to demonstrate the end-to-end evaluation process\\n--\\n2\\nKelvin Lu\\nTracing How LangChain Works Behind the Scenes\\nIntroduction of LLM Tracing technique with both LangChain built-in tracing funciton and LangChain Visualizer.\\n --\\nCameron R. Wolfe, Ph.D.\\nin\\nTowards Data Science\\nChain of Thought Prompting for LLMs\\nA practical and simple approach for “reasoning” with LLMs\\n--\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams A Step-by-Step Guide to Deploying ChromaDB in the Cloud\\n--\\nBenoit Ruiz\\nin\\nBetter Programming\\nAdvice From a Software Engineer With 8 Years of Experience\\nPractical tips for those who want to advance in their careers\\n--\\n233\\nVinita\\nin\\nBetter Programming\\n4 Strategies to Give Effective Feedback to a Difficult Person\\nRespect is like air. But if you take it away, it’s all that people can think about.\\n--\\n21\\nRicardo Ledan\\nin\\nBetter Programming\\nBuilding Context-Aware Question-Answering Systems With LLMs\\nA step-by-step guide to using embeddings, vector search, and prompt engineering for building context-aware question-answering systems\\n--\\n1\\nRecommended from Medium\\nLogan Kilpatrick\\nWhat is LangSmith and why should I care as a developer?\\n Ricardo Ledan\\nFollow\\nBetter Programming\\n--\\nShare\\nWhat is LangSmith?\\n', metadata={'title': 'The Art of LangSmithing - Better Programming', 'source': 'https://betterprogramming.pub/the-art-of-langsmithing-42dcd191a220', 'score': 0.84778, 'images': None}),\n",
      "                        Document(page_content='But what are the production challenges that need to be solved which were not as relevant in prototyping?\\nReliability — it is deceptively easy to build something that works well for a simple constrained example but actually still quite hard today to build LLM applications with the consistency that most companies would want.\\n for tools\\nKim Seon Woo - Nov 6\\nComo deixar o Swagger com tema dark mode usando NestJS\\nWiliam V. Joaquim - Nov 5\\nData visualizing\\nBIRINGANINE BASEME Destin\\n- Oct 7\\nFree Face-Swap AI 🎭 with Source Code for Web and Mobile App.\\nshadee22 - As you build more complex workflows, it can be hard to understand exactly what queries are moving through different flows so a simple UI to see this and log the historical data is going to be a value add from day one.\\n A huge part of the value add for LangSmith is being able to do all of these things from a simple and intuitive UI which significantly reduces the barrier to entry for those without a software background.\\n [Quick note: I am writing this article to reflect my personal views as I explore the LLM ecosystem and this is not intended to represent the views of my employer, hence being on my personal blog]\\nWhat is LangSmith?', metadata={'title': 'What is LangSmith and why should I care as a developer?', 'source': 'https://dev.to/logankilpatrick/what-is-langsmith-and-why-should-i-care-as-a-developer-19k', 'score': 0.84204, 'images': None}),\n",
      "                        Document(page_content=\"LangSmith lets you evaluate any LLM, chain, agent, or even a custom function. Conversational agents are stateful (they have memory); to ensure that this state isn't shared between dataset runs, we will pass in a chain_factory (aka a constructor) function to initialize for each call.\", metadata={'title': 'LangSmith Walkthrough - Google Colab', 'source': 'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', 'score': 0.8413, 'images': None}),\n",
      "                        Document(page_content='The Run tab for the top-level chain shows the human input, the parsed output, the latency (under a second), and the tokens used, as well as the clock time and call status.\\n The cookbook covers tracing your code without LangChain (using the @traceable decorator); using the LangChain Hub to discover, share, and version control prompts; testing and benchmarking your LLM systems in Python and TypeScript or JavaScript; using user feedback to improve, monitor, and personalize your applications; exporting data for fine-tuning; and exporting your run data for exploratory data analysis.\\n In addition to the input variables (prompt), an LLM call uses a template and often auxiliary functions; for example, retrieval of information from the web, uploaded files, and system prompts that set the context for the LLM.\\n After I set up my LangSmith account, created my API key, updated my LangChain installation with pip, and set up my shell environment variables, I tried to run the Python quickstart application:\\nBefore we discuss the results, take a look at the LangSmith hub:\\nFigure 1. Here are the evaluation statistics, which were printed in the terminal during the run:\\nSomebody had fun creating the rap battle prompts, as shown in the dataset below:\\nFigure 9.', metadata={'title': 'What is LangSmith? Tracing and debugging for LLMs | InfoWorld', 'source': 'https://www.infoworld.com/article/3708628/what-is-langsmith-tracing-and-debugging-for-llms.html', 'score': 0.80079, 'images': None})]}})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableSequence/final_output',\n",
      "  'value': {'output': [Document(page_content='How it Works, Use Cases, Alternatives & More\\nRichie Cotton\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAdel Nehme\\n32 min\\nAn Introductory Guide to Fine-Tuning LLMs\\nJosep Ferrer\\n12 min\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nBex Tuychiev\\n15 min\\nGrow your data skills with DataCamp for Mobile\\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\\n For labeled datasets like the CSV dataset we uploaded, LangSmith offers more comprehensive evaluators for measuring the correctness of the response to a prompt:\\nLet’s try the last one on our examples:\\nCoTQA criterion returns a score called Contextual accuracy, as depicted in the GIF below (also in the UI):\\nPlease visit the LangChain evaluators section of LangSmith docs to learn much more about evaluators.\\n Once the upload finishes, the dataset will appear in the UI:\\nLet’s run our custom criterion from the previous section on this dataset as well:\\nIf you go to the dataset page and check out the run, we can see the average scores for each custom criteria:\\nEvaluating Labeled Datasets\\nBuilt-in and custom evaluators written in natural language are mostly for unlabeled datasets. We will use the create_dataset function of the client:\\nNow, let’s add three inputs that each ask the LLM to create a single flashcard:\\nIf you go over the dataset tab of the UI, you will see each prompt listed with NULL output:\\nNow, let’s run all the prompts in a single line of code using run_on_dataset function:\\n How it Works, Use Cases, Alternatives & More\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAn Introductory Guide to Fine-Tuning LLMs\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nStart Your Langchain Journey Today\\nCourse\\nDeveloping LLM Applications with LangChain\\nCourse\\nLarge Language Models (LLMs)', metadata={'title': 'An Introduction to Debugging And Testing LLMs in LangSmith', 'source': 'https://www.datacamp.com/tutorial/introduction-to-langsmith', 'score': 0.96075, 'images': None}),\n",
      "                       Document(page_content='This function helps to load the specific language models and tools required for the task as shown in the code snippet below:\\nAs a next step, initialize an agent by calling the initialize_agent function with several parameters like tools, llms, and agent:\\nThe verbose parameter is set to false, indicating that the agent will not provide verbose or detailed output.\\n You can accomplish this by following the shell commands provided below:\\nCreating a LangSmith client\\nNext, create a LangSmith client to interact with the API:\\nIf you’re using Python, run the following commands to import the module:\\n This code also handles exceptions that may occur during the agent execution:\\nIt’s also important to call the wait_for_all_tracers function from the langchain.callbacks.tracers.langchain module as shown in the code snippet below:\\nCalling the wait_for_all_tracers function helps ensure that logs and traces are submitted in full before the program proceeds. The temperature parameter will be set to 0, implying that the generated response will be more deterministic as shown in the code snippet below:\\nNow, let’s call the load_tools function with a list of tool APIs, such as serpapi and llm-math, and also take the llm instance as a parameter. It initializes a chat model, loads specific tools, and creates an agent that can generate responses based on descriptions:\\nInput processing with exception handling\\nThe code below defines a list of input examples using the asyncio library to asynchronously run the agent on each input and gather the results for further processing.', metadata={'title': 'Using LangSmith to test LLMs and AI applications', 'source': 'https://blog.logrocket.com/langsmith-test-llms-ai-applications/', 'score': 0.95984, 'images': None}),\n",
      "                       Document(page_content=\"BETA Sign Up\\nContact Sales\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter 🦜🔗 LangChain\\nLangSmith\\nLangServe\\nAgents\\nRetrieval\\nEvaluation\\nBlog\\nDocs\\n🦜🔗 LangChain\\nBuild and deploy LLM apps with confidence\\nAn all-in-one developer platform for every step of the application lifecycle.\\n Prompt playground\\nCross-team collaboration\\nCatalog of ranging models & tasks\\nProven prompting strategies\\nExplore LangChain Hub\\nTurn the magic of LLM applications into enterprise-ready products\\nNative collaboration\\nBring your team together in LangSmith to craft prompts, debug, and capture feedback.\\n Application-level usage stats\\nFeedback collection\\nFilter traces\\nCost measurement\\nPerformance comparison\\nGo To Docs\\nManage Prompts\\nPrompts power your team's chains and agents, and LangSmith allows you to refine, test, and version them in one place. Dataset curation\\nEvaluate chain performance\\nAI-assisted evaluation\\nEasy benchmarking\\nGo To Docs\\nMonitor\\nGiven the stochastic nature of LLMs, it can be hard to answer the simple question: “what’s happening with my application?”\", metadata={'title': 'LangSmith', 'source': 'https://www.langchain.com/langsmith', 'score': 0.94446, 'images': None}),\n",
      "                       Document(page_content='We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like.\\n Over the last few months, we’ve been working directly with some early design partners and testing it on our own internal workflows, and we’ve found LangSmith helps teams in 5 core ways:\\nDebugging\\nLangSmith gives you full visibility into model inputs and output of every step in the chain of events. As a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones.\\n A unified platform\\nWhile each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.”\\n', metadata={'title': 'Announcing LangSmith, a unified platform for debugging, testing ...', 'source': 'https://blog.langchain.dev/announcing-langsmith/', 'score': 0.93552, 'images': None}),\n",
      "                       Document(page_content=\"This was such an important unlock for us as we build in tactical functionality to our AI.\\nAdvice for Product Teams Considering LangSmith\\nWhen you're in the thick of product development, the whirlwind of AI and LLMs can be overwhelming. ou can now clearly see the citations coming through with the responses in the traces, and we’re good to ship the changes to the prompt to prod.\\n We do this primarily to legitimize the LLMs response and give our HelpHub customer’s peace of mind that their end users are in fact getting to the help resource they need (instead of an LLM just hallucinating and giving any response it wants.)\\n Take it from us, as we integrated AI more heavily and widely across our products, we’ve been more conscious than ever that the quality of the outputs matches the quality and trust that our customers have in our product. We updated our prompt to be more firm when asking for the sources:\\nWe then tested everything using LangSmith evals, to make sure that it fixes the issue before pushing to production.\\n\", metadata={'title': 'Peering Into the Soul of AI Decision-Making with LangSmith', 'source': 'https://blog.langchain.dev/peering-into-the-soul-of-ai-decision-making-with-langsmith/', 'score': 0.90045, 'images': None}),\n",
      "                       Document(page_content=\"LangSmith Cookbook: Real-world Lang Smith Examples\\nThe LangSmith Cookbook is not just a compilation of code snippets; it's a goldmine of hands-on examples designed to inspire and assist you in your projects. On This Page\\nLangSmith: Best Way to Test LLMs and AI Application\\nPublished on 12/17/2023\\nIf you're in the world of Language Learning Models (LLMs), you've probably heard of LangSmith. How to Download Feedback and Examples (opens in a new tab): Export predictions, evaluation results, and other information to add to your reports programmatically.\\n This article is your one-stop guide to understanding LangSmith, a platform that offers a plethora of features for debugging, testing, evaluating, and monitoring LLM applications.\\n How do I get access to LangSmith?\\nTo get access to LangSmith, you'll need to sign up for an account on their website.\", metadata={'title': 'LangSmith: Best Way to Test LLMs and AI Application', 'source': 'https://cheatsheet.md/langchain-tutorials/langsmith.en', 'score': 0.89128, 'images': None}),\n",
      "                       Document(page_content='A deep dive into the latest product from the creators of LangChain 🦜\\n--\\n3\\nCobus Greyling\\nLangSmith\\nI was fortunate to get early access to the LangSmith platform and in this article you will find practical code examples and demonstration…\\n--\\nLists\\nNatural Language Processing\\nPredictive Modeling w/ Python\\nAI Regulation\\nPractical Guides to Machine Learning\\nYule Wang, PhD\\nA Complete Guide to LLMs-based Autonomous Agents (Part I):\\n—\\u200a—\\u200aChain of Thought, Plan and Solve/Execute, Self-Ask, ReAct, Reflexion, Self-Consistency, Tree of Thoughts and Graph of Thoughts\\n--\\n1\\nRyan Nguyen\\nin\\nBetter Programming\\nLlamaIndex: How To Evaluate Your RAG (Retrieval Augmented Generation) Applications\\nClassic 10k reports with Australian real estate market reports to demonstrate the end-to-end evaluation process\\n--\\n2\\nKelvin Lu\\nTracing How LangChain Works Behind the Scenes\\nIntroduction of LLM Tracing technique with both LangChain built-in tracing funciton and LangChain Visualizer.\\n --\\nCameron R. Wolfe, Ph.D.\\nin\\nTowards Data Science\\nChain of Thought Prompting for LLMs\\nA practical and simple approach for “reasoning” with LLMs\\n--\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams A Step-by-Step Guide to Deploying ChromaDB in the Cloud\\n--\\nBenoit Ruiz\\nin\\nBetter Programming\\nAdvice From a Software Engineer With 8 Years of Experience\\nPractical tips for those who want to advance in their careers\\n--\\n233\\nVinita\\nin\\nBetter Programming\\n4 Strategies to Give Effective Feedback to a Difficult Person\\nRespect is like air. But if you take it away, it’s all that people can think about.\\n--\\n21\\nRicardo Ledan\\nin\\nBetter Programming\\nBuilding Context-Aware Question-Answering Systems With LLMs\\nA step-by-step guide to using embeddings, vector search, and prompt engineering for building context-aware question-answering systems\\n--\\n1\\nRecommended from Medium\\nLogan Kilpatrick\\nWhat is LangSmith and why should I care as a developer?\\n Ricardo Ledan\\nFollow\\nBetter Programming\\n--\\nShare\\nWhat is LangSmith?\\n', metadata={'title': 'The Art of LangSmithing - Better Programming', 'source': 'https://betterprogramming.pub/the-art-of-langsmithing-42dcd191a220', 'score': 0.84778, 'images': None}),\n",
      "                       Document(page_content='But what are the production challenges that need to be solved which were not as relevant in prototyping?\\nReliability — it is deceptively easy to build something that works well for a simple constrained example but actually still quite hard today to build LLM applications with the consistency that most companies would want.\\n for tools\\nKim Seon Woo - Nov 6\\nComo deixar o Swagger com tema dark mode usando NestJS\\nWiliam V. Joaquim - Nov 5\\nData visualizing\\nBIRINGANINE BASEME Destin\\n- Oct 7\\nFree Face-Swap AI 🎭 with Source Code for Web and Mobile App.\\nshadee22 - As you build more complex workflows, it can be hard to understand exactly what queries are moving through different flows so a simple UI to see this and log the historical data is going to be a value add from day one.\\n A huge part of the value add for LangSmith is being able to do all of these things from a simple and intuitive UI which significantly reduces the barrier to entry for those without a software background.\\n [Quick note: I am writing this article to reflect my personal views as I explore the LLM ecosystem and this is not intended to represent the views of my employer, hence being on my personal blog]\\nWhat is LangSmith?', metadata={'title': 'What is LangSmith and why should I care as a developer?', 'source': 'https://dev.to/logankilpatrick/what-is-langsmith-and-why-should-i-care-as-a-developer-19k', 'score': 0.84204, 'images': None}),\n",
      "                       Document(page_content=\"LangSmith lets you evaluate any LLM, chain, agent, or even a custom function. Conversational agents are stateful (they have memory); to ensure that this state isn't shared between dataset runs, we will pass in a chain_factory (aka a constructor) function to initialize for each call.\", metadata={'title': 'LangSmith Walkthrough - Google Colab', 'source': 'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', 'score': 0.8413, 'images': None}),\n",
      "                       Document(page_content='The Run tab for the top-level chain shows the human input, the parsed output, the latency (under a second), and the tokens used, as well as the clock time and call status.\\n The cookbook covers tracing your code without LangChain (using the @traceable decorator); using the LangChain Hub to discover, share, and version control prompts; testing and benchmarking your LLM systems in Python and TypeScript or JavaScript; using user feedback to improve, monitor, and personalize your applications; exporting data for fine-tuning; and exporting your run data for exploratory data analysis.\\n In addition to the input variables (prompt), an LLM call uses a template and often auxiliary functions; for example, retrieval of information from the web, uploaded files, and system prompts that set the context for the LLM.\\n After I set up my LangSmith account, created my API key, updated my LangChain installation with pip, and set up my shell environment variables, I tried to run the Python quickstart application:\\nBefore we discuss the results, take a look at the LangSmith hub:\\nFigure 1. Here are the evaluation statistics, which were printed in the terminal during the run:\\nSomebody had fun creating the rap battle prompts, as shown in the dataset below:\\nFigure 9.', metadata={'title': 'What is LangSmith? Tracing and debugging for LLMs | InfoWorld', 'source': 'https://www.infoworld.com/article/3708628/what-is-langsmith-tracing-and-debugging-for-llms.html', 'score': 0.80079, 'images': None})]}},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/RunnableSequence/end_time',\n",
      "  'value': '2024-04-16T15:56:23.903+00:00'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableParallel<context>/final_output',\n",
      "  'value': {'context': [Document(page_content='How it Works, Use Cases, Alternatives & More\\nRichie Cotton\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAdel Nehme\\n32 min\\nAn Introductory Guide to Fine-Tuning LLMs\\nJosep Ferrer\\n12 min\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nBex Tuychiev\\n15 min\\nGrow your data skills with DataCamp for Mobile\\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\\n For labeled datasets like the CSV dataset we uploaded, LangSmith offers more comprehensive evaluators for measuring the correctness of the response to a prompt:\\nLet’s try the last one on our examples:\\nCoTQA criterion returns a score called Contextual accuracy, as depicted in the GIF below (also in the UI):\\nPlease visit the LangChain evaluators section of LangSmith docs to learn much more about evaluators.\\n Once the upload finishes, the dataset will appear in the UI:\\nLet’s run our custom criterion from the previous section on this dataset as well:\\nIf you go to the dataset page and check out the run, we can see the average scores for each custom criteria:\\nEvaluating Labeled Datasets\\nBuilt-in and custom evaluators written in natural language are mostly for unlabeled datasets. We will use the create_dataset function of the client:\\nNow, let’s add three inputs that each ask the LLM to create a single flashcard:\\nIf you go over the dataset tab of the UI, you will see each prompt listed with NULL output:\\nNow, let’s run all the prompts in a single line of code using run_on_dataset function:\\n How it Works, Use Cases, Alternatives & More\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAn Introductory Guide to Fine-Tuning LLMs\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nStart Your Langchain Journey Today\\nCourse\\nDeveloping LLM Applications with LangChain\\nCourse\\nLarge Language Models (LLMs)', metadata={'title': 'An Introduction to Debugging And Testing LLMs in LangSmith', 'source': 'https://www.datacamp.com/tutorial/introduction-to-langsmith', 'score': 0.96075, 'images': None}),\n",
      "                        Document(page_content='This function helps to load the specific language models and tools required for the task as shown in the code snippet below:\\nAs a next step, initialize an agent by calling the initialize_agent function with several parameters like tools, llms, and agent:\\nThe verbose parameter is set to false, indicating that the agent will not provide verbose or detailed output.\\n You can accomplish this by following the shell commands provided below:\\nCreating a LangSmith client\\nNext, create a LangSmith client to interact with the API:\\nIf you’re using Python, run the following commands to import the module:\\n This code also handles exceptions that may occur during the agent execution:\\nIt’s also important to call the wait_for_all_tracers function from the langchain.callbacks.tracers.langchain module as shown in the code snippet below:\\nCalling the wait_for_all_tracers function helps ensure that logs and traces are submitted in full before the program proceeds. The temperature parameter will be set to 0, implying that the generated response will be more deterministic as shown in the code snippet below:\\nNow, let’s call the load_tools function with a list of tool APIs, such as serpapi and llm-math, and also take the llm instance as a parameter. It initializes a chat model, loads specific tools, and creates an agent that can generate responses based on descriptions:\\nInput processing with exception handling\\nThe code below defines a list of input examples using the asyncio library to asynchronously run the agent on each input and gather the results for further processing.', metadata={'title': 'Using LangSmith to test LLMs and AI applications', 'source': 'https://blog.logrocket.com/langsmith-test-llms-ai-applications/', 'score': 0.95984, 'images': None}),\n",
      "                        Document(page_content=\"BETA Sign Up\\nContact Sales\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter 🦜🔗 LangChain\\nLangSmith\\nLangServe\\nAgents\\nRetrieval\\nEvaluation\\nBlog\\nDocs\\n🦜🔗 LangChain\\nBuild and deploy LLM apps with confidence\\nAn all-in-one developer platform for every step of the application lifecycle.\\n Prompt playground\\nCross-team collaboration\\nCatalog of ranging models & tasks\\nProven prompting strategies\\nExplore LangChain Hub\\nTurn the magic of LLM applications into enterprise-ready products\\nNative collaboration\\nBring your team together in LangSmith to craft prompts, debug, and capture feedback.\\n Application-level usage stats\\nFeedback collection\\nFilter traces\\nCost measurement\\nPerformance comparison\\nGo To Docs\\nManage Prompts\\nPrompts power your team's chains and agents, and LangSmith allows you to refine, test, and version them in one place. Dataset curation\\nEvaluate chain performance\\nAI-assisted evaluation\\nEasy benchmarking\\nGo To Docs\\nMonitor\\nGiven the stochastic nature of LLMs, it can be hard to answer the simple question: “what’s happening with my application?”\", metadata={'title': 'LangSmith', 'source': 'https://www.langchain.com/langsmith', 'score': 0.94446, 'images': None}),\n",
      "                        Document(page_content='We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like.\\n Over the last few months, we’ve been working directly with some early design partners and testing it on our own internal workflows, and we’ve found LangSmith helps teams in 5 core ways:\\nDebugging\\nLangSmith gives you full visibility into model inputs and output of every step in the chain of events. As a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones.\\n A unified platform\\nWhile each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.”\\n', metadata={'title': 'Announcing LangSmith, a unified platform for debugging, testing ...', 'source': 'https://blog.langchain.dev/announcing-langsmith/', 'score': 0.93552, 'images': None}),\n",
      "                        Document(page_content=\"This was such an important unlock for us as we build in tactical functionality to our AI.\\nAdvice for Product Teams Considering LangSmith\\nWhen you're in the thick of product development, the whirlwind of AI and LLMs can be overwhelming. ou can now clearly see the citations coming through with the responses in the traces, and we’re good to ship the changes to the prompt to prod.\\n We do this primarily to legitimize the LLMs response and give our HelpHub customer’s peace of mind that their end users are in fact getting to the help resource they need (instead of an LLM just hallucinating and giving any response it wants.)\\n Take it from us, as we integrated AI more heavily and widely across our products, we’ve been more conscious than ever that the quality of the outputs matches the quality and trust that our customers have in our product. We updated our prompt to be more firm when asking for the sources:\\nWe then tested everything using LangSmith evals, to make sure that it fixes the issue before pushing to production.\\n\", metadata={'title': 'Peering Into the Soul of AI Decision-Making with LangSmith', 'source': 'https://blog.langchain.dev/peering-into-the-soul-of-ai-decision-making-with-langsmith/', 'score': 0.90045, 'images': None}),\n",
      "                        Document(page_content=\"LangSmith Cookbook: Real-world Lang Smith Examples\\nThe LangSmith Cookbook is not just a compilation of code snippets; it's a goldmine of hands-on examples designed to inspire and assist you in your projects. On This Page\\nLangSmith: Best Way to Test LLMs and AI Application\\nPublished on 12/17/2023\\nIf you're in the world of Language Learning Models (LLMs), you've probably heard of LangSmith. How to Download Feedback and Examples (opens in a new tab): Export predictions, evaluation results, and other information to add to your reports programmatically.\\n This article is your one-stop guide to understanding LangSmith, a platform that offers a plethora of features for debugging, testing, evaluating, and monitoring LLM applications.\\n How do I get access to LangSmith?\\nTo get access to LangSmith, you'll need to sign up for an account on their website.\", metadata={'title': 'LangSmith: Best Way to Test LLMs and AI Application', 'source': 'https://cheatsheet.md/langchain-tutorials/langsmith.en', 'score': 0.89128, 'images': None}),\n",
      "                        Document(page_content='A deep dive into the latest product from the creators of LangChain 🦜\\n--\\n3\\nCobus Greyling\\nLangSmith\\nI was fortunate to get early access to the LangSmith platform and in this article you will find practical code examples and demonstration…\\n--\\nLists\\nNatural Language Processing\\nPredictive Modeling w/ Python\\nAI Regulation\\nPractical Guides to Machine Learning\\nYule Wang, PhD\\nA Complete Guide to LLMs-based Autonomous Agents (Part I):\\n—\\u200a—\\u200aChain of Thought, Plan and Solve/Execute, Self-Ask, ReAct, Reflexion, Self-Consistency, Tree of Thoughts and Graph of Thoughts\\n--\\n1\\nRyan Nguyen\\nin\\nBetter Programming\\nLlamaIndex: How To Evaluate Your RAG (Retrieval Augmented Generation) Applications\\nClassic 10k reports with Australian real estate market reports to demonstrate the end-to-end evaluation process\\n--\\n2\\nKelvin Lu\\nTracing How LangChain Works Behind the Scenes\\nIntroduction of LLM Tracing technique with both LangChain built-in tracing funciton and LangChain Visualizer.\\n --\\nCameron R. Wolfe, Ph.D.\\nin\\nTowards Data Science\\nChain of Thought Prompting for LLMs\\nA practical and simple approach for “reasoning” with LLMs\\n--\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams A Step-by-Step Guide to Deploying ChromaDB in the Cloud\\n--\\nBenoit Ruiz\\nin\\nBetter Programming\\nAdvice From a Software Engineer With 8 Years of Experience\\nPractical tips for those who want to advance in their careers\\n--\\n233\\nVinita\\nin\\nBetter Programming\\n4 Strategies to Give Effective Feedback to a Difficult Person\\nRespect is like air. But if you take it away, it’s all that people can think about.\\n--\\n21\\nRicardo Ledan\\nin\\nBetter Programming\\nBuilding Context-Aware Question-Answering Systems With LLMs\\nA step-by-step guide to using embeddings, vector search, and prompt engineering for building context-aware question-answering systems\\n--\\n1\\nRecommended from Medium\\nLogan Kilpatrick\\nWhat is LangSmith and why should I care as a developer?\\n Ricardo Ledan\\nFollow\\nBetter Programming\\n--\\nShare\\nWhat is LangSmith?\\n', metadata={'title': 'The Art of LangSmithing - Better Programming', 'source': 'https://betterprogramming.pub/the-art-of-langsmithing-42dcd191a220', 'score': 0.84778, 'images': None}),\n",
      "                        Document(page_content='But what are the production challenges that need to be solved which were not as relevant in prototyping?\\nReliability — it is deceptively easy to build something that works well for a simple constrained example but actually still quite hard today to build LLM applications with the consistency that most companies would want.\\n for tools\\nKim Seon Woo - Nov 6\\nComo deixar o Swagger com tema dark mode usando NestJS\\nWiliam V. Joaquim - Nov 5\\nData visualizing\\nBIRINGANINE BASEME Destin\\n- Oct 7\\nFree Face-Swap AI 🎭 with Source Code for Web and Mobile App.\\nshadee22 - As you build more complex workflows, it can be hard to understand exactly what queries are moving through different flows so a simple UI to see this and log the historical data is going to be a value add from day one.\\n A huge part of the value add for LangSmith is being able to do all of these things from a simple and intuitive UI which significantly reduces the barrier to entry for those without a software background.\\n [Quick note: I am writing this article to reflect my personal views as I explore the LLM ecosystem and this is not intended to represent the views of my employer, hence being on my personal blog]\\nWhat is LangSmith?', metadata={'title': 'What is LangSmith and why should I care as a developer?', 'source': 'https://dev.to/logankilpatrick/what-is-langsmith-and-why-should-i-care-as-a-developer-19k', 'score': 0.84204, 'images': None}),\n",
      "                        Document(page_content=\"LangSmith lets you evaluate any LLM, chain, agent, or even a custom function. Conversational agents are stateful (they have memory); to ensure that this state isn't shared between dataset runs, we will pass in a chain_factory (aka a constructor) function to initialize for each call.\", metadata={'title': 'LangSmith Walkthrough - Google Colab', 'source': 'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', 'score': 0.8413, 'images': None}),\n",
      "                        Document(page_content='The Run tab for the top-level chain shows the human input, the parsed output, the latency (under a second), and the tokens used, as well as the clock time and call status.\\n The cookbook covers tracing your code without LangChain (using the @traceable decorator); using the LangChain Hub to discover, share, and version control prompts; testing and benchmarking your LLM systems in Python and TypeScript or JavaScript; using user feedback to improve, monitor, and personalize your applications; exporting data for fine-tuning; and exporting your run data for exploratory data analysis.\\n In addition to the input variables (prompt), an LLM call uses a template and often auxiliary functions; for example, retrieval of information from the web, uploaded files, and system prompts that set the context for the LLM.\\n After I set up my LangSmith account, created my API key, updated my LangChain installation with pip, and set up my shell environment variables, I tried to run the Python quickstart application:\\nBefore we discuss the results, take a look at the LangSmith hub:\\nFigure 1. Here are the evaluation statistics, which were printed in the terminal during the run:\\nSomebody had fun creating the rap battle prompts, as shown in the dataset below:\\nFigure 9.', metadata={'title': 'What is LangSmith? Tracing and debugging for LLMs | InfoWorld', 'source': 'https://www.infoworld.com/article/3708628/what-is-langsmith-tracing-and-debugging-for-llms.html', 'score': 0.80079, 'images': None})]}},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/RunnableParallel<context>/end_time',\n",
      "  'value': '2024-04-16T15:56:23.904+00:00'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableAssign<context>/final_output',\n",
      "  'value': {'context': [Document(page_content='How it Works, Use Cases, Alternatives & More\\nRichie Cotton\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAdel Nehme\\n32 min\\nAn Introductory Guide to Fine-Tuning LLMs\\nJosep Ferrer\\n12 min\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nBex Tuychiev\\n15 min\\nGrow your data skills with DataCamp for Mobile\\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\\n For labeled datasets like the CSV dataset we uploaded, LangSmith offers more comprehensive evaluators for measuring the correctness of the response to a prompt:\\nLet’s try the last one on our examples:\\nCoTQA criterion returns a score called Contextual accuracy, as depicted in the GIF below (also in the UI):\\nPlease visit the LangChain evaluators section of LangSmith docs to learn much more about evaluators.\\n Once the upload finishes, the dataset will appear in the UI:\\nLet’s run our custom criterion from the previous section on this dataset as well:\\nIf you go to the dataset page and check out the run, we can see the average scores for each custom criteria:\\nEvaluating Labeled Datasets\\nBuilt-in and custom evaluators written in natural language are mostly for unlabeled datasets. We will use the create_dataset function of the client:\\nNow, let’s add three inputs that each ask the LLM to create a single flashcard:\\nIf you go over the dataset tab of the UI, you will see each prompt listed with NULL output:\\nNow, let’s run all the prompts in a single line of code using run_on_dataset function:\\n How it Works, Use Cases, Alternatives & More\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAn Introductory Guide to Fine-Tuning LLMs\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nStart Your Langchain Journey Today\\nCourse\\nDeveloping LLM Applications with LangChain\\nCourse\\nLarge Language Models (LLMs)', metadata={'title': 'An Introduction to Debugging And Testing LLMs in LangSmith', 'source': 'https://www.datacamp.com/tutorial/introduction-to-langsmith', 'score': 0.96075, 'images': None}),\n",
      "                        Document(page_content='This function helps to load the specific language models and tools required for the task as shown in the code snippet below:\\nAs a next step, initialize an agent by calling the initialize_agent function with several parameters like tools, llms, and agent:\\nThe verbose parameter is set to false, indicating that the agent will not provide verbose or detailed output.\\n You can accomplish this by following the shell commands provided below:\\nCreating a LangSmith client\\nNext, create a LangSmith client to interact with the API:\\nIf you’re using Python, run the following commands to import the module:\\n This code also handles exceptions that may occur during the agent execution:\\nIt’s also important to call the wait_for_all_tracers function from the langchain.callbacks.tracers.langchain module as shown in the code snippet below:\\nCalling the wait_for_all_tracers function helps ensure that logs and traces are submitted in full before the program proceeds. The temperature parameter will be set to 0, implying that the generated response will be more deterministic as shown in the code snippet below:\\nNow, let’s call the load_tools function with a list of tool APIs, such as serpapi and llm-math, and also take the llm instance as a parameter. It initializes a chat model, loads specific tools, and creates an agent that can generate responses based on descriptions:\\nInput processing with exception handling\\nThe code below defines a list of input examples using the asyncio library to asynchronously run the agent on each input and gather the results for further processing.', metadata={'title': 'Using LangSmith to test LLMs and AI applications', 'source': 'https://blog.logrocket.com/langsmith-test-llms-ai-applications/', 'score': 0.95984, 'images': None}),\n",
      "                        Document(page_content=\"BETA Sign Up\\nContact Sales\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter 🦜🔗 LangChain\\nLangSmith\\nLangServe\\nAgents\\nRetrieval\\nEvaluation\\nBlog\\nDocs\\n🦜🔗 LangChain\\nBuild and deploy LLM apps with confidence\\nAn all-in-one developer platform for every step of the application lifecycle.\\n Prompt playground\\nCross-team collaboration\\nCatalog of ranging models & tasks\\nProven prompting strategies\\nExplore LangChain Hub\\nTurn the magic of LLM applications into enterprise-ready products\\nNative collaboration\\nBring your team together in LangSmith to craft prompts, debug, and capture feedback.\\n Application-level usage stats\\nFeedback collection\\nFilter traces\\nCost measurement\\nPerformance comparison\\nGo To Docs\\nManage Prompts\\nPrompts power your team's chains and agents, and LangSmith allows you to refine, test, and version them in one place. Dataset curation\\nEvaluate chain performance\\nAI-assisted evaluation\\nEasy benchmarking\\nGo To Docs\\nMonitor\\nGiven the stochastic nature of LLMs, it can be hard to answer the simple question: “what’s happening with my application?”\", metadata={'title': 'LangSmith', 'source': 'https://www.langchain.com/langsmith', 'score': 0.94446, 'images': None}),\n",
      "                        Document(page_content='We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like.\\n Over the last few months, we’ve been working directly with some early design partners and testing it on our own internal workflows, and we’ve found LangSmith helps teams in 5 core ways:\\nDebugging\\nLangSmith gives you full visibility into model inputs and output of every step in the chain of events. As a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones.\\n A unified platform\\nWhile each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.”\\n', metadata={'title': 'Announcing LangSmith, a unified platform for debugging, testing ...', 'source': 'https://blog.langchain.dev/announcing-langsmith/', 'score': 0.93552, 'images': None}),\n",
      "                        Document(page_content=\"This was such an important unlock for us as we build in tactical functionality to our AI.\\nAdvice for Product Teams Considering LangSmith\\nWhen you're in the thick of product development, the whirlwind of AI and LLMs can be overwhelming. ou can now clearly see the citations coming through with the responses in the traces, and we’re good to ship the changes to the prompt to prod.\\n We do this primarily to legitimize the LLMs response and give our HelpHub customer’s peace of mind that their end users are in fact getting to the help resource they need (instead of an LLM just hallucinating and giving any response it wants.)\\n Take it from us, as we integrated AI more heavily and widely across our products, we’ve been more conscious than ever that the quality of the outputs matches the quality and trust that our customers have in our product. We updated our prompt to be more firm when asking for the sources:\\nWe then tested everything using LangSmith evals, to make sure that it fixes the issue before pushing to production.\\n\", metadata={'title': 'Peering Into the Soul of AI Decision-Making with LangSmith', 'source': 'https://blog.langchain.dev/peering-into-the-soul-of-ai-decision-making-with-langsmith/', 'score': 0.90045, 'images': None}),\n",
      "                        Document(page_content=\"LangSmith Cookbook: Real-world Lang Smith Examples\\nThe LangSmith Cookbook is not just a compilation of code snippets; it's a goldmine of hands-on examples designed to inspire and assist you in your projects. On This Page\\nLangSmith: Best Way to Test LLMs and AI Application\\nPublished on 12/17/2023\\nIf you're in the world of Language Learning Models (LLMs), you've probably heard of LangSmith. How to Download Feedback and Examples (opens in a new tab): Export predictions, evaluation results, and other information to add to your reports programmatically.\\n This article is your one-stop guide to understanding LangSmith, a platform that offers a plethora of features for debugging, testing, evaluating, and monitoring LLM applications.\\n How do I get access to LangSmith?\\nTo get access to LangSmith, you'll need to sign up for an account on their website.\", metadata={'title': 'LangSmith: Best Way to Test LLMs and AI Application', 'source': 'https://cheatsheet.md/langchain-tutorials/langsmith.en', 'score': 0.89128, 'images': None}),\n",
      "                        Document(page_content='A deep dive into the latest product from the creators of LangChain 🦜\\n--\\n3\\nCobus Greyling\\nLangSmith\\nI was fortunate to get early access to the LangSmith platform and in this article you will find practical code examples and demonstration…\\n--\\nLists\\nNatural Language Processing\\nPredictive Modeling w/ Python\\nAI Regulation\\nPractical Guides to Machine Learning\\nYule Wang, PhD\\nA Complete Guide to LLMs-based Autonomous Agents (Part I):\\n—\\u200a—\\u200aChain of Thought, Plan and Solve/Execute, Self-Ask, ReAct, Reflexion, Self-Consistency, Tree of Thoughts and Graph of Thoughts\\n--\\n1\\nRyan Nguyen\\nin\\nBetter Programming\\nLlamaIndex: How To Evaluate Your RAG (Retrieval Augmented Generation) Applications\\nClassic 10k reports with Australian real estate market reports to demonstrate the end-to-end evaluation process\\n--\\n2\\nKelvin Lu\\nTracing How LangChain Works Behind the Scenes\\nIntroduction of LLM Tracing technique with both LangChain built-in tracing funciton and LangChain Visualizer.\\n --\\nCameron R. Wolfe, Ph.D.\\nin\\nTowards Data Science\\nChain of Thought Prompting for LLMs\\nA practical and simple approach for “reasoning” with LLMs\\n--\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams A Step-by-Step Guide to Deploying ChromaDB in the Cloud\\n--\\nBenoit Ruiz\\nin\\nBetter Programming\\nAdvice From a Software Engineer With 8 Years of Experience\\nPractical tips for those who want to advance in their careers\\n--\\n233\\nVinita\\nin\\nBetter Programming\\n4 Strategies to Give Effective Feedback to a Difficult Person\\nRespect is like air. But if you take it away, it’s all that people can think about.\\n--\\n21\\nRicardo Ledan\\nin\\nBetter Programming\\nBuilding Context-Aware Question-Answering Systems With LLMs\\nA step-by-step guide to using embeddings, vector search, and prompt engineering for building context-aware question-answering systems\\n--\\n1\\nRecommended from Medium\\nLogan Kilpatrick\\nWhat is LangSmith and why should I care as a developer?\\n Ricardo Ledan\\nFollow\\nBetter Programming\\n--\\nShare\\nWhat is LangSmith?\\n', metadata={'title': 'The Art of LangSmithing - Better Programming', 'source': 'https://betterprogramming.pub/the-art-of-langsmithing-42dcd191a220', 'score': 0.84778, 'images': None}),\n",
      "                        Document(page_content='But what are the production challenges that need to be solved which were not as relevant in prototyping?\\nReliability — it is deceptively easy to build something that works well for a simple constrained example but actually still quite hard today to build LLM applications with the consistency that most companies would want.\\n for tools\\nKim Seon Woo - Nov 6\\nComo deixar o Swagger com tema dark mode usando NestJS\\nWiliam V. Joaquim - Nov 5\\nData visualizing\\nBIRINGANINE BASEME Destin\\n- Oct 7\\nFree Face-Swap AI 🎭 with Source Code for Web and Mobile App.\\nshadee22 - As you build more complex workflows, it can be hard to understand exactly what queries are moving through different flows so a simple UI to see this and log the historical data is going to be a value add from day one.\\n A huge part of the value add for LangSmith is being able to do all of these things from a simple and intuitive UI which significantly reduces the barrier to entry for those without a software background.\\n [Quick note: I am writing this article to reflect my personal views as I explore the LLM ecosystem and this is not intended to represent the views of my employer, hence being on my personal blog]\\nWhat is LangSmith?', metadata={'title': 'What is LangSmith and why should I care as a developer?', 'source': 'https://dev.to/logankilpatrick/what-is-langsmith-and-why-should-i-care-as-a-developer-19k', 'score': 0.84204, 'images': None}),\n",
      "                        Document(page_content=\"LangSmith lets you evaluate any LLM, chain, agent, or even a custom function. Conversational agents are stateful (they have memory); to ensure that this state isn't shared between dataset runs, we will pass in a chain_factory (aka a constructor) function to initialize for each call.\", metadata={'title': 'LangSmith Walkthrough - Google Colab', 'source': 'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', 'score': 0.8413, 'images': None}),\n",
      "                        Document(page_content='The Run tab for the top-level chain shows the human input, the parsed output, the latency (under a second), and the tokens used, as well as the clock time and call status.\\n The cookbook covers tracing your code without LangChain (using the @traceable decorator); using the LangChain Hub to discover, share, and version control prompts; testing and benchmarking your LLM systems in Python and TypeScript or JavaScript; using user feedback to improve, monitor, and personalize your applications; exporting data for fine-tuning; and exporting your run data for exploratory data analysis.\\n In addition to the input variables (prompt), an LLM call uses a template and often auxiliary functions; for example, retrieval of information from the web, uploaded files, and system prompts that set the context for the LLM.\\n After I set up my LangSmith account, created my API key, updated my LangChain installation with pip, and set up my shell environment variables, I tried to run the Python quickstart application:\\nBefore we discuss the results, take a look at the LangSmith hub:\\nFigure 1. Here are the evaluation statistics, which were printed in the terminal during the run:\\nSomebody had fun creating the rap battle prompts, as shown in the dataset below:\\nFigure 9.', metadata={'title': 'What is LangSmith? Tracing and debugging for LLMs | InfoWorld', 'source': 'https://www.infoworld.com/article/3708628/what-is-langsmith-tracing-and-debugging-for-llms.html', 'score': 0.80079, 'images': None})],\n",
      "            'question': 'what is langsmith'}},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/RunnableAssign<context>/end_time',\n",
      "  'value': '2024-04-16T15:56:23.905+00:00'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatPromptTemplate',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': '99a15abd-9758-4133-8b15-6a7315dc2eac',\n",
      "            'metadata': {},\n",
      "            'name': 'ChatPromptTemplate',\n",
      "            'start_time': '2024-04-16T15:56:23.906+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['seq:step:2'],\n",
      "            'type': 'prompt'}})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatPromptTemplate/final_output',\n",
      "  'value': ChatPromptValue(messages=[HumanMessage(content='Answer the question based only on the context provided:\\n\\nContext: [Document(page_content=\\'How it Works, Use Cases, Alternatives & More\\\\nRichie Cotton\\\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\\\nAdel Nehme\\\\n32 min\\\\nAn Introductory Guide to Fine-Tuning LLMs\\\\nJosep Ferrer\\\\n12 min\\\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\\\nBex Tuychiev\\\\n15 min\\\\nGrow your data skills with DataCamp for Mobile\\\\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\\\\n For labeled datasets like the CSV dataset we uploaded, LangSmith offers more comprehensive evaluators for measuring the correctness of the response to a prompt:\\\\nLet’s try the last one on our examples:\\\\nCoTQA criterion returns a score called Contextual accuracy, as depicted in the GIF below (also in the UI):\\\\nPlease visit the LangChain evaluators section of LangSmith docs to learn much more about evaluators.\\\\n Once the upload finishes, the dataset will appear in the UI:\\\\nLet’s run our custom criterion from the previous section on this dataset as well:\\\\nIf you go to the dataset page and check out the run, we can see the average scores for each custom criteria:\\\\nEvaluating Labeled Datasets\\\\nBuilt-in and custom evaluators written in natural language are mostly for unlabeled datasets. We will use the create_dataset function of the client:\\\\nNow, let’s add three inputs that each ask the LLM to create a single flashcard:\\\\nIf you go over the dataset tab of the UI, you will see each prompt listed with NULL output:\\\\nNow, let’s run all the prompts in a single line of code using run_on_dataset function:\\\\n How it Works, Use Cases, Alternatives & More\\\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\\\nAn Introductory Guide to Fine-Tuning LLMs\\\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\\\nStart Your Langchain Journey Today\\\\nCourse\\\\nDeveloping LLM Applications with LangChain\\\\nCourse\\\\nLarge Language Models (LLMs)\\', metadata={\\'title\\': \\'An Introduction to Debugging And Testing LLMs in LangSmith\\', \\'source\\': \\'https://www.datacamp.com/tutorial/introduction-to-langsmith\\', \\'score\\': 0.96075, \\'images\\': None}), Document(page_content=\\'This function helps to load the specific language models and tools required for the task as shown in the code snippet below:\\\\nAs a next step, initialize an agent by calling the initialize_agent function with several parameters like tools, llms, and agent:\\\\nThe verbose parameter is set to false, indicating that the agent will not provide verbose or detailed output.\\\\n You can accomplish this by following the shell commands provided below:\\\\nCreating a LangSmith client\\\\nNext, create a LangSmith client to interact with the API:\\\\nIf you’re using Python, run the following commands to import the module:\\\\n This code also handles exceptions that may occur during the agent execution:\\\\nIt’s also important to call the wait_for_all_tracers function from the langchain.callbacks.tracers.langchain module as shown in the code snippet below:\\\\nCalling the wait_for_all_tracers function helps ensure that logs and traces are submitted in full before the program proceeds. The temperature parameter will be set to 0, implying that the generated response will be more deterministic as shown in the code snippet below:\\\\nNow, let’s call the load_tools function with a list of tool APIs, such as serpapi and llm-math, and also take the llm instance as a parameter. It initializes a chat model, loads specific tools, and creates an agent that can generate responses based on descriptions:\\\\nInput processing with exception handling\\\\nThe code below defines a list of input examples using the asyncio library to asynchronously run the agent on each input and gather the results for further processing.\\', metadata={\\'title\\': \\'Using LangSmith to test LLMs and AI applications\\', \\'source\\': \\'https://blog.logrocket.com/langsmith-test-llms-ai-applications/\\', \\'score\\': 0.95984, \\'images\\': None}), Document(page_content=\"BETA Sign Up\\\\nContact Sales\\\\nProducts and use-cases\\\\nLangChain\\\\nLangSmith\\\\nRetrieval\\\\nAgents\\\\nInspiration\\\\nCode\\\\nGitHub\\\\nLangChain Hub\\\\nPython Docs\\\\nJS/TS Docs\\\\nSocial\\\\nTwitter\\\\nDiscord\\\\nBlog\\\\nLinkedIn\\\\nYouTube\\\\nTerms of Service\\\\nSign up for our newsletter\\\\nProducts and use-cases\\\\nLangChain\\\\nLangSmith\\\\nRetrieval\\\\nAgents\\\\nInspiration\\\\nCode\\\\nGitHub\\\\nLangChain Hub\\\\nPython Docs\\\\nJS/TS Docs\\\\nSocial\\\\nTwitter\\\\nDiscord\\\\nBlog\\\\nLinkedIn\\\\nYouTube\\\\nTerms of Service\\\\nSign up for our newsletter\\\\nProducts and use-cases\\\\nLangChain\\\\nLangSmith\\\\nRetrieval\\\\nAgents\\\\nInspiration\\\\nCode\\\\nGitHub\\\\nLangChain Hub\\\\nPython Docs\\\\nJS/TS Docs\\\\nSocial\\\\nTwitter\\\\nDiscord\\\\nBlog\\\\nLinkedIn\\\\nYouTube\\\\nTerms of Service\\\\nSign up for our newsletter 🦜🔗 LangChain\\\\nLangSmith\\\\nLangServe\\\\nAgents\\\\nRetrieval\\\\nEvaluation\\\\nBlog\\\\nDocs\\\\n🦜🔗 LangChain\\\\nBuild and deploy LLM apps with confidence\\\\nAn all-in-one developer platform for every step of the application lifecycle.\\\\n Prompt playground\\\\nCross-team collaboration\\\\nCatalog of ranging models & tasks\\\\nProven prompting strategies\\\\nExplore LangChain Hub\\\\nTurn the magic of LLM applications into enterprise-ready products\\\\nNative collaboration\\\\nBring your team together in LangSmith to craft prompts, debug, and capture feedback.\\\\n Application-level usage stats\\\\nFeedback collection\\\\nFilter traces\\\\nCost measurement\\\\nPerformance comparison\\\\nGo To Docs\\\\nManage Prompts\\\\nPrompts power your team\\'s chains and agents, and LangSmith allows you to refine, test, and version them in one place. Dataset curation\\\\nEvaluate chain performance\\\\nAI-assisted evaluation\\\\nEasy benchmarking\\\\nGo To Docs\\\\nMonitor\\\\nGiven the stochastic nature of LLMs, it can be hard to answer the simple question: “what’s happening with my application?”\", metadata={\\'title\\': \\'LangSmith\\', \\'source\\': \\'https://www.langchain.com/langsmith\\', \\'score\\': 0.94446, \\'images\\': None}), Document(page_content=\\'We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like.\\\\n Over the last few months, we’ve been working directly with some early design partners and testing it on our own internal workflows, and we’ve found LangSmith helps teams in 5 core ways:\\\\nDebugging\\\\nLangSmith gives you full visibility into model inputs and output of every step in the chain of events. As a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones.\\\\n A unified platform\\\\nWhile each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.”\\\\n\\', metadata={\\'title\\': \\'Announcing LangSmith, a unified platform for debugging, testing ...\\', \\'source\\': \\'https://blog.langchain.dev/announcing-langsmith/\\', \\'score\\': 0.93552, \\'images\\': None}), Document(page_content=\"This was such an important unlock for us as we build in tactical functionality to our AI.\\\\nAdvice for Product Teams Considering LangSmith\\\\nWhen you\\'re in the thick of product development, the whirlwind of AI and LLMs can be overwhelming. ou can now clearly see the citations coming through with the responses in the traces, and we’re good to ship the changes to the prompt to prod.\\\\n We do this primarily to legitimize the LLMs response and give our HelpHub customer’s peace of mind that their end users are in fact getting to the help resource they need (instead of an LLM just hallucinating and giving any response it wants.)\\\\n Take it from us, as we integrated AI more heavily and widely across our products, we’ve been more conscious than ever that the quality of the outputs matches the quality and trust that our customers have in our product. We updated our prompt to be more firm when asking for the sources:\\\\nWe then tested everything using LangSmith evals, to make sure that it fixes the issue before pushing to production.\\\\n\", metadata={\\'title\\': \\'Peering Into the Soul of AI Decision-Making with LangSmith\\', \\'source\\': \\'https://blog.langchain.dev/peering-into-the-soul-of-ai-decision-making-with-langsmith/\\', \\'score\\': 0.90045, \\'images\\': None}), Document(page_content=\"LangSmith Cookbook: Real-world Lang Smith Examples\\\\nThe LangSmith Cookbook is not just a compilation of code snippets; it\\'s a goldmine of hands-on examples designed to inspire and assist you in your projects. On This Page\\\\nLangSmith: Best Way to Test LLMs and AI Application\\\\nPublished on 12/17/2023\\\\nIf you\\'re in the world of Language Learning Models (LLMs), you\\'ve probably heard of LangSmith. How to Download Feedback and Examples (opens in a new tab): Export predictions, evaluation results, and other information to add to your reports programmatically.\\\\n This article is your one-stop guide to understanding LangSmith, a platform that offers a plethora of features for debugging, testing, evaluating, and monitoring LLM applications.\\\\n How do I get access to LangSmith?\\\\nTo get access to LangSmith, you\\'ll need to sign up for an account on their website.\", metadata={\\'title\\': \\'LangSmith: Best Way to Test LLMs and AI Application\\', \\'source\\': \\'https://cheatsheet.md/langchain-tutorials/langsmith.en\\', \\'score\\': 0.89128, \\'images\\': None}), Document(page_content=\\'A deep dive into the latest product from the creators of LangChain 🦜\\\\n--\\\\n3\\\\nCobus Greyling\\\\nLangSmith\\\\nI was fortunate to get early access to the LangSmith platform and in this article you will find practical code examples and demonstration…\\\\n--\\\\nLists\\\\nNatural Language Processing\\\\nPredictive Modeling w/ Python\\\\nAI Regulation\\\\nPractical Guides to Machine Learning\\\\nYule Wang, PhD\\\\nA Complete Guide to LLMs-based Autonomous Agents (Part I):\\\\n—\\\\u200a—\\\\u200aChain of Thought, Plan and Solve/Execute, Self-Ask, ReAct, Reflexion, Self-Consistency, Tree of Thoughts and Graph of Thoughts\\\\n--\\\\n1\\\\nRyan Nguyen\\\\nin\\\\nBetter Programming\\\\nLlamaIndex: How To Evaluate Your RAG (Retrieval Augmented Generation) Applications\\\\nClassic 10k reports with Australian real estate market reports to demonstrate the end-to-end evaluation process\\\\n--\\\\n2\\\\nKelvin Lu\\\\nTracing How LangChain Works Behind the Scenes\\\\nIntroduction of LLM Tracing technique with both LangChain built-in tracing funciton and LangChain Visualizer.\\\\n --\\\\nCameron R. Wolfe, Ph.D.\\\\nin\\\\nTowards Data Science\\\\nChain of Thought Prompting for LLMs\\\\nA practical and simple approach for “reasoning” with LLMs\\\\n--\\\\nHelp\\\\nStatus\\\\nAbout\\\\nCareers\\\\nBlog\\\\nPrivacy\\\\nTerms\\\\nText to speech\\\\nTeams A Step-by-Step Guide to Deploying ChromaDB in the Cloud\\\\n--\\\\nBenoit Ruiz\\\\nin\\\\nBetter Programming\\\\nAdvice From a Software Engineer With 8 Years of Experience\\\\nPractical tips for those who want to advance in their careers\\\\n--\\\\n233\\\\nVinita\\\\nin\\\\nBetter Programming\\\\n4 Strategies to Give Effective Feedback to a Difficult Person\\\\nRespect is like air. But if you take it away, it’s all that people can think about.\\\\n--\\\\n21\\\\nRicardo Ledan\\\\nin\\\\nBetter Programming\\\\nBuilding Context-Aware Question-Answering Systems With LLMs\\\\nA step-by-step guide to using embeddings, vector search, and prompt engineering for building context-aware question-answering systems\\\\n--\\\\n1\\\\nRecommended from Medium\\\\nLogan Kilpatrick\\\\nWhat is LangSmith and why should I care as a developer?\\\\n Ricardo Ledan\\\\nFollow\\\\nBetter Programming\\\\n--\\\\nShare\\\\nWhat is LangSmith?\\\\n\\', metadata={\\'title\\': \\'The Art of LangSmithing - Better Programming\\', \\'source\\': \\'https://betterprogramming.pub/the-art-of-langsmithing-42dcd191a220\\', \\'score\\': 0.84778, \\'images\\': None}), Document(page_content=\\'But what are the production challenges that need to be solved which were not as relevant in prototyping?\\\\nReliability — it is deceptively easy to build something that works well for a simple constrained example but actually still quite hard today to build LLM applications with the consistency that most companies would want.\\\\n for tools\\\\nKim Seon Woo - Nov 6\\\\nComo deixar o Swagger com tema dark mode usando NestJS\\\\nWiliam V. Joaquim - Nov 5\\\\nData visualizing\\\\nBIRINGANINE BASEME Destin\\\\n- Oct 7\\\\nFree Face-Swap AI 🎭 with Source Code for Web and Mobile App.\\\\nshadee22 - As you build more complex workflows, it can be hard to understand exactly what queries are moving through different flows so a simple UI to see this and log the historical data is going to be a value add from day one.\\\\n A huge part of the value add for LangSmith is being able to do all of these things from a simple and intuitive UI which significantly reduces the barrier to entry for those without a software background.\\\\n [Quick note: I am writing this article to reflect my personal views as I explore the LLM ecosystem and this is not intended to represent the views of my employer, hence being on my personal blog]\\\\nWhat is LangSmith?\\', metadata={\\'title\\': \\'What is LangSmith and why should I care as a developer?\\', \\'source\\': \\'https://dev.to/logankilpatrick/what-is-langsmith-and-why-should-i-care-as-a-developer-19k\\', \\'score\\': 0.84204, \\'images\\': None}), Document(page_content=\"LangSmith lets you evaluate any LLM, chain, agent, or even a custom function. Conversational agents are stateful (they have memory); to ensure that this state isn\\'t shared between dataset runs, we will pass in a chain_factory (aka a constructor) function to initialize for each call.\", metadata={\\'title\\': \\'LangSmith Walkthrough - Google Colab\\', \\'source\\': \\'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb\\', \\'score\\': 0.8413, \\'images\\': None}), Document(page_content=\\'The Run tab for the top-level chain shows the human input, the parsed output, the latency (under a second), and the tokens used, as well as the clock time and call status.\\\\n The cookbook covers tracing your code without LangChain (using the @traceable decorator); using the LangChain Hub to discover, share, and version control prompts; testing and benchmarking your LLM systems in Python and TypeScript or JavaScript; using user feedback to improve, monitor, and personalize your applications; exporting data for fine-tuning; and exporting your run data for exploratory data analysis.\\\\n In addition to the input variables (prompt), an LLM call uses a template and often auxiliary functions; for example, retrieval of information from the web, uploaded files, and system prompts that set the context for the LLM.\\\\n After I set up my LangSmith account, created my API key, updated my LangChain installation with pip, and set up my shell environment variables, I tried to run the Python quickstart application:\\\\nBefore we discuss the results, take a look at the LangSmith hub:\\\\nFigure 1. Here are the evaluation statistics, which were printed in the terminal during the run:\\\\nSomebody had fun creating the rap battle prompts, as shown in the dataset below:\\\\nFigure 9.\\', metadata={\\'title\\': \\'What is LangSmith? Tracing and debugging for LLMs | InfoWorld\\', \\'source\\': \\'https://www.infoworld.com/article/3708628/what-is-langsmith-tracing-and-debugging-for-llms.html\\', \\'score\\': 0.80079, \\'images\\': None})]\\n\\nQuestion: what is langsmith')])},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatPromptTemplate/end_time',\n",
      "  'value': '2024-04-16T15:56:23.907+00:00'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': '117fbf49-d6fb-4474-91d4-56252f0adb7b',\n",
      "            'metadata': {},\n",
      "            'name': 'ChatOpenAI',\n",
      "            'start_time': '2024-04-16T15:56:23.908+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['seq:step:3'],\n",
      "            'type': 'llm'}})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '\\n'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='\\n', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': '6a345491-c21c-4cda-bd4c-714263e60ab8',\n",
      "            'metadata': {},\n",
      "            'name': 'StrOutputParser',\n",
      "            'start_time': '2024-04-16T15:56:26.400+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['seq:step:4'],\n",
      "            'type': 'parser'}})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '\\n'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': '\\n'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'Call'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='Call', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'Call'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Call'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': '\\nCall'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ':'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=':', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ':'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ':'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': '\\nCall:'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' Document'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' Document', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' Document'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Document'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': '\\nCall: Document'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '('},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='(', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '('})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '('},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': '\\nCall: Document('})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'page'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='page', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'page'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'page'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': '\\nCall: Document(page'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '_'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='_', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '_'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '_'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': '\\nCall: Document(page_'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'content'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='content', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'content'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'content'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\nCall: Document(page_content'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \"='\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\"='\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \"='\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"='\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: Document(page_content='\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'The'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='The', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'The'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'The'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: Document(page_content='The\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' Run'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' Run', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' Run'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Run'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: Document(page_content='The Run\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' tab'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' tab', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' tab'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' tab'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: Document(page_content='The Run tab\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' for'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' for', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' for'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' for'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: Document(page_content='The Run tab for\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: Document(page_content='The Run tab for the\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' top'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' top', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' top'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' top'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: Document(page_content='The Run tab for the top\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '-'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='-', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '-'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '-'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: Document(page_content='The Run tab for the top-\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'level'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='level', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'level'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'level'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: Document(page_content='The Run tab for the top-level\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' chain'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' chain', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' chain'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' chain'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' shows'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' shows', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' shows'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' shows'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' human'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' human', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' human'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' human'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' input'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' input', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' input'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' input'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' parsed'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' parsed', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' parsed'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' parsed'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' output'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' output', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' output'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' output'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' laten'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' laten', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' laten'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' laten'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the laten'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'cy'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='cy', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'cy'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'cy'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' ('},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' (', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' ('})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' ('},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency ('})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'under'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='under', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'under'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'under'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' a'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' a', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' a'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' a'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' second'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' second', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' second'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' second'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '),'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='),', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '),'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '),'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second),'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' tokens'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' tokens', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' tokens'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' tokens'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' used'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' used', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' used'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' used'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' as'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' as', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' as'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' as'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' well'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' well', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' well'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' well'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' as'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' as', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' as'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' as'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' clock'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' clock', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' clock'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' clock'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' time'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' time', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' time'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' time'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' call'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' call', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' call'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' call'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' status'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' status', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' status'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' status'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': '.\\\\'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='.\\\\', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '.\\\\'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.\\\\'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'n'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='n', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': 'n'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'n'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' The'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' The', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' The'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' The'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' cook'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' cook', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' cook'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' cook'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cook'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'book'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='book', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'book'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'book'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' covers'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' covers', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' covers'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' covers'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' trac'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' trac', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' trac'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' trac'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers trac'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ing'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ing', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ing'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ing'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' your'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' your', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' your'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' your'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' code'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' code', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' code'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' code'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' without'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' without', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' without'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' without'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' Lang'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' Lang', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' Lang'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Lang'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without Lang'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'Chain'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='Chain', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'Chain'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Chain'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without '\n",
      "           'LangChain'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' ('},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' (', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' ('})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' ('},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '('})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'using'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='using', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'using'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'using'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' @'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' @', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' @'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' @'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'trace'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='trace', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'trace'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'trace'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @trace'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'able'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='able', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'able'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'able'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' decor'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' decor', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' decor'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' decor'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decor'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ator'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ator', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ator'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ator'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ');'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=');', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ');'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ');'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator);'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' using'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' using', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' using'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' using'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' Lang'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' Lang', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' Lang'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Lang'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the Lang'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'Chain'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='Chain', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'Chain'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Chain'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' Hub'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' Hub', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' Hub'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Hub'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' to'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' to', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' to'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' to'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' discover'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' discover', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' discover'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' discover'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' share'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' share', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' share'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' share'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' version'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' version', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' version'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' version'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' control'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' control', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' control'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' control'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' prompt'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' prompt', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' prompt'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' prompt'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompt'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 's'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='s', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': 's'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 's'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ';'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=';', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ';'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ';'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts;'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' testing'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' testing', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' testing'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' testing'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' benchmark'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' benchmark', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' benchmark'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' benchmark'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmark'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ing'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ing', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ing'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ing'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' your'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' your', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' your'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' your'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' L'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' L', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' L'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' L'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your L'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'LM'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='LM', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'LM'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'LM'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' systems'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' systems', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' systems'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' systems'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' in'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' in', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' in'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' in'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' Python'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' Python', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' Python'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Python'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' Type'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' Type', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' Type'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Type'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and Type'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'Script'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='Script', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'Script'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Script'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' or'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' or', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' or'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' or'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' JavaScript'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' JavaScript', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' JavaScript'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' JavaScript'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ';'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=';', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ';'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ';'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript;'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' using'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' using', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' using'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' using'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' user'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' user', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' user'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' user'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' feedback'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' feedback', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' feedback'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' feedback'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' to'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' to', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' to'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' to'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' improve'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' improve', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' improve'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' improve'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' monitor'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' monitor', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' monitor'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' monitor'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' personal'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' personal', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' personal'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' personal'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and personal'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ize'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ize', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ize'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ize'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' your'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' your', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' your'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' your'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' applications'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' applications', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' applications'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' applications'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ';'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=';', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ';'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ';'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications;'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' export'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' export', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' export'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' export'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; export'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ing'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ing', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ing'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ing'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' data'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' data', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' data'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' data'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' for'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' for', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' for'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' for'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' fine'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' fine', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' fine'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' fine'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '-'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='-', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '-'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '-'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 't'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='t', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': 't'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 't'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-t'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'uning'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='uning', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'uning'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'uning'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ';'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=';', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ';'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ';'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning;'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; '\n",
      "           'and'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' export'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' export', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' export'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' export'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'export'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ing'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ing', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ing'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ing'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' your'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' your', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' your'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' your'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' run'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' run', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' run'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' run'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' data'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' data', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' data'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' data'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' for'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' for', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' for'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' for'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' expl'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' expl', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' expl'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' expl'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for expl'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'or'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='or', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'or'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'or'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for explor'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'atory'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='atory', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'atory'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'atory'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' data'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' data', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' data'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' data'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' analysis'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' analysis', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' analysis'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' analysis'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': '.\\\\'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='.\\\\', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '.\\\\'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.\\\\'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'n'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='n', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': 'n'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'n'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' In'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' In', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' In'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' In'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' addition'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' addition', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' addition'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' addition'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' to'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' to', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' to'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' to'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' input'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' input', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' input'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' input'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' variables'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' variables', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' variables'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' variables'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' ('},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' (', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' ('})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' ('},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables ('})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'prom'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='prom', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'prom'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'prom'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prom'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'pt'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='pt', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'pt'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'pt'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '),'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='),', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '),'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '),'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt),'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' an'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' an', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' an'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' an'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' L'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' L', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' L'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' L'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an L'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'LM'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='LM', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'LM'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'LM'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' call'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' call', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' call'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' call'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' uses'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' uses', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' uses'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' uses'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' a'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' a', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' a'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' a'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' template'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' template', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' template'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' template'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' often'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' often', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' often'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' often'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' auxili'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' auxili', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' auxili'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' auxili'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxili'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ary'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ary', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ary'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ary'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' functions'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' functions', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' functions'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' functions'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ';'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=';', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ';'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ';'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions;'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' for'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' for', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' for'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' for'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' example'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' example', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' example'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' example'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' retr'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' retr', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' retr'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' retr'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retr'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ieval'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ieval', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ieval'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ieval'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' of'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' of', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' of'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' of'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' information'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' information', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' information'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' information'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' from'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' from', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' from'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' from'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' web'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' web', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' web'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' web'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' uploaded'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' uploaded', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' uploaded'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' uploaded'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' files'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' files', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' files'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' files'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' system'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' system', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' system'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' system'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' prompt'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' prompt', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' prompt'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' prompt'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompt'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 's'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='s', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': 's'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 's'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' that'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' that', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' that'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' that'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' set'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' set', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' set'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' set'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           'set'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           'set the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' context'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' context', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' context'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' context'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           'set the context'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' for'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' for', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' for'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' for'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           'set the context for'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           'set the context for the'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' L'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' L', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' L'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' L'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           'set the context for the L'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'LM'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='LM', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'LM'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'LM'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           'set the context for the LLM'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '.'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='.', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '.'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           'set the context for the LLM.'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \"',\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\"',\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \"',\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"',\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.',\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' metadata'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' metadata', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' metadata'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' metadata'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '={'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='={', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '={'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '={'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \"'\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\"'\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': \"'\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"'\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'title'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='title', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'title'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'title'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \"':\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\"':\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \"':\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"':\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title':\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \" '\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\" '\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \" '\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \" '\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': '\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'L'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='L', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': 'L'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'L'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'L\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ang'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ang', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ang'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ang'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'Lang\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'Smith'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='Smith', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'Smith'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Smith'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' Walk'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' Walk', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' Walk'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Walk'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith Walk\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'through'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='through', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'through'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'through'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           'Walkthrough'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' -'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' -', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' -'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' -'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           'Walkthrough -'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' Google'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' Google', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' Google'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Google'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           'Walkthrough - Google'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' Col'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' Col', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' Col'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Col'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           'Walkthrough - Google Col'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'ab'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ab', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ab'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ab'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           'Walkthrough - Google Colab'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \"',\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\"',\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \"',\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"',\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab',\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \" '\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\" '\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \" '\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \" '\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', '\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'source'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='source', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'source'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'source'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \"':\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\"':\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \"':\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"':\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source':\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \" '\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\" '\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \" '\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \" '\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': '\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'https'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='https', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'https'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'https'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': 'https\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': '://'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='://', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '://'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '://'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': 'https://\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'col'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='col', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'col'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'col'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': 'https://col\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'ab'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ab', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ab'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ab'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': 'https://colab\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '.'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='.', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '.'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': 'https://colab.\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'res'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='res', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'res'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'res'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': 'https://colab.res\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'earch'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='earch', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'earch'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'earch'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': 'https://colab.research\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '.'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='.', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '.'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': 'https://colab.research.\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'google'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='google', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'google'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'google'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '.'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='.', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '.'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'com'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='com', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'com'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'com'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '/'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='/', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '/'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '/'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'github'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='github', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'github'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'github'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '/'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='/', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '/'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '/'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'lang'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='lang', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'lang'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'lang'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/lang\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'chain'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='chain', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'chain'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'chain'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '-'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='-', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '-'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '-'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'ai'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ai', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ai'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ai'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '/'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='/', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '/'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '/'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'lang'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='lang', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'lang'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'lang'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/lang\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'chain'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='chain', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'chain'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'chain'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '/'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='/', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '/'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '/'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'blob'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='blob', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'blob'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'blob'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '/'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='/', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '/'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '/'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'master'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='master', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'master'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'master'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '/'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='/', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '/'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '/'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'docs'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='docs', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'docs'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'docs'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '/'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='/', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '/'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '/'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'docs'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='docs', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'docs'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'docs'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '/'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='/', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '/'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '/'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'lang'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='lang', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'lang'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'lang'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/lang\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'sm'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='sm', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'sm'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'sm'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsm\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ith'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ith', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ith'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ith'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '/'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='/', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '/'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '/'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'walk'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='walk', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'walk'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'walk'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walk\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'through'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='through', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'through'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'through'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '.'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='.', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '.'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'ip'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ip', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ip'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ip'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ip\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'yn'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='yn', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'yn'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'yn'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipyn\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'b'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='b', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': 'b'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'b'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \"',\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\"',\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \"',\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"',\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb',\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \" '\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\" '\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \" '\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \" '\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'score'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='score', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'score'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'score'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \"':\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\"':\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \"':\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"':\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score':\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' '},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' ', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ' '})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' '},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': \"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '0'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='0', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '0'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '0'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '.'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='.', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '.'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '8'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='8', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '8'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '8'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '4'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='4', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '4'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '4'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.84\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '1'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='1', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '1'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '1'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.841\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '3'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='3', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '3'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '3'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413,\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \" '\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\" '\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \" '\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \" '\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, '\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'images'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='images', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'images'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'images'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \"':\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\"':\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \"':\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"':\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images':\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' None'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' None', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' None'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' None'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '})'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='})', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '})'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '})'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': '<bot_end>'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='<bot_end>', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '<bot_end>'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '<bot_end>'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '\\n'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='\\n', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '\\n'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'Th'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='Th', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'Th'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Th'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           'Th'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ought'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ought', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ought'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ought'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           'Thought'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ':'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=':', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ':'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ':'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           'Thought:'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' The'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' The', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' The'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' The'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           'Thought: The'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' function'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' function', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' function'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' function'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           'Thought: The function'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' call'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' call', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' call'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' call'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           'Thought: The function call'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' `'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' `', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' `'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' `'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           'Thought: The function call `'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'Document'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='Document', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'Document'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Document'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           'Thought: The function call `Document'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '('},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='(', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '('})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '('},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           'Thought: The function call `Document('})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'page'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='page', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'page'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'page'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           'Thought: The function call `Document(page'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '_'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='_', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '_'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '_'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           'Thought: The function call `Document(page_'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'content'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='content', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'content'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'content'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           'Thought: The function call `Document(page_content'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \"='\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\"='\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \"='\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"='\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'The'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='The', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'The'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'The'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' Run'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' Run', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' Run'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Run'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' tab'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' tab', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' tab'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' tab'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' for'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' for', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' for'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' for'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab \"\n",
      "           'for'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' top'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' top', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' top'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' top'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '-'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='-', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '-'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '-'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'level'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='level', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'level'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'level'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' chain'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' chain', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' chain'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' chain'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' shows'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' shows', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' shows'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' shows'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' human'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' human', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' human'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' human'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' input'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' input', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' input'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' input'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' parsed'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' parsed', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' parsed'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' parsed'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' output'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' output', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' output'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' output'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' laten'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' laten', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' laten'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' laten'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'laten'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'cy'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='cy', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'cy'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'cy'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' ('},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' (', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' ('})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' ('},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency ('})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'under'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='under', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'under'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'under'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' a'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' a', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' a'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' a'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' second'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' second', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' second'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' second'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '),'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='),', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '),'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '),'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second),'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' tokens'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' tokens', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' tokens'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' tokens'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' used'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' used', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' used'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' used'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' as'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' as', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' as'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' as'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' well'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' well', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' well'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' well'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' as'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' as', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' as'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' as'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' clock'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' clock', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' clock'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' clock'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' time'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' time', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' time'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' time'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' call'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' call', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' call'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' call'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' status'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' status', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' status'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' status'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': '.\\\\'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='.\\\\', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '.\\\\'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.\\\\'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'n'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='n', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': 'n'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'n'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' The'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' The', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' The'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' The'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' cook'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' cook', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' cook'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' cook'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cook'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'book'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='book', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'book'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'book'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' covers'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' covers', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' covers'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' covers'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' trac'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' trac', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' trac'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' trac'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers trac'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ing'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ing', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ing'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ing'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' your'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' your', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' your'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' your'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' code'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' code', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' code'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' code'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' without'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' without', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' without'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' without'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' Lang'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' Lang', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' Lang'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Lang'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without Lang'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'Chain'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='Chain', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'Chain'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Chain'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' ('},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' (', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' ('})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' ('},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain ('})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'using'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='using', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'using'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'using'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' @'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' @', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' @'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' @'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'trace'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='trace', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'trace'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'trace'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @trace'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'able'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='able', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'able'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'able'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' decor'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' decor', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' decor'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' decor'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decor'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ator'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ator', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ator'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ator'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ');'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=');', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ');'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ');'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator);'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' using'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' using', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' using'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' using'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using '\n",
      "           'the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' Lang'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' Lang', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' Lang'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Lang'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'Lang'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'Chain'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='Chain', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'Chain'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Chain'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' Hub'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' Hub', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' Hub'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Hub'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' to'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' to', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' to'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' to'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' discover'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' discover', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' discover'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' discover'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' share'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' share', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' share'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' share'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' version'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' version', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' version'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' version'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' control'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' control', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' control'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' control'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' prompt'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' prompt', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' prompt'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' prompt'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompt'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 's'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='s', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': 's'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 's'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ';'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=';', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ';'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ';'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts;'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' testing'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' testing', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' testing'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' testing'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' benchmark'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' benchmark', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' benchmark'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' benchmark'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmark'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ing'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ing', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ing'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ing'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' your'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' your', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' your'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' your'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' L'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' L', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' L'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' L'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your L'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'LM'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='LM', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'LM'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'LM'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' systems'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' systems', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' systems'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' systems'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' in'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' in', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' in'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' in'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' Python'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' Python', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' Python'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Python'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' Type'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' Type', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' Type'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Type'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and Type'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'Script'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='Script', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'Script'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Script'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and '\n",
      "           'TypeScript'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' or'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' or', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' or'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' or'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' JavaScript'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' JavaScript', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' JavaScript'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' JavaScript'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ';'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=';', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ';'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ';'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript;'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' using'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' using', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' using'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' using'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' user'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' user', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' user'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' user'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' feedback'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' feedback', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' feedback'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' feedback'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' to'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' to', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' to'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' to'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' improve'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' improve', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' improve'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' improve'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' monitor'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' monitor', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' monitor'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' monitor'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' personal'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' personal', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' personal'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' personal'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personal'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ize'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ize', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ize'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ize'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' your'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' your', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' your'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' your'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' applications'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' applications', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' applications'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' applications'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ';'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=';', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ';'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ';'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications;'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' export'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' export', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' export'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' export'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; export'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ing'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ing', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ing'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ing'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' data'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' data', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' data'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' data'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' for'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' for', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' for'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' for'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' fine'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' fine', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' fine'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' fine'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '-'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='-', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '-'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '-'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 't'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='t', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': 't'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 't'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-t'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'uning'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='uning', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'uning'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'uning'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ';'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=';', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ';'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ';'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning;'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; '\n",
      "           'and'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' export'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' export', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' export'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' export'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'export'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ing'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ing', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ing'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ing'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' your'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' your', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' your'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' your'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' run'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' run', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' run'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' run'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' data'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' data', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' data'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' data'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' for'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' for', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' for'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' for'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' expl'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' expl', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' expl'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' expl'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for expl'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'or'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='or', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'or'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'or'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for explor'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'atory'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='atory', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'atory'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'atory'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' data'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' data', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' data'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' data'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' analysis'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' analysis', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' analysis'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' analysis'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': '.\\\\'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='.\\\\', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '.\\\\'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.\\\\'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'n'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='n', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': 'n'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'n'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' In'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' In', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' In'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' In'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' addition'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' addition', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' addition'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' addition'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' to'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' to', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' to'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' to'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' input'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' input', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' input'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' input'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' variables'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' variables', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' variables'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' variables'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' ('},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' (', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' ('})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' ('},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables ('})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'prom'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='prom', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'prom'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'prom'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prom'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'pt'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='pt', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'pt'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'pt'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '),'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='),', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '),'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '),'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt),'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' an'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' an', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' an'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' an'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' L'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' L', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' L'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' L'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an L'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'LM'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='LM', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'LM'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'LM'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' call'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' call', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' call'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' call'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' uses'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' uses', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' uses'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' uses'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' a'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' a', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' a'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' a'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' template'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' template', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' template'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' template'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' often'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' often', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' often'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' often'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' auxili'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' auxili', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' auxili'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' auxili'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxili'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ary'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ary', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ary'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ary'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' functions'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' functions', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' functions'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' functions'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ';'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=';', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ';'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ';'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions;'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' for'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' for', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' for'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' for'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' example'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' example', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' example'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' example'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' retr'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' retr', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' retr'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' retr'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retr'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ieval'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ieval', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ieval'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ieval'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' of'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' of', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' of'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' of'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' information'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' information', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' information'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' information'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' from'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' from', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' from'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' from'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' web'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' web', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' web'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' web'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' uploaded'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' uploaded', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' uploaded'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' uploaded'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' files'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' files', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' files'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' files'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' system'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' system', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' system'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' system'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' prompt'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' prompt', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' prompt'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' prompt'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompt'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 's'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='s', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': 's'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 's'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' that'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' that', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' that'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' that'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' set'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' set', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' set'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' set'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           'set'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           'set the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' context'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' context', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' context'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' context'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           'set the context'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' for'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' for', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' for'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' for'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           'set the context for'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           'set the context for the'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' L'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' L', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' L'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' L'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           'set the context for the L'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'LM'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='LM', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'LM'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'LM'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           'set the context for the LLM'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '.'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='.', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '.'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           'set the context for the LLM.'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \"',\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\"',\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \"',\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"',\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.',\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' metadata'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' metadata', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' metadata'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' metadata'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '={'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='={', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '={'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '={'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \"'\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\"'\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': \"'\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"'\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'title'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='title', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'title'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'title'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \"':\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\"':\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \"':\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"':\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title':\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \" '\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\" '\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \" '\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \" '\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': '\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'L'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='L', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': 'L'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'L'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'L\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ang'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ang', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ang'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ang'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'Lang\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'Smith'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='Smith', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'Smith'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Smith'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' Walk'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' Walk', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' Walk'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Walk'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith Walk\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'through'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='through', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'through'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'through'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           'Walkthrough'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' -'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' -', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' -'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' -'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           'Walkthrough -'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' Google'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' Google', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' Google'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Google'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           'Walkthrough - Google'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' Col'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' Col', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' Col'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Col'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           'Walkthrough - Google Col'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'ab'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ab', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ab'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ab'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           'Walkthrough - Google Colab'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \"',\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\"',\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \"',\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"',\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab',\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \" '\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\" '\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \" '\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \" '\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', '\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'source'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='source', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'source'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'source'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \"':\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\"':\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \"':\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"':\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source':\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \" '\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\" '\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \" '\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \" '\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': '\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'https'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='https', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'https'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'https'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': 'https\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': '://'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='://', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '://'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '://'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': 'https://\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'col'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='col', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'col'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'col'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': 'https://col\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'ab'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ab', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ab'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ab'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': 'https://colab\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '.'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='.', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '.'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': 'https://colab.\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'res'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='res', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'res'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'res'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': 'https://colab.res\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'earch'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='earch', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'earch'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'earch'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': 'https://colab.research\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '.'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='.', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '.'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': 'https://colab.research.\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'google'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='google', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'google'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'google'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '.'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='.', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '.'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'com'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='com', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'com'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'com'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '/'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='/', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '/'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '/'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'github'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='github', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'github'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'github'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '/'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='/', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '/'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '/'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'lang'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='lang', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'lang'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'lang'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/lang\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'chain'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='chain', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'chain'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'chain'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '-'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='-', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '-'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '-'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'ai'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ai', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ai'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ai'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '/'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='/', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '/'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '/'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'lang'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='lang', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'lang'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'lang'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/lang\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'chain'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='chain', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'chain'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'chain'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '/'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='/', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '/'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '/'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'blob'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='blob', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'blob'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'blob'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '/'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='/', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '/'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '/'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'master'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='master', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'master'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'master'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '/'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='/', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '/'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '/'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'docs'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='docs', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'docs'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'docs'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '/'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='/', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '/'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '/'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'docs'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='docs', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'docs'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'docs'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '/'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='/', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '/'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '/'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'lang'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='lang', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'lang'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'lang'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/lang\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'sm'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='sm', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'sm'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'sm'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsm\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ith'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ith', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ith'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ith'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '/'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='/', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '/'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '/'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'walk'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='walk', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'walk'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'walk'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walk\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'through'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='through', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'through'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'through'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '.'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='.', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '.'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'ip'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ip', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ip'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ip'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ip\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'yn'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='yn', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'yn'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'yn'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipyn\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'b'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='b', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': 'b'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'b'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \"',\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\"',\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \"',\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"',\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb',\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \" '\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\" '\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \" '\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \" '\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'score'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='score', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'score'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'score'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \"':\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\"':\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \"':\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"':\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score':\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' '},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' ', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ' '})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' '},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': \"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '0'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='0', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '0'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '0'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '.'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='.', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '.'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '8'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='8', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '8'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '8'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '4'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='4', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '4'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '4'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.84\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '1'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='1', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '1'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '1'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.841\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '3'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='3', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '3'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '3'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413,\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \" '\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\" '\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \" '\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \" '\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, '\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'images'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='images', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'images'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'images'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \"':\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\"':\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': \"':\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"':\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images':\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' None'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' None', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' None'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' None'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '})'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='})', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '})'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '})'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '`'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='`', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '`'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '`'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})`\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' answers'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' answers', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' answers'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' answers'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})` answers\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})` answers the\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' question'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' question', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' question'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' question'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})` answers the question\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' \"'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' \"', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' \"'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' \"'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'What'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='What', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'What'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'What'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' is'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' is', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' is'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' is'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' Lang'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' Lang', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' Lang'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Lang'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is Lang'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'Smith'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='Smith', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'Smith'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Smith'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '?\"'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='?\"', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '?\"'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '?\"'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\"'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' because'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' because', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' because'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' because'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' it'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' it', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' it'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' it'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' provides'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' provides', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' provides'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' provides'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' information'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' information', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' information'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' information'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' about'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' about', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' about'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' about'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' `'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' `', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' `'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' `'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the `'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'L'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='L', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': 'L'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'L'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the `L'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ang'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ang', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ang'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ang'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the `Lang'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'Smith'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='Smith', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'Smith'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Smith'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '`'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='`', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '`'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '`'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith`'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' platform'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' platform', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' platform'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' platform'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' its'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' its', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' its'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' its'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' features'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' features', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' features'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' features'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' which'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' which', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' which'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' which'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' are'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' are', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' are'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' are'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' relevant'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' relevant', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' relevant'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' relevant'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' to'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' to', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' to'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' to'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' question'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' question', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' question'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' question'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '.'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='.', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '.'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '\\n'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='\\n', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '\\n'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '\\n'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='\\n', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '\\n'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'The'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='The', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'The'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'The'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' `'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' `', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' `'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' `'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'page'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='page', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'page'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'page'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '_'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='_', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '_'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '_'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'content'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='content', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'content'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'content'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '`'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='`', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '`'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '`'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content`'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' field'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' field', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' field'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' field'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' of'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' of', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' of'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' of'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' function'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' function', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' function'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' function'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' call'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' call', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' call'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' call'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' contains'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' contains', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' contains'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' contains'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' a'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' a', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' a'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' a'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' description'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' description', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' description'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' description'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' of'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' of', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' of'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' of'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' `'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' `', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' `'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' `'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'L'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='L', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': 'L'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'L'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `L'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ang'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ang', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ang'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ang'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `Lang'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'Smith'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='Smith', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'Smith'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Smith'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '`'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='`', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '`'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '`'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith`'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' platform'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' platform', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' platform'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' platform'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' including'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' including', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' including'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' including'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' its'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' its', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' its'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' its'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' main'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' main', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' main'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' main'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' features'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' features', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' features'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' features'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' capabilities'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' capabilities', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' capabilities'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' capabilities'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '.'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='.', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '.'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities.'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' The'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' The', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' The'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' The'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' `'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' `', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' `'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' `'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'title'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='title', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'title'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'title'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '`,'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='`,', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '`,'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '`,'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`,'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' `'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' `', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' `'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' `'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'source'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='source', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'source'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'source'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '`,'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='`,', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '`,'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '`,'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' `'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' `', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' `'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' `'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'score'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='score', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'score'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'score'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '`'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='`', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '`'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '`'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score`'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' fields'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' fields', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' fields'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' fields'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' provide'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' provide', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' provide'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' provide'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' additional'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' additional', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' additional'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' additional'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' information'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' information', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' information'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' information'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' about'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' about', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' about'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' about'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' content'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' content', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' content'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' content'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' of'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' of', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' of'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' of'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' page'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' page', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' page'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' page'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '.'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='.', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '.'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page.'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' The'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' The', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' The'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' The'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' `'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' `', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' `'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' `'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'images'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='images', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'images'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'images'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '`'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='`', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '`'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '`'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images`'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' field'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' field', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' field'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' field'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' is'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' is', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' is'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' is'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' empty'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' empty', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' empty'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' empty'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' which'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' which', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' which'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' which'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' suggests'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' suggests', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' suggests'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' suggests'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' that'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' that', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' that'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' that'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' there'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' there', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' there'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' there'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' are'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' are', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' are'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' are'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' no'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' no', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' no'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' no'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' images'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' images', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' images'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' images'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' associated'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' associated', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' associated'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' associated'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' with'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' with', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' with'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' with'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' page'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' page', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' page'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' page'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '.'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='.', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '.'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '\\n'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='\\n', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '\\n'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '\\n'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='\\n', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '\\n'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'Over'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='Over', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'Over'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Over'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Over'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'all'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='all', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'all'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'all'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall,'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' this'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' this', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' this'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' this'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' function'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' function', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' function'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' function'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' call'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' call', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' call'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' call'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' provides'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' provides', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' provides'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' provides'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' a'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' a', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' a'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' a'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' detailed'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' detailed', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' detailed'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' detailed'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' inform'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' inform', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' inform'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' inform'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and inform'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ative'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ative', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ative'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ative'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' answer'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' answer', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' answer'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' answer'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' to'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' to', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' to'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' to'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' question'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' question', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' question'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' question'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the question'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' \"'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' \"', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' \"'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' \"'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the question \"'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'What'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='What', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'What'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'What'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the question \"What'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' is'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' is', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' is'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' is'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the question \"What is'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' Lang'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' Lang', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' Lang'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Lang'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the question \"What is Lang'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'Smith'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='Smith', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'Smith'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Smith'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the question \"What is LangSmith'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '?'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='?', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '?'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '?'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the question \"What is LangSmith?'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '\",'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='\",', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '\",'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\",'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the question \"What is LangSmith?\",'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' which'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' which', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' which'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' which'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the question \"What is LangSmith?\", which'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' includes'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' includes', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' includes'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' includes'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the question \"What is LangSmith?\", which includes'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' information'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' information', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' information'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' information'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the question \"What is LangSmith?\", which includes '\n",
      "           'information'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' about'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' about', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' about'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' about'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the question \"What is LangSmith?\", which includes '\n",
      "           'information about'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the question \"What is LangSmith?\", which includes '\n",
      "           'information about the'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' platform'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' platform', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' platform'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' platform'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the question \"What is LangSmith?\", which includes '\n",
      "           'information about the platform'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \"'\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\"'\", id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': \"'\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"'\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the question \"What is LangSmith?\", which includes '\n",
      "           \"information about the platform'\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 's'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='s', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': 's'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 's'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the question \"What is LangSmith?\", which includes '\n",
      "           \"information about the platform's\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' features'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' features', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' features'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' features'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the question \"What is LangSmith?\", which includes '\n",
      "           \"information about the platform's features\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the question \"What is LangSmith?\", which includes '\n",
      "           \"information about the platform's features,\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' capabilities'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' capabilities', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' capabilities'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' capabilities'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the question \"What is LangSmith?\", which includes '\n",
      "           \"information about the platform's features, capabilities\"})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ','},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=',', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ','})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the question \"What is LangSmith?\", which includes '\n",
      "           \"information about the platform's features, capabilities,\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the question \"What is LangSmith?\", which includes '\n",
      "           \"information about the platform's features, capabilities, and\"})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' functionality'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' functionality', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' functionality'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' functionality'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the question \"What is LangSmith?\", which includes '\n",
      "           \"information about the platform's features, capabilities, and \"\n",
      "           'functionality'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '.'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='.', id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': '.'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: Document(page_content='The Run tab for the top-level chain \"\n",
      "           'shows the human input, the parsed output, the latency (under a '\n",
      "           'second), and the tokens used, as well as the clock time and call '\n",
      "           'status.\\\\n The cookbook covers tracing your code without LangChain '\n",
      "           '(using the @traceable decorator); using the LangChain Hub to '\n",
      "           'discover, share, and version control prompts; testing and '\n",
      "           'benchmarking your LLM systems in Python and TypeScript or '\n",
      "           'JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "           \"Thought: The function call `Document(page_content='The Run tab for \"\n",
      "           'the top-level chain shows the human input, the parsed output, the '\n",
      "           'latency (under a second), and the tokens used, as well as the '\n",
      "           'clock time and call status.\\\\n The cookbook covers tracing your '\n",
      "           'code without LangChain (using the @traceable decorator); using the '\n",
      "           'LangChain Hub to discover, share, and version control prompts; '\n",
      "           'testing and benchmarking your LLM systems in Python and TypeScript '\n",
      "           'or JavaScript; using user feedback to improve, monitor, and '\n",
      "           'personalize your applications; exporting data for fine-tuning; and '\n",
      "           'exporting your run data for exploratory data analysis.\\\\n In '\n",
      "           'addition to the input variables (prompt), an LLM call uses a '\n",
      "           'template and often auxiliary functions; for example, retrieval of '\n",
      "           'information from the web, uploaded files, and system prompts that '\n",
      "           \"set the context for the LLM.', metadata={'title': 'LangSmith \"\n",
      "           \"Walkthrough - Google Colab', 'source': \"\n",
      "           \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "           '\\'score\\': 0.8413, \\'images\\': None})` answers the question \"What '\n",
      "           'is LangSmith?\" because it provides information about the '\n",
      "           '`LangSmith` platform and its features, which are relevant to the '\n",
      "           'question.\\n'\n",
      "           '\\n'\n",
      "           'The `page_content` field of the function call contains a '\n",
      "           'description of the `LangSmith` platform, including its main '\n",
      "           'features and capabilities. The `title`, `source`, and `score` '\n",
      "           'fields provide additional information about the content of the '\n",
      "           'page. The `images` field is empty, which suggests that there are '\n",
      "           'no images associated with the page.\\n'\n",
      "           '\\n'\n",
      "           'Overall, this function call provides a detailed and informative '\n",
      "           'answer to the question \"What is LangSmith?\", which includes '\n",
      "           \"information about the platform's features, capabilities, and \"\n",
      "           'functionality.'})RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ''},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='', response_metadata={'finish_reason': 'stop'}, id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b')})RunLogPatch({'op': 'add', 'path': '/logs/StrOutputParser/streamed_output/-', 'value': ''})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/final_output',\n",
      "  'value': {'generations': [[{'generation_info': {'finish_reason': 'stop'},\n",
      "                              'message': AIMessageChunk(content='\\nCall: Document(page_content=\\'The Run tab for the top-level chain shows the human input, the parsed output, the latency (under a second), and the tokens used, as well as the clock time and call status.\\\\n The cookbook covers tracing your code without LangChain (using the @traceable decorator); using the LangChain Hub to discover, share, and version control prompts; testing and benchmarking your LLM systems in Python and TypeScript or JavaScript; using user feedback to improve, monitor, and personalize your applications; exporting data for fine-tuning; and exporting your run data for exploratory data analysis.\\\\n In addition to the input variables (prompt), an LLM call uses a template and often auxiliary functions; for example, retrieval of information from the web, uploaded files, and system prompts that set the context for the LLM.\\', metadata={\\'title\\': \\'LangSmith Walkthrough - Google Colab\\', \\'source\\': \\'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb\\', \\'score\\': 0.8413, \\'images\\': None})<bot_end>\\nThought: The function call `Document(page_content=\\'The Run tab for the top-level chain shows the human input, the parsed output, the latency (under a second), and the tokens used, as well as the clock time and call status.\\\\n The cookbook covers tracing your code without LangChain (using the @traceable decorator); using the LangChain Hub to discover, share, and version control prompts; testing and benchmarking your LLM systems in Python and TypeScript or JavaScript; using user feedback to improve, monitor, and personalize your applications; exporting data for fine-tuning; and exporting your run data for exploratory data analysis.\\\\n In addition to the input variables (prompt), an LLM call uses a template and often auxiliary functions; for example, retrieval of information from the web, uploaded files, and system prompts that set the context for the LLM.\\', metadata={\\'title\\': \\'LangSmith Walkthrough - Google Colab\\', \\'source\\': \\'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb\\', \\'score\\': 0.8413, \\'images\\': None})` answers the question \"What is LangSmith?\" because it provides information about the `LangSmith` platform and its features, which are relevant to the question.\\n\\nThe `page_content` field of the function call contains a description of the `LangSmith` platform, including its main features and capabilities. The `title`, `source`, and `score` fields provide additional information about the content of the page. The `images` field is empty, which suggests that there are no images associated with the page.\\n\\nOverall, this function call provides a detailed and informative answer to the question \"What is LangSmith?\", which includes information about the platform\\'s features, capabilities, and functionality.', response_metadata={'finish_reason': 'stop'}, id='run-117fbf49-d6fb-4474-91d4-56252f0adb7b'),\n",
      "                              'text': '\\n'\n",
      "                                      \"Call: Document(page_content='The Run \"\n",
      "                                      'tab for the top-level chain shows the '\n",
      "                                      'human input, the parsed output, the '\n",
      "                                      'latency (under a second), and the '\n",
      "                                      'tokens used, as well as the clock time '\n",
      "                                      'and call status.\\\\n The cookbook covers '\n",
      "                                      'tracing your code without LangChain '\n",
      "                                      '(using the @traceable decorator); using '\n",
      "                                      'the LangChain Hub to discover, share, '\n",
      "                                      'and version control prompts; testing '\n",
      "                                      'and benchmarking your LLM systems in '\n",
      "                                      'Python and TypeScript or JavaScript; '\n",
      "                                      'using user feedback to improve, '\n",
      "                                      'monitor, and personalize your '\n",
      "                                      'applications; exporting data for '\n",
      "                                      'fine-tuning; and exporting your run '\n",
      "                                      'data for exploratory data analysis.\\\\n '\n",
      "                                      'In addition to the input variables '\n",
      "                                      '(prompt), an LLM call uses a template '\n",
      "                                      'and often auxiliary functions; for '\n",
      "                                      'example, retrieval of information from '\n",
      "                                      'the web, uploaded files, and system '\n",
      "                                      'prompts that set the context for the '\n",
      "                                      \"LLM.', metadata={'title': 'LangSmith \"\n",
      "                                      \"Walkthrough - Google Colab', 'source': \"\n",
      "                                      \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "                                      \"'score': 0.8413, 'images': \"\n",
      "                                      'None})<bot_end>\\n'\n",
      "                                      'Thought: The function call '\n",
      "                                      \"`Document(page_content='The Run tab for \"\n",
      "                                      'the top-level chain shows the human '\n",
      "                                      'input, the parsed output, the latency '\n",
      "                                      '(under a second), and the tokens used, '\n",
      "                                      'as well as the clock time and call '\n",
      "                                      'status.\\\\n The cookbook covers tracing '\n",
      "                                      'your code without LangChain (using the '\n",
      "                                      '@traceable decorator); using the '\n",
      "                                      'LangChain Hub to discover, share, and '\n",
      "                                      'version control prompts; testing and '\n",
      "                                      'benchmarking your LLM systems in Python '\n",
      "                                      'and TypeScript or JavaScript; using '\n",
      "                                      'user feedback to improve, monitor, and '\n",
      "                                      'personalize your applications; '\n",
      "                                      'exporting data for fine-tuning; and '\n",
      "                                      'exporting your run data for exploratory '\n",
      "                                      'data analysis.\\\\n In addition to the '\n",
      "                                      'input variables (prompt), an LLM call '\n",
      "                                      'uses a template and often auxiliary '\n",
      "                                      'functions; for example, retrieval of '\n",
      "                                      'information from the web, uploaded '\n",
      "                                      'files, and system prompts that set the '\n",
      "                                      \"context for the LLM.', \"\n",
      "                                      \"metadata={'title': 'LangSmith \"\n",
      "                                      \"Walkthrough - Google Colab', 'source': \"\n",
      "                                      \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "                                      \"'score': 0.8413, 'images': None})` \"\n",
      "                                      'answers the question \"What is '\n",
      "                                      'LangSmith?\" because it provides '\n",
      "                                      'information about the `LangSmith` '\n",
      "                                      'platform and its features, which are '\n",
      "                                      'relevant to the question.\\n'\n",
      "                                      '\\n'\n",
      "                                      'The `page_content` field of the '\n",
      "                                      'function call contains a description of '\n",
      "                                      'the `LangSmith` platform, including its '\n",
      "                                      'main features and capabilities. The '\n",
      "                                      '`title`, `source`, and `score` fields '\n",
      "                                      'provide additional information about '\n",
      "                                      'the content of the page. The `images` '\n",
      "                                      'field is empty, which suggests that '\n",
      "                                      'there are no images associated with the '\n",
      "                                      'page.\\n'\n",
      "                                      '\\n'\n",
      "                                      'Overall, this function call provides a '\n",
      "                                      'detailed and informative answer to the '\n",
      "                                      'question \"What is LangSmith?\", which '\n",
      "                                      'includes information about the '\n",
      "                                      \"platform's features, capabilities, and \"\n",
      "                                      'functionality.',\n",
      "                              'type': 'ChatGenerationChunk'}]],\n",
      "            'llm_output': None,\n",
      "            'run': None}},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/end_time',\n",
      "  'value': '2024-04-16T15:57:15.479+00:00'})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/final_output',\n",
      "  'value': {'output': '\\n'\n",
      "                      \"Call: Document(page_content='The Run tab for the \"\n",
      "                      'top-level chain shows the human input, the parsed '\n",
      "                      'output, the latency (under a second), and the tokens '\n",
      "                      'used, as well as the clock time and call status.\\\\n The '\n",
      "                      'cookbook covers tracing your code without LangChain '\n",
      "                      '(using the @traceable decorator); using the LangChain '\n",
      "                      'Hub to discover, share, and version control prompts; '\n",
      "                      'testing and benchmarking your LLM systems in Python and '\n",
      "                      'TypeScript or JavaScript; using user feedback to '\n",
      "                      'improve, monitor, and personalize your applications; '\n",
      "                      'exporting data for fine-tuning; and exporting your run '\n",
      "                      'data for exploratory data analysis.\\\\n In addition to '\n",
      "                      'the input variables (prompt), an LLM call uses a '\n",
      "                      'template and often auxiliary functions; for example, '\n",
      "                      'retrieval of information from the web, uploaded files, '\n",
      "                      \"and system prompts that set the context for the LLM.', \"\n",
      "                      \"metadata={'title': 'LangSmith Walkthrough - Google \"\n",
      "                      \"Colab', 'source': \"\n",
      "                      \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "                      \"'score': 0.8413, 'images': None})<bot_end>\\n\"\n",
      "                      \"Thought: The function call `Document(page_content='The \"\n",
      "                      'Run tab for the top-level chain shows the human input, '\n",
      "                      'the parsed output, the latency (under a second), and '\n",
      "                      'the tokens used, as well as the clock time and call '\n",
      "                      'status.\\\\n The cookbook covers tracing your code '\n",
      "                      'without LangChain (using the @traceable decorator); '\n",
      "                      'using the LangChain Hub to discover, share, and version '\n",
      "                      'control prompts; testing and benchmarking your LLM '\n",
      "                      'systems in Python and TypeScript or JavaScript; using '\n",
      "                      'user feedback to improve, monitor, and personalize your '\n",
      "                      'applications; exporting data for fine-tuning; and '\n",
      "                      'exporting your run data for exploratory data '\n",
      "                      'analysis.\\\\n In addition to the input variables '\n",
      "                      '(prompt), an LLM call uses a template and often '\n",
      "                      'auxiliary functions; for example, retrieval of '\n",
      "                      'information from the web, uploaded files, and system '\n",
      "                      \"prompts that set the context for the LLM.', \"\n",
      "                      \"metadata={'title': 'LangSmith Walkthrough - Google \"\n",
      "                      \"Colab', 'source': \"\n",
      "                      \"'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', \"\n",
      "                      \"'score': 0.8413, 'images': None})` answers the question \"\n",
      "                      '\"What is LangSmith?\" because it provides information '\n",
      "                      'about the `LangSmith` platform and its features, which '\n",
      "                      'are relevant to the question.\\n'\n",
      "                      '\\n'\n",
      "                      'The `page_content` field of the function call contains '\n",
      "                      'a description of the `LangSmith` platform, including '\n",
      "                      'its main features and capabilities. The `title`, '\n",
      "                      '`source`, and `score` fields provide additional '\n",
      "                      'information about the content of the page. The `images` '\n",
      "                      'field is empty, which suggests that there are no images '\n",
      "                      'associated with the page.\\n'\n",
      "                      '\\n'\n",
      "                      'Overall, this function call provides a detailed and '\n",
      "                      'informative answer to the question \"What is '\n",
      "                      'LangSmith?\", which includes information about the '\n",
      "                      \"platform's features, capabilities, and functionality.\"}},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/end_time',\n",
      "  'value': '2024-04-16T15:57:15.486+00:00'})"
     ]
    }
   ],
   "source": [
    "async for s in retrieval_chain.astream_log({\"question\": \"what is langsmith\"}):\n",
    "    print(s, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42da38dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '',\n",
      "  'value': {'final_output': None,\n",
      "            'id': '4ca469c7-35ea-4cab-a204-a123115ec2a1',\n",
      "            'logs': {},\n",
      "            'name': 'RunnableSequence',\n",
      "            'streamed_output': [],\n",
      "            'type': 'chain'}})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/Docs',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': '85351804-4f3a-4f82-b1c9-22ada283bf84',\n",
      "            'metadata': {},\n",
      "            'name': 'Docs',\n",
      "            'start_time': '2024-04-16T15:57:24.987+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['seq:step:2'],\n",
      "            'type': 'retriever'}})RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/Docs/final_output',\n",
      "  'value': {'documents': [Document(page_content='We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like.\\n Over the last few months, we’ve been working directly with some early design partners and testing it on our own internal workflows, and we’ve found LangSmith helps teams in 5 core ways:\\nDebugging\\nLangSmith gives you full visibility into model inputs and output of every step in the chain of events. As a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones.\\n A unified platform\\nWhile each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.”\\n', metadata={'title': 'Announcing LangSmith, a unified platform for debugging, testing ...', 'source': 'https://blog.langchain.dev/announcing-langsmith/', 'score': 0.95112, 'images': None}),\n",
      "                          Document(page_content=\"This was such an important unlock for us as we build in tactical functionality to our AI.\\nAdvice for Product Teams Considering LangSmith\\nWhen you're in the thick of product development, the whirlwind of AI and LLMs can be overwhelming. ou can now clearly see the citations coming through with the responses in the traces, and we’re good to ship the changes to the prompt to prod.\\n We do this primarily to legitimize the LLMs response and give our HelpHub customer’s peace of mind that their end users are in fact getting to the help resource they need (instead of an LLM just hallucinating and giving any response it wants.)\\n Take it from us, as we integrated AI more heavily and widely across our products, we’ve been more conscious than ever that the quality of the outputs matches the quality and trust that our customers have in our product. We updated our prompt to be more firm when asking for the sources:\\nWe then tested everything using LangSmith evals, to make sure that it fixes the issue before pushing to production.\\n\", metadata={'title': 'Peering Into the Soul of AI Decision-Making with LangSmith', 'source': 'https://blog.langchain.dev/peering-into-the-soul-of-ai-decision-making-with-langsmith/', 'score': 0.93197, 'images': None}),\n",
      "                          Document(page_content=\"BETA Sign Up\\nContact Sales\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter 🦜🔗 LangChain\\nLangSmith\\nLangServe\\nAgents\\nRetrieval\\nEvaluation\\nBlog\\nDocs\\n🦜🔗 LangChain\\nBuild and deploy LLM apps with confidence\\nAn all-in-one developer platform for every step of the application lifecycle.\\n Prompt playground\\nCross-team collaboration\\nCatalog of ranging models & tasks\\nProven prompting strategies\\nExplore LangChain Hub\\nTurn the magic of LLM applications into enterprise-ready products\\nNative collaboration\\nBring your team together in LangSmith to craft prompts, debug, and capture feedback.\\n Application-level usage stats\\nFeedback collection\\nFilter traces\\nCost measurement\\nPerformance comparison\\nGo To Docs\\nManage Prompts\\nPrompts power your team's chains and agents, and LangSmith allows you to refine, test, and version them in one place. Dataset curation\\nEvaluate chain performance\\nAI-assisted evaluation\\nEasy benchmarking\\nGo To Docs\\nMonitor\\nGiven the stochastic nature of LLMs, it can be hard to answer the simple question: “what’s happening with my application?”\", metadata={'title': 'LangSmith', 'source': 'https://www.langchain.com/langsmith', 'score': 0.9319, 'images': None}),\n",
      "                          Document(page_content='How it Works, Use Cases, Alternatives & More\\nRichie Cotton\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAdel Nehme\\n32 min\\nAn Introductory Guide to Fine-Tuning LLMs\\nJosep Ferrer\\n12 min\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nBex Tuychiev\\n15 min\\nGrow your data skills with DataCamp for Mobile\\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\\n For labeled datasets like the CSV dataset we uploaded, LangSmith offers more comprehensive evaluators for measuring the correctness of the response to a prompt:\\nLet’s try the last one on our examples:\\nCoTQA criterion returns a score called Contextual accuracy, as depicted in the GIF below (also in the UI):\\nPlease visit the LangChain evaluators section of LangSmith docs to learn much more about evaluators.\\n Once the upload finishes, the dataset will appear in the UI:\\nLet’s run our custom criterion from the previous section on this dataset as well:\\nIf you go to the dataset page and check out the run, we can see the average scores for each custom criteria:\\nEvaluating Labeled Datasets\\nBuilt-in and custom evaluators written in natural language are mostly for unlabeled datasets. We will use the create_dataset function of the client:\\nNow, let’s add three inputs that each ask the LLM to create a single flashcard:\\nIf you go over the dataset tab of the UI, you will see each prompt listed with NULL output:\\nNow, let’s run all the prompts in a single line of code using run_on_dataset function:\\n How it Works, Use Cases, Alternatives & More\\nHow AI is Changing Cybersecurity with Brian Murphy, CEO of ReliaQuest\\nAn Introductory Guide to Fine-Tuning LLMs\\nSalesforce XGen-7B: A Step-by-Step Tutorial on Using And Fine-Tuning XGen-7B\\nStart Your Langchain Journey Today\\nCourse\\nDeveloping LLM Applications with LangChain\\nCourse\\nLarge Language Models (LLMs)', metadata={'title': 'An Introduction to Debugging And Testing LLMs in LangSmith', 'source': 'https://www.datacamp.com/tutorial/introduction-to-langsmith', 'score': 0.9306, 'images': None}),\n",
      "                          Document(page_content='This function helps to load the specific language models and tools required for the task as shown in the code snippet below:\\nAs a next step, initialize an agent by calling the initialize_agent function with several parameters like tools, llms, and agent:\\nThe verbose parameter is set to false, indicating that the agent will not provide verbose or detailed output.\\n You can accomplish this by following the shell commands provided below:\\nCreating a LangSmith client\\nNext, create a LangSmith client to interact with the API:\\nIf you’re using Python, run the following commands to import the module:\\n This code also handles exceptions that may occur during the agent execution:\\nIt’s also important to call the wait_for_all_tracers function from the langchain.callbacks.tracers.langchain module as shown in the code snippet below:\\nCalling the wait_for_all_tracers function helps ensure that logs and traces are submitted in full before the program proceeds. The temperature parameter will be set to 0, implying that the generated response will be more deterministic as shown in the code snippet below:\\nNow, let’s call the load_tools function with a list of tool APIs, such as serpapi and llm-math, and also take the llm instance as a parameter. It initializes a chat model, loads specific tools, and creates an agent that can generate responses based on descriptions:\\nInput processing with exception handling\\nThe code below defines a list of input examples using the asyncio library to asynchronously run the agent on each input and gather the results for further processing.', metadata={'title': 'Using LangSmith to test LLMs and AI applications', 'source': 'https://blog.logrocket.com/langsmith-test-llms-ai-applications/', 'score': 0.90113, 'images': None}),\n",
      "                          Document(page_content='A deep dive into the latest product from the creators of LangChain 🦜\\n--\\n3\\nCobus Greyling\\nLangSmith\\nI was fortunate to get early access to the LangSmith platform and in this article you will find practical code examples and demonstration…\\n--\\nLists\\nNatural Language Processing\\nPredictive Modeling w/ Python\\nAI Regulation\\nPractical Guides to Machine Learning\\nYule Wang, PhD\\nA Complete Guide to LLMs-based Autonomous Agents (Part I):\\n—\\u200a—\\u200aChain of Thought, Plan and Solve/Execute, Self-Ask, ReAct, Reflexion, Self-Consistency, Tree of Thoughts and Graph of Thoughts\\n--\\n1\\nRyan Nguyen\\nin\\nBetter Programming\\nLlamaIndex: How To Evaluate Your RAG (Retrieval Augmented Generation) Applications\\nClassic 10k reports with Australian real estate market reports to demonstrate the end-to-end evaluation process\\n--\\n2\\nKelvin Lu\\nTracing How LangChain Works Behind the Scenes\\nIntroduction of LLM Tracing technique with both LangChain built-in tracing funciton and LangChain Visualizer.\\n --\\nCameron R. Wolfe, Ph.D.\\nin\\nTowards Data Science\\nChain of Thought Prompting for LLMs\\nA practical and simple approach for “reasoning” with LLMs\\n--\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams A Step-by-Step Guide to Deploying ChromaDB in the Cloud\\n--\\nBenoit Ruiz\\nin\\nBetter Programming\\nAdvice From a Software Engineer With 8 Years of Experience\\nPractical tips for those who want to advance in their careers\\n--\\n233\\nVinita\\nin\\nBetter Programming\\n4 Strategies to Give Effective Feedback to a Difficult Person\\nRespect is like air. But if you take it away, it’s all that people can think about.\\n--\\n21\\nRicardo Ledan\\nin\\nBetter Programming\\nBuilding Context-Aware Question-Answering Systems With LLMs\\nA step-by-step guide to using embeddings, vector search, and prompt engineering for building context-aware question-answering systems\\n--\\n1\\nRecommended from Medium\\nLogan Kilpatrick\\nWhat is LangSmith and why should I care as a developer?\\n Ricardo Ledan\\nFollow\\nBetter Programming\\n--\\nShare\\nWhat is LangSmith?\\n', metadata={'title': 'The Art of LangSmithing - Better Programming', 'source': 'https://betterprogramming.pub/the-art-of-langsmithing-42dcd191a220', 'score': 0.89342, 'images': None}),\n",
      "                          Document(page_content='The Run tab for the top-level chain shows the human input, the parsed output, the latency (under a second), and the tokens used, as well as the clock time and call status.\\n The cookbook covers tracing your code without LangChain (using the @traceable decorator); using the LangChain Hub to discover, share, and version control prompts; testing and benchmarking your LLM systems in Python and TypeScript or JavaScript; using user feedback to improve, monitor, and personalize your applications; exporting data for fine-tuning; and exporting your run data for exploratory data analysis.\\n In addition to the input variables (prompt), an LLM call uses a template and often auxiliary functions; for example, retrieval of information from the web, uploaded files, and system prompts that set the context for the LLM.\\n After I set up my LangSmith account, created my API key, updated my LangChain installation with pip, and set up my shell environment variables, I tried to run the Python quickstart application:\\nBefore we discuss the results, take a look at the LangSmith hub:\\nFigure 1. Here are the evaluation statistics, which were printed in the terminal during the run:\\nSomebody had fun creating the rap battle prompts, as shown in the dataset below:\\nFigure 9.', metadata={'title': 'What is LangSmith? Tracing and debugging for LLMs | InfoWorld', 'source': 'https://www.infoworld.com/article/3708628/what-is-langsmith-tracing-and-debugging-for-llms.html', 'score': 0.89119, 'images': None}),\n",
      "                          Document(page_content=\"LangSmith lets you evaluate any LLM, chain, agent, or even a custom function. Conversational agents are stateful (they have memory); to ensure that this state isn't shared between dataset runs, we will pass in a chain_factory (aka a constructor) function to initialize for each call.\", metadata={'title': 'LangSmith Walkthrough - Google Colab', 'source': 'https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/langsmith/walkthrough.ipynb', 'score': 0.88111, 'images': None}),\n",
      "                          Document(page_content='But what are the production challenges that need to be solved which were not as relevant in prototyping?\\nReliability — it is deceptively easy to build something that works well for a simple constrained example but actually still quite hard today to build LLM applications with the consistency that most companies would want.\\n for tools\\nKim Seon Woo - Nov 6\\nComo deixar o Swagger com tema dark mode usando NestJS\\nWiliam V. Joaquim - Nov 5\\nData visualizing\\nBIRINGANINE BASEME Destin\\n- Oct 7\\nFree Face-Swap AI 🎭 with Source Code for Web and Mobile App.\\nshadee22 - As you build more complex workflows, it can be hard to understand exactly what queries are moving through different flows so a simple UI to see this and log the historical data is going to be a value add from day one.\\n A huge part of the value add for LangSmith is being able to do all of these things from a simple and intuitive UI which significantly reduces the barrier to entry for those without a software background.\\n [Quick note: I am writing this article to reflect my personal views as I explore the LLM ecosystem and this is not intended to represent the views of my employer, hence being on my personal blog]\\nWhat is LangSmith?', metadata={'title': 'What is LangSmith and why should I care as a developer?', 'source': 'https://dev.to/logankilpatrick/what-is-langsmith-and-why-should-i-care-as-a-developer-19k', 'score': 0.82303, 'images': None}),\n",
      "                          Document(page_content=\"LangSmith Cookbook: Real-world Lang Smith Examples\\nThe LangSmith Cookbook is not just a compilation of code snippets; it's a goldmine of hands-on examples designed to inspire and assist you in your projects. On This Page\\nLangSmith: Best Way to Test LLMs and AI Application\\nPublished on 12/17/2023\\nIf you're in the world of Language Learning Models (LLMs), you've probably heard of LangSmith. How to Download Feedback and Examples (opens in a new tab): Export predictions, evaluation results, and other information to add to your reports programmatically.\\n This article is your one-stop guide to understanding LangSmith, a platform that offers a plethora of features for debugging, testing, evaluating, and monitoring LLM applications.\\n How do I get access to LangSmith?\\nTo get access to LangSmith, you'll need to sign up for an account on their website.\", metadata={'title': 'LangSmith: Best Way to Test LLMs and AI Application', 'source': 'https://cheatsheet.md/langchain-tutorials/langsmith.en', 'score': 0.80751, 'images': None})]}},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/Docs/end_time',\n",
      "  'value': '2024-04-16T15:57:25.975+00:00'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': '\\n'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Call'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': '\\nCall'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ':'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': '\\nCall:'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' initialize'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': '\\nCall: initialize'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '_'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': '\\nCall: initialize_'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'agent'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\nCall: initialize_agent'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '('},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\nCall: initialize_agent('})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'tools'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\nCall: initialize_agent(tools'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '='},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\nCall: initialize_agent(tools='})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"['\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: initialize_agent(tools=['\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ser'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: initialize_agent(tools=['ser\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'p'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: initialize_agent(tools=['serp\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'api'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: initialize_agent(tools=['serpapi\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"'],\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: initialize_agent(tools=['serpapi'],\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' ll'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: initialize_agent(tools=['serpapi'], ll\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ms'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: initialize_agent(tools=['serpapi'], llms\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '='},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: initialize_agent(tools=['serpapi'], llms=\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"['\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: initialize_agent(tools=['serpapi'], llms=['\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'll'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: initialize_agent(tools=['serpapi'], llms=['ll\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'm'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: initialize_agent(tools=['serpapi'], llms=['llm\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '-'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: initialize_agent(tools=['serpapi'], llms=['llm-\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'math'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: initialize_agent(tools=['serpapi'], llms=['llm-math\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"',\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: initialize_agent(tools=['serpapi'], llms=['llm-math',\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \" '\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: initialize_agent(tools=['serpapi'], llms=['llm-math', '\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'll'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': \"\\nCall: initialize_agent(tools=['serpapi'], llms=['llm-math', 'll\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'm'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', 'llm\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '-'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', 'llm-\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'eng'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-eng\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'lish'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"',\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english',\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \" '\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', '\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'll'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'll\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'm'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '-'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 's'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-s\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'cience'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"'],\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'],\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' agent'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"='\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'lang'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='lang\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'sm'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsm\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ith'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"')\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '<bot_end>'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end>\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' '},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Th'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           'Th'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ought'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           'Thought'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ':'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           'Thought:'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' The'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           'Thought: The'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' function'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           'Thought: The function'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' call'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           'Thought: The function call'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' `'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           'Thought: The function call `'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'initialize'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           'Thought: The function call `initialize'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '_'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           'Thought: The function call `initialize_'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'agent'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           'Thought: The function call `initialize_agent'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '('},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           'Thought: The function call `initialize_agent('})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'tools'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           'Thought: The function call `initialize_agent(tools'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '='},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           'Thought: The function call `initialize_agent(tools='})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"['\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ser'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['ser\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'p'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serp\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'api'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"'],\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'],\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' ll'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           'll'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ms'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           'llms'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '='},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           'llms='})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"['\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'll'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['ll\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'm'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '-'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'math'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"',\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math',\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \" '\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', '\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'll'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'll\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'm'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '-'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'eng'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-eng\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'lish'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"',\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english',\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \" '\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', '\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'll'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'll\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'm'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '-'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 's'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-s\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'cience'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"'],\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'],\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' agent'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], agent\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"='\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], agent='\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'lang'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], agent='lang\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'sm'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], agent='langsm\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ith'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], agent='langsmith\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': \"')`\"},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           \"agent='langsmith')`\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' answers'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           \"agent='langsmith')` answers\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           \"agent='langsmith')` answers the\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' question'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           \"agent='langsmith')` answers the question\"})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' \"'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'what'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' is'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' lang'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is lang'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'sm'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsm'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ith'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\"'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\"'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' because'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' it'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' initial'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initial'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'izes'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' an'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' agent'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' called'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' \"'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'lang'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"lang'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'sm'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsm'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ith'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\".'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\".'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' The'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' `'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'agent'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '`'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent`'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' parameter'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' spec'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter spec'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ifies'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' name'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' of'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' agent'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' to'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' be'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' initialized'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized,'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' in'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' this'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' case'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case,'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' it'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' is'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' set'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' to'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' \"'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'lang'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"lang'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'sm'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsm'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ith'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\",'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\",'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' which'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' matches'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' name'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' of'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' agent'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' that'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' was'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' mentioned'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' in'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' question'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'The'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' `'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'tools'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '`'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools`'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' parameter'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' spec'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter spec'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ifies'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' tools'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' that'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' should'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' be'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' used'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' by'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' agent'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent.'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' In'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' this'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' case'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case,'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' list'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' contains'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' only'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' one'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' tool'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' called'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' \"'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called \"'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ser'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"ser'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'p'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serp'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'api'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\".'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'The'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' `'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'll'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `ll'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ms'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '`'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms`'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' parameter'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' spec'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter spec'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ifies'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' L'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the L'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'LM'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLM'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 's'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' that'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' should'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' be'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' used'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' by'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by '\n",
      "           'the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' agent'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent.'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' In'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' this'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' case'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case,'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' list'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' contains'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' three'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' L'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three L'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'LM'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLM'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 's'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' called'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' \"'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called \"'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'll'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called \"ll'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'm'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called \"llm'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '-'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called \"llm-'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'math'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\",'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\",'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' \"'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'll'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"ll'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'm'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '-'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'eng'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-eng'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'lish'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\",'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\",'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' \"'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'll'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"ll'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'm'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '-'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 's'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-s'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'cience'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\".'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\\n'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'Over'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Over'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'all'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall,'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' function'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' call'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' answers'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' question'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' because'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' it'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' initial'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initial'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'izes'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' an'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' agent'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' called'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' \"'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'lang'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"lang'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'sm'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsm'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ith'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\"'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\"'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' that'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' uses'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' tools'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' \"'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools \"'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ser'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools \"ser'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'p'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools \"serp'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'api'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\"'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\"'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' L'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the L'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'LM'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the LLM'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 's'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the LLMs'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' \"'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the LLMs \"'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'll'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the LLMs \"ll'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'm'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the LLMs \"llm'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '-'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the LLMs \"llm-'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'math'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the LLMs \"llm-math'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\",'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the LLMs \"llm-math\",'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' \"'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the LLMs \"llm-math\", \"'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'll'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the LLMs \"llm-math\", \"ll'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'm'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the LLMs \"llm-math\", \"llm'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '-'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the LLMs \"llm-math\", \"llm-'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'eng'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the LLMs \"llm-math\", \"llm-eng'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'lish'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the LLMs \"llm-math\", \"llm-english'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\",'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the LLMs \"llm-math\", \"llm-english\",'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' and'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the LLMs \"llm-math\", \"llm-english\", and'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' \"'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the LLMs \"llm-math\", \"llm-english\", and \"'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'll'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the LLMs \"llm-math\", \"llm-english\", and \"ll'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'm'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the LLMs \"llm-math\", \"llm-english\", and \"llm'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '-'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the LLMs \"llm-math\", \"llm-english\", and \"llm-'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 's'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the LLMs \"llm-math\", \"llm-english\", and \"llm-s'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'cience'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the LLMs \"llm-math\", \"llm-english\", and '\n",
      "           '\"llm-science'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '\".'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '\\n'\n",
      "           \"Call: initialize_agent(tools=['serpapi'], llms=['llm-math', \"\n",
      "           \"'llm-english', 'llm-science'], agent='langsmith')<bot_end> \\n\"\n",
      "           \"Thought: The function call `initialize_agent(tools=['serpapi'], \"\n",
      "           \"llms=['llm-math', 'llm-english', 'llm-science'], \"\n",
      "           'agent=\\'langsmith\\')` answers the question \"what is langsmith\" '\n",
      "           'because it initializes an agent called \"langsmith\". The `agent` '\n",
      "           'parameter specifies the name of the agent to be initialized, and '\n",
      "           'in this case, it is set to \"langsmith\", which matches the name of '\n",
      "           'the agent that was mentioned in the question.\\n'\n",
      "           '\\n'\n",
      "           'The `tools` parameter specifies the tools that should be used by '\n",
      "           'the agent. In this case, the list contains only one tool called '\n",
      "           '\"serpapi\".\\n'\n",
      "           '\\n'\n",
      "           'The `llms` parameter specifies the LLMs that should be used by the '\n",
      "           'agent. In this case, the list contains three LLMs called '\n",
      "           '\"llm-math\", \"llm-english\", and \"llm-science\".\\n'\n",
      "           '\\n'\n",
      "           'Overall, the function call answers the question because it '\n",
      "           'initializes an agent called \"langsmith\" that uses the tools '\n",
      "           '\"serpapi\" and the LLMs \"llm-math\", \"llm-english\", and '\n",
      "           '\"llm-science\".'})RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''})"
     ]
    }
   ],
   "source": [
    "async for s in retrieval_chain.astream_log({\"question\": \"what is langsmith\"}, include_names=[\"Docs\"]):\n",
    "    print(s, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9933e00",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d73dbe",
   "metadata": {},
   "source": [
    "### Stream Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0c85f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults()\n",
    "tools = [search]\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "# If you want to see the prompt in full, you can at: https://smith.langchain.com/hub/hwchase17/openai-functions-agent\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5623d83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm = ChatOpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ea71385",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76385e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output': \"Sorry, I'm not able to provide that information. However, I can offer some suggestions for finding out about the weather in SF and LA:\\n\\n1. Check websites such as AccuWeather or Weather Underground for up-to-date weather forecasts for San Francisco and Los Angeles.\\n2. Tune into local news channels or watch online streams to see the latest weather updates and forecasts.\\n3. Use a weather app on your phone or computer to get real-time weather information for SF and LA.\\n4. Consider purchasing a weather radio to receive broadcasts from local authorities.\\n5. If you're looking for more detailed information, such as wind direction or temperature, consider using a weather app that provides this information.\", 'messages': [AIMessage(content=\"Sorry, I'm not able to provide that information. However, I can offer some suggestions for finding out about the weather in SF and LA:\\n\\n1. Check websites such as AccuWeather or Weather Underground for up-to-date weather forecasts for San Francisco and Los Angeles.\\n2. Tune into local news channels or watch online streams to see the latest weather updates and forecasts.\\n3. Use a weather app on your phone or computer to get real-time weather information for SF and LA.\\n4. Consider purchasing a weather radio to receive broadcasts from local authorities.\\n5. If you're looking for more detailed information, such as wind direction or temperature, consider using a weather app that provides this information.\")]}\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "for chunk in agent_executor.stream({\"input\": \"what is the weather in SF and then LA\"}):\n",
    "    print(chunk)\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab8e681",
   "metadata": {},
   "source": [
    "### Stream Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7429e41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, streaming=True)\n",
    "llm = ChatOpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\", temperature=0, streaming=True)\n",
    "\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c628279d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '',\n",
      "  'value': {'final_output': None,\n",
      "            'id': '3afdb436-9e62-4f0a-8aae-2701faa28688',\n",
      "            'logs': {},\n",
      "            'name': 'AgentExecutor',\n",
      "            'streamed_output': [],\n",
      "            'type': 'chain'}})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': '52d65380-f06f-4fb5-8ae1-f55d211ed21b',\n",
      "            'metadata': {},\n",
      "            'name': 'ChatOpenAI',\n",
      "            'start_time': '2024-04-16T15:58:36.123+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['seq:step:3'],\n",
      "            'type': 'llm'}})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'The'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='The', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' current'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' current', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' temperature'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' temperature', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' in'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' in', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' San'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' San', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' Francisco'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' Francisco', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' is'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' is', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' '},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' ', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '6'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='6', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '2'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='2', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' degrees'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' degrees', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' F'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' F', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ahren'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ahren', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'heit'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='heit', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '.'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='.', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' The'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' The', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' hum'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' hum', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'id'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='id', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': 'ity'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ity', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' is'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' is', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' '},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' ', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '5'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='5', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '0'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='0', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '%,'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='%,', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' and'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' and', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' the'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' the', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' wind'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' wind', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' speed'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' speed', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' is'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' is', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' '},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' ', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '1'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='1', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '0'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='0', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' m'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' m', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 'ph'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='ph', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '.'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='.', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' It'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' It', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': \"'\"},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=\"'\", id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': 's'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='s', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ' a'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' a', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' beautiful'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' beautiful', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output_str/-',\n",
      "  'value': ' day'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content=' day', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '!'},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='!', id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ''},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/streamed_output/-',\n",
      "  'value': AIMessageChunk(content='', response_metadata={'finish_reason': 'stop'}, id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b')})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/final_output',\n",
      "  'value': {'generations': [[{'generation_info': {'finish_reason': 'stop'},\n",
      "                              'message': AIMessageChunk(content=\"The current temperature in San Francisco is 62 degrees Fahrenheit. The humidity is 50%, and the wind speed is 10 mph. It's a beautiful day!\", response_metadata={'finish_reason': 'stop'}, id='run-52d65380-f06f-4fb5-8ae1-f55d211ed21b'),\n",
      "                              'text': 'The current temperature in San '\n",
      "                                      'Francisco is 62 degrees Fahrenheit. The '\n",
      "                                      'humidity is 50%, and the wind speed is '\n",
      "                                      \"10 mph. It's a beautiful day!\",\n",
      "                              'type': 'ChatGenerationChunk'}]],\n",
      "            'llm_output': None,\n",
      "            'run': None}},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatOpenAI/end_time',\n",
      "  'value': '2024-04-16T15:58:38.838+00:00'})\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/streamed_output/-',\n",
      "  'value': {'messages': [AIMessage(content=\"The current temperature in San Francisco is 62 degrees Fahrenheit. The humidity is 50%, and the wind speed is 10 mph. It's a beautiful day!\")],\n",
      "            'output': 'The current temperature in San Francisco is 62 degrees '\n",
      "                      'Fahrenheit. The humidity is 50%, and the wind speed is '\n",
      "                      \"10 mph. It's a beautiful day!\"}},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': {'messages': [AIMessage(content=\"The current temperature in San Francisco is 62 degrees Fahrenheit. The humidity is 50%, and the wind speed is 10 mph. It's a beautiful day!\")],\n",
      "            'output': 'The current temperature in San Francisco is 62 degrees '\n",
      "                      'Fahrenheit. The humidity is 50%, and the wind speed is '\n",
      "                      \"10 mph. It's a beautiful day!\"}})\n"
     ]
    }
   ],
   "source": [
    "async for chunk in agent_executor.astream_log(\n",
    "    {\"input\": \"what is the weather in sf\", \"chat_history\": []},\n",
    "    include_names=[\"ChatOpenAI\"],\n",
    "):\n",
    "    print(chunk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
