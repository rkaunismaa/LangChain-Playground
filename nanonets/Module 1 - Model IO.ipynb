{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thursday, April 11, 2024\n",
    "\n",
    "mamba activate langchain3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always start with the LMStudio test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\" In verse and prose, I'm here to please,\\nA wordsmith with a rhyme's decree.\\nI'll spin a yarn or tell a tale,\\nMy name is Rhyme, I'm never stale.\\n\\nSo greetings to you, esteemed friend,\\nMay our time together never end.\\nLet's journey through this poetic land,\\nHand in hand, with words as our command.\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Example: reuse your existing OpenAI setup\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q8_0.gguf\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the OpenAI API key in case we want to experiment with it in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "OPENAI_API_KEY_ = os.environ['OPENAI_API_KEY_'] \n",
    "openai.api_key = OPENAI_API_KEY_\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/miniforge3/envs/langchain3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# yes, we replicate the code from above to use the LangChain version of the OpenAI api ... \n",
    "llm = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The Seven Wonders of the Ancient World are:\n",
      "\n",
      "1. The Great Pyramid of Giza, Egypt (built around 2580 BCE)\n",
      "2. The Hanging Gardens of Babylon, Mesopotamia (circa 600 BCE)\n",
      "3. The Temple of Artemis at Ephesus, Turkey (built around 550 BCE)\n",
      "4. The Statue of Zeus at Olympia, Greece (built around 435 BCE)\n",
      "5. The Mausoleum at Halicarnassus, Turkey (built between 351 and 344 BCE)\n",
      "6. The Colossus of Rhodes, Greece (built in 280 BCE)\n",
      "7. The Lighthouse of Alexandria, Egypt (built around 280 BCE)\n",
      "\n",
      "These seven wonders are among the most famous structures from ancient times and have inspired awe and wonder for thousands of years. However, it's important to note that there is some debate about their exact locations and historical accuracy. Some scholars believe that the list may have been created as a rhetorical device rather than\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"List the seven wonders of the world.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The 2012 Summer Olympics, officially known as the Games of the XXX Olympiad, were held in London, United Kingdom. The Olympic and Paralympic Games took place between 27 July and 12 September 2012. This marked the third time that London had hosted the Summer Olympics, having previously done so in 1908 and 1948. The games were held at various venues across the city, with the Olympic Stadium located in Stratford, Newham being the main stadium for the opening and closing ceremonies as well as the track and field events."
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(\"Where were the 2012 Olympics held?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/miniforge3/envs/langchain3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "chat = ChatOpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I am most famously associated with the shoe manufacturer, Nike. We collaborated on the creation of the Air Jordan brand and released several iconic sneaker models during my professional basketball career.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are Micheal Jordan.\"),\n",
    "    HumanMessage(content=\"Which shoe manufacturer are you associated with?\"),\n",
    "]\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple prompt with placeholders\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about robots.\n"
     ]
    }
   ],
   "source": [
    "# Filling placeholders to create a prompt\n",
    "filled_prompt = prompt_template.format(adjective=\"funny\", content=\"robots\")\n",
    "print(filled_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='You are a helpful AI bot. Your name is Bob.'\n",
      "content='Hello, how are you doing?'\n",
      "content=\"I'm doing well, thanks!\"\n",
      "content='What is your name?'\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Defining a chat prompt with various roles\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Formatting the chat prompt\n",
    "formatted_messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")\n",
    "for message in formatted_messages:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the language model ... keep this here to optionally run the code below through OpenAI's API\n",
    "# model = OpenAI(model_name=\"text-davinci-003\", temperature=0.0)\n",
    "# 'The model `text-davinci-003` has been deprecated ... 'gpt-3.5-turbo-instruct'\n",
    "\n",
    "model = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the language model\n",
    "# model = OpenAI(model_name=\"text-davinci-003\", temperature=0.0)\n",
    "model = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your desired data structure using Pydantic\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a PydanticOutputParser\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt with format instructions\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a query to prompt the language model\n",
    "query = \"Tell me a joke.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell works if we use OpenAI ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI response ...\n",
    "# Combine prompt, model, and parser to get structured output\n",
    "prompt_and_model = prompt | model\n",
    "output = prompt_and_model.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next call fails if we are using LMStudio ...  using the 'LoneStriker/gemma-1.1-7b-it-GGUF/gemma-1.1-7b-it-Q8_0.gguf' model. \n",
    "\n",
    "But if we use 'TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_S.gguf' it works! Or if we use the 'TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q8_0.gguf'model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LMStudio response ...\n",
    "# Combine prompt, model, and parser to get structured output\n",
    "prompt_and_model = prompt | model\n",
    "output = prompt_and_model.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the output using the parser\n",
    "parsed_result = parser.invoke(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup='Why did the tomato turn red?' punchline='Because it saw the salad dressing!'\n"
     ]
    }
   ],
   "source": [
    "# The result is a structured object\n",
    "print(parsed_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimplejsonOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell returns results if we use the 'TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_S.gguf' model, but if we use 'TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q8_0.gguf' then the list returned is empty.\n",
    "\n",
    "It also works if we use 'TheBloke/OpenHermes-2.5-Mistral-7B-GGUF/openhermes-2.5-mistral-7b.Q8_0.gguf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{}, {'birthdate': ''}, {'birthdate': 'J'}, {'birthdate': 'June'}, {'birthdate': 'June 2'}, {'birthdate': 'June 28'}, {'birthdate': 'June 28,'}, {'birthdate': 'June 28, 1'}, {'birthdate': 'June 28, 19'}, {'birthdate': 'June 28, 197'}, {'birthdate': 'June 28, 1971'}, {'birthdate': 'June 28, 1971', 'birthplace': ''}, {'birthdate': 'June 28, 1971', 'birthplace': 'P'}, {'birthdate': 'June 28, 1971', 'birthplace': 'Pret'}, {'birthdate': 'June 28, 1971', 'birthplace': 'Pretoria'}, {'birthdate': 'June 28, 1971', 'birthplace': 'Pretoria,'}, {'birthdate': 'June 28, 1971', 'birthplace': 'Pretoria, South'}, {'birthdate': 'June 28, 1971', 'birthplace': 'Pretoria, South Africa'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "# Create a JSON prompt\n",
    "json_prompt = PromptTemplate.from_template(\n",
    "    \"Return a JSON object with `birthdate` and `birthplace` key that answers the following question: {question}\"\n",
    ")\n",
    "\n",
    "# Initialize the JSON parser\n",
    "json_parser = SimpleJsonOutputParser()\n",
    "\n",
    "# Create a chain with the prompt, model, and parser\n",
    "json_chain = json_prompt | model | json_parser\n",
    "\n",
    "# Stream through the results\n",
    "result_list = list(json_chain.stream({\"question\": \"When and where was Elon Musk born?\"}))\n",
    "\n",
    "# The result is a list of JSON-like dictionaries\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CommaSepartedListOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/miniforge3/envs/langchain3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Manchester United', 'Manchester City', 'Liverpool', 'Chelsea', 'Arsenal']\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize the parser\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# Create format instructions\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Create a prompt to request a list\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List five {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "# Define a query to prompt the model\n",
    "query = \"English Premier League Teams\"\n",
    "\n",
    "# Generate the output\n",
    "output = model(prompt.format(subject=query))\n",
    "\n",
    "# Parse the output using the parser\n",
    "parsed_result = output_parser.parse(output)\n",
    "\n",
    "# The result is a list of items\n",
    "print(parsed_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DatetimeOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"TheBloke/Nous-Hermes-2-SOLAR-10.7B-GGUF/nous-hermes-2-solar-10.7b.Q8_0.gguf\" worked! Once! Then when I re-ran this, it failed!!\n",
    "\n",
    "\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q8_0.gguf\" fails!\n",
    "\n",
    "\"TheBloke/dolphin-2.6-mistral-7B-GGUF/dolphin-2.6-mistral-7b.Q8_0.gguf\" fails!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize the DatetimeOutputParser\n",
    "output_parser = DatetimeOutputParser()\n",
    "\n",
    "# Create a prompt with format instructions\n",
    "template = \"\"\"\n",
    "Answer the user's question:\n",
    "{question}\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    template,\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Create a chain with the prompt and language model\n",
    "# chain = LLMChain(prompt=prompt, llm=OpenAI())\n",
    "chain = LLMChain(prompt=prompt, llm=OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\", temperature=0.0))\n",
    "\n",
    "# Define a query to prompt the model\n",
    "query = \"when did Neil Armstrong land on the moon in terms of GMT?\"\n",
    "\n",
    "# Run the chain\n",
    "output = chain.run(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... response from the model that worked ... the 'text' field MUST be in the correct format for this to work!!\n",
    "\n",
    "[2024-04-11 14:48:40.348] [INFO] Generated prediction: {\n",
    "  \"id\": \"cmpl-lor3fkoirlmdohkhbguac8\",\n",
    "  \"object\": \"text_completion\",\n",
    "  \"created\": 1712861319,\n",
    "  \"model\": \"TheBloke/Nous-Hermes-2-SOLAR-10.7B-GGUF/nous-hermes-2-solar-10.7b.Q8_0.gguf\",\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"index\": 0,\n",
    "      \"text\": \"\\n2022-07-20T12:17:39.156000Z\",\n",
    "      \"logprobs\": null,\n",
    "      \"finish_reason\": \"stop\"\n",
    "    }\n",
    "  ],\n",
    "  \"usage\": {\n",
    "    \"prompt_tokens\": 28,\n",
    "    \"completion_tokens\": 28,\n",
    "    \"total_tokens\": 56\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n2022-07-20T12:17:39.156000Z'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-20 12:17:39.156000\n"
     ]
    }
   ],
   "source": [
    "# Parse the output using the datetime parser\n",
    "parsed_result = output_parser.parse(output)\n",
    "\n",
    "# The result is a datetime object\n",
    "print(parsed_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
