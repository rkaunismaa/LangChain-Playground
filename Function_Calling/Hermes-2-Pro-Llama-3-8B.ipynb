{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saturday, May 4, 2024\n",
    "\n",
    "mamba activate langchain3\n",
    "\n",
    "This notebook will explore the function calling capabilities of [NousResearch/Hermes-2-Pro-Llama-3-8B](https://huggingface.co/NousResearch/Hermes-2-Pro-Llama-3-8B)\n",
    "\n",
    "This simple code sample will not run unless we specify to only target the 4090 ...\n",
    "\n",
    "This all runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Code to inference Hermes with HF Transformers\n",
    "# Requires pytorch, transformers, bitsandbytes, sentencepiece, protobuf, and flash-attn packages\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM\n",
    "import bitsandbytes, flash_attn\n",
    "\n",
    "# mamba install conda-forge::bitsandbytes\n",
    "# Could not find the bitsandbytes CUDA binary at PosixPath('/home/rob/miniforge3/envs/langchain3/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda118.so')\n",
    "# The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May  4 17:45:21 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1050        Off | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   39C    P0              N/A /  70W |    434MiB /  2048MiB |      5%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off | 00000000:02:00.0 Off |                  Off |\n",
      "|  0%   31C    P8               8W / 450W |     16MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      5370      G   /usr/lib/xorg/Xorg                          161MiB |\n",
      "|    0   N/A  N/A      5639      G   /usr/bin/gnome-shell                        158MiB |\n",
      "|    0   N/A  N/A      7203      G   ...erProcess --variations-seed-version      110MiB |\n",
      "|    1   N/A  N/A      5370      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model_name = \"NousResearch/Hermes-2-Pro-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b1f05c5f4b4d878beaf603122b9292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Original code had ... load_in_8bit=False, load_in_4bit=True ... \n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=False,\n",
    "    load_in_4bit=False,\n",
    "    use_flash_attention_2=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128003 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\n",
      "<|im_start|>user\n",
      "Write a short story about Goku discovering kirby has teamed up with Majin Buu to destroy the world.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Response: \n",
      "In the vast universe of planets and galaxies, Goku, the legendary Saiyan warrior, was known for his heroic deeds and epic battles against some of the strongest fighters in existence. One day, Goku received an unusual message from one of his trusted allies, informing him of a shocking development.\n",
      "\n",
      "The message revealed that Kirby, the powerful pink puffball with remarkable abilities, had unexpectedly joined forces with Majin Buu, the shape-shifting evil entity who once posed a great threat to Earth's existence. According to the information, this nefarious duo had embarked on a treacherous plan to destroy the world, putting all of their hard-earned peace at risk.\n",
      "\n",
      "Goku sensed the urgency of the situation and immediately set out to investigate the matter. His first stop was the location where he believed Kirby resided -- Popstar, a whimsical planet teeming with magical creatures and strange phenomena. Upon arriving, he found signs of Kirby's presence but no sign of any hostile activity.\n",
      "\n",
      "Goku decided to follow the trail left by Kirby, which led him across various worlds and dimensions, each more surreal than the last. As he journeyed, he encountered numerous allies and foes alike â€“ powerful beings whose help he sought to unravel the mystery surrounding Kirby and Majin Buu's alliance.\n",
      "\n",
      "One fateful encounter brought Goku face-to-face with King Dedede, the tyrannical ruler of Dream Land on Popstar. Surprisingly, King Dedede informed Goku that Kirby had indeed joined forces with Majin Buu, but not for sinister purposes. It turned out that Majin Buu had been brainwashed by an unknown entity, and Kirby was attempting to save the villain without anyone noticing.\n",
      "\n",
      "Goku realized that he needed to confront Majin Buu directly and free it from the control of the mysterious force. With the assistance of King Dedede and other unlikely allies, they managed to track down the source of the mind control - a hidden artifact imbued with dark energy.\n",
      "\n",
      "As the final battle unfolded, Goku, Kirby, and their friends faced off against countless minions manipulated by the malicious artifact. Through teamwork, perseverance, and sheer determination, they successfully destroyed the artifact, liberating Majin Buu from its control.\n",
      "\n",
      "Kirby and Majin Buu thanked Goku and his companions for their assistance, promising to make amends for their misguided actions. In the end, the bonds of friendship and trust forged during this incredible adventure transcended boundaries, leading to a harmonious interstellar community where even seemingly unbeatable foes could find redemption. And so, another tale of courage, compassion, and unity cemented itself within the annals of cosmic history.\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"\"\"<|im_start|>system\n",
    "You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\n",
    "<|im_start|>user\n",
    "Write a short story about Goku discovering kirby has teamed up with Majin Buu to destroy the world.<|im_end|>\n",
    "<|im_start|>assistant\"\"\",\n",
    "    ]\n",
    "\n",
    "for chat in prompts:\n",
    "    print(chat)\n",
    "    input_ids = tokenizer(chat, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    generated_ids = model.generate(input_ids, max_new_tokens=750, temperature=0.8, repetition_penalty=1.1, do_sample=True, eos_token_id=tokenizer.eos_token_id)\n",
    "    response = tokenizer.decode(generated_ids[0][input_ids.shape[-1]:], skip_special_tokens=True, clean_up_tokenization_space=True)\n",
    "    print(f\"Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May  4 17:46:09 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1050        Off | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   40C    P0              N/A /  70W |    457MiB /  2048MiB |      5%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off | 00000000:02:00.0 Off |                  Off |\n",
      "| 31%   34C    P2              52W / 450W |  16248MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      5370      G   /usr/lib/xorg/Xorg                          161MiB |\n",
      "|    0   N/A  N/A      5639      G   /usr/bin/gnome-shell                        158MiB |\n",
      "|    0   N/A  N/A      7203      G   ...erProcess --variations-seed-version      134MiB |\n",
      "|    1   N/A  N/A      5370      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A      8135      C   ...niforge3/envs/langchain3/bin/python    16228MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_templates\t functions.py  prompt_assets  requirements.txt\tvalidator.py\n",
      "examples\t jsonmode.py   prompter.py    schema.py\n",
      "functioncall.py  LICENSE       README.md      utils.py\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rob/Data/Documents/Github/NousResearch/Hermes-Function-Calling/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hermes-Function-Calling\n",
    "\n",
    "This repository contains code for the Hermes Pro Large Language Model to perform function calling based on the provided schema. It allows users to query the model and retrieve information related to stock prices, company fundamentals, financial statements, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_templates\t functions.py  prompt_assets  requirements.txt\tvalidator.py\n",
      "examples\t jsonmode.py   prompter.py    schema.py\n",
      "functioncall.py  LICENSE       README.md      utils.py\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rob/Data/Documents/Github/NousResearch/Hermes-Function-Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the path to the repo ...\n",
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '/home/rob/Data/Documents/Github/NousResearch/Hermes-Function-Calling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/home/rob/Data/Documents/Github/rkaunismaa/LangChain-Playground/Function_Calling/functioncall.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python functioncall.py --query \"I need the current stock price of Tesla (TSLA)\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
