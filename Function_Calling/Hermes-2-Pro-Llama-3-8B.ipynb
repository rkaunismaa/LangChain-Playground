{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saturday, May 4, 2024\n",
    "\n",
    "mamba activate langchain3\n",
    "\n",
    "This notebook will explore the function calling capabilities of [NousResearch/Hermes-2-Pro-Llama-3-8B](https://huggingface.co/NousResearch/Hermes-2-Pro-Llama-3-8B)\n",
    "\n",
    "This simple code sample will not run unless we specify to only target the 4090 ...\n",
    "\n",
    "This all runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Code to inference Hermes with HF Transformers\n",
    "# Requires pytorch, transformers, bitsandbytes, sentencepiece, protobuf, and flash-attn packages\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM\n",
    "import bitsandbytes, flash_attn\n",
    "\n",
    "# mamba install conda-forge::bitsandbytes\n",
    "# Could not find the bitsandbytes CUDA binary at PosixPath('/home/rob/miniforge3/envs/langchain3/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda118.so')\n",
    "# The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May  4 15:48:23 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1050        Off | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   57C    P0              N/A /  70W |    177MiB /  2048MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off | 00000000:02:00.0 Off |                  Off |\n",
      "|  0%   39C    P5              26W / 450W |     16MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      5441      G   /usr/lib/xorg/Xorg                          152MiB |\n",
      "|    0   N/A  N/A      5706      G   /usr/bin/gnome-shell                         22MiB |\n",
      "|    1   N/A  N/A      5441      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model_name = \"NousResearch/Hermes-2-Pro-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b426f1fbe04691ae10e6ed6ab8b0dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Original code had ... load_in_8bit=False, load_in_4bit=True ... \n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=False,\n",
    "    load_in_4bit=False,\n",
    "    use_flash_attention_2=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128003 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\n",
      "<|im_start|>user\n",
      "Write a short story about Goku discovering kirby has teamed up with Majin Buu to destroy the world.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Response: \n",
      "As the sun rose over the horizon, casting its golden light upon the Earth's surface, Goku awoke in his bed. The legendary Saiyan warrior had spent years training and competing in battles across the galaxy, honing his skills as one of the universe's most powerful fighters. His life was filled with adventures and friendships forged through intense combat.\n",
      "\n",
      "One day, while on a peaceful stroll through a lush forest, Goku sensed a strange energy nearby. He couldn't believe what he was sensing - it was a unique power that he had never encountered before. Unable to resist the urge to find out more, Goku followed the signature aura until he came face-to-face with an unlikely duo: Kirby, a small pink ball of fluff, and Majin Buu, the plump, mischievous being he'd fought alongside against many villains.\n",
      "\n",
      "Kirby greeted Goku with a cheerful \"Hiya!\" as Majin Buu grinned widely, exposing his large buckteeth. But something wasn't right. These two powerful beings were not acting like friends; rather, their energies spoke of sinister intentions.\n",
      "\n",
      "\"Goku, I see you've met my new buddy, Kirby!\" exclaimed Majin Buu, attempting to sound innocent and lighthearted. However, Goku could sense the dark energy emanating from both creatures.\n",
      "\n",
      "\"What is this, Buu? Why have you teamed up with Kirby?\" Goku asked cautiously. He tried to maintain a calm demeanor, but his instincts told him that these two meant harm to the planet he called home.\n",
      "\n",
      "Majin Buu hesitated for a moment but then replied, \"We want to create something big, something no one has ever seen before! We're going to destroy this world together.\"\n",
      "\n",
      "Goku frowned at the thought of destroying the planet he cherished, and his eyes locked onto Kirby. This tiny creature was much more cunning than he appeared, as he had managed to convince Majin Buu to join forces with him for such a nefarious scheme.\n",
      "\n",
      "With determination in his heart, Goku knew he had to put an end to their wicked plan. He summoned all his strength and launched himself into action, his ki intensifying around him as he charged headfirst at the villainous pair.\n",
      "\n",
      "A fierce battle ensued, with each fighter pushing themselves to the limit. While Kirby displayed an array of incredible powers, including copying abilities that allowed him to harness some of Goku's own techniques, the indomitable Saiyan proved to be too formidable an opponent.\n",
      "\n",
      "Finally, after an epic struggle, Goku managed to land a devastating blow on Kirby, causing the villain to reveal his true form. In his core was the essence of pure evil – the spirit of Destroyah, a malevolent force that thrived on chaos and destruction.\n",
      "\n",
      "With the truth exposed, Goku rallied all his might and unleashed the ultimate attack: the Spirit Bomb, a massive energy sphere fueled by the collective energies of life on Earth.\n",
      "\n",
      "The Spirit Bomb crashed down upon Kirby-Destroyah, obliterating his vile presence forever. As for Majin Buu, he was left in a state of shock and remorse, realizing the error of his ways and the darkness that had clouded his judgment.\n",
      "\n",
      "Together, Goku and Majin Buu returned to their former selves, vowing to protect the Earth and restore their bonds of friendship. Though they would always remember the dangerous path they once tread, they emerged stronger and more united than ever before, knowing that their love for the planet and its inhabitants far surpassed any enemy they might face.\n",
      "\n",
      "And so, the tale of Goku, Kirby, and Majin Buu serves as a reminder that even when faced with insurmountable odds or deception, true heroes will rise above, embracing the values of\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"\"\"<|im_start|>system\n",
    "You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\n",
    "<|im_start|>user\n",
    "Write a short story about Goku discovering kirby has teamed up with Majin Buu to destroy the world.<|im_end|>\n",
    "<|im_start|>assistant\"\"\",\n",
    "    ]\n",
    "\n",
    "for chat in prompts:\n",
    "    print(chat)\n",
    "    input_ids = tokenizer(chat, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    generated_ids = model.generate(input_ids, max_new_tokens=750, temperature=0.8, repetition_penalty=1.1, do_sample=True, eos_token_id=tokenizer.eos_token_id)\n",
    "    response = tokenizer.decode(generated_ids[0][input_ids.shape[-1]:], skip_special_tokens=True, clean_up_tokenization_space=True)\n",
    "    print(f\"Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May  4 15:48:47 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1050        Off | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   58C    P0              N/A /  70W |    177MiB /  2048MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off | 00000000:02:00.0 Off |                  Off |\n",
      "| 32%   44C    P2             155W / 450W |  16308MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      5441      G   /usr/lib/xorg/Xorg                          152MiB |\n",
      "|    0   N/A  N/A      5706      G   /usr/bin/gnome-shell                         22MiB |\n",
      "|    1   N/A  N/A      5441      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A     60362      C   ...niforge3/envs/langchain3/bin/python    16288MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_templates\t functions.py  prompt_assets  requirements.txt\tvalidator.py\n",
      "examples\t jsonmode.py   prompter.py    schema.py\n",
      "functioncall.py  LICENSE       README.md      utils.py\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rob/Data/Documents/Github/NousResearch/Hermes-Function-Calling/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hermes-Function-Calling\n",
    "\n",
    "This repository contains code for the Hermes Pro Large Language Model to perform function calling based on the provided schema. It allows users to query the model and retrieve information related to stock prices, company fundamentals, financial statements, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_templates\t functions.py  prompt_assets  requirements.txt\tvalidator.py\n",
      "examples\t jsonmode.py   prompter.py    schema.py\n",
      "functioncall.py  LICENSE       README.md      utils.py\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rob/Data/Documents/Github/NousResearch/Hermes-Function-Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the path to the repo ...\n",
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '/home/rob/Data/Documents/Github/NousResearch/Hermes-Function-Calling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/home/rob/Data/Documents/Github/rkaunismaa/LangChain-Playground/Function_Calling/functioncall.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python functioncall.py --query \"I need the current stock price of Tesla (TSLA)\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
