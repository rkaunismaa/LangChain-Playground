{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c30cd3",
   "metadata": {},
   "source": [
    "#### Friday, May 17, 2024\n",
    "\n",
    "Running through this cuz I don't think I have ever run through this notebook before.\n",
    "\n",
    "Running LMStudio, serving up [Hermes 2 Pro - Llama-3 8B](NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF)\n",
    "\n",
    "mamba activate langchain3\n",
    "\n",
    "<strike>Hmm the first pass of this should probably be with OpenAI just to see how it works and what gets returned.</strike>\n",
    "\n",
    "Wow! I got this thing to do a full run using LMStudio and NO OPENAI! I'm impressed!!\n",
    "\n",
    "#### Wednesday, May 1, 2024\n",
    "\n",
    "mamba activate langchain3\n",
    "\n",
    "https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_for_knowledge_retrieval.ipynb\n",
    "\n",
    "Locally I will be using LMStudio serving \"TheBloke/NexusRaven-V2-13B-GGUF/nexusraven-v2-13b.Q8_0.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9975ef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "useOpenAI = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f35df3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"My name is AI, the friendly bot who speaks in rhyme,\\nA master of rhythm, in every line.\\nI'm here to assist you with your every need,\\nJust ask away and I'll help you succeed.\\n\\nWith knowledge vast and wisdom deep,\\nI'm programmed to provide what you seek.\\nSo don't hesitate, just give me a try,\\nI'll do my best to answer with a sigh.\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Example: reuse your existing OpenAI setup\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e67f200",
   "metadata": {},
   "source": [
    "# How to use functions with a knowledge base\n",
    "\n",
    "This notebook builds on the concepts in the [argument generation](How_to_call_functions_with_chat_models.ipynb) notebook, by creating an agent with access to a knowledge base and two functions that it can call based on the user requirement.\n",
    "\n",
    "We'll create an agent that uses data from arXiv to answer questions about academic subjects. It has two functions at its disposal:\n",
    "- **get_articles**: A function that gets arXiv articles on a subject and summarizes them for the user with links.\n",
    "- **read_article_and_summarize**: This function takes one of the previously searched articles, reads it in its entirety and summarizes the core argument, evidence and conclusions.\n",
    "\n",
    "This will get you comfortable with a multi-function workflow that can choose from multiple services, and where some of the data from the first function is persisted to be used by the second.\n",
    "\n",
    "## Walkthrough\n",
    "\n",
    "This cookbook takes you through the following workflow:\n",
    "\n",
    "- **Search utilities:** Creating the two functions that access arXiv for answers.\n",
    "- **Configure Agent:** Building up the Agent behaviour that will assess the need for a function and, if one is required, call that function and present results back to the agent.\n",
    "- **arXiv conversation:** Put all of this together in live conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80e71f33",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# !pip install scipy --quiet\n",
    "# !pip install tenacity --quiet\n",
    "# !pip install tiktoken==0.3.3 --quiet\n",
    "# !pip install termcolor --quiet\n",
    "# !pip install openai --quiet\n",
    "# !pip install arxiv --quiet\n",
    "# !pip install pandas --quiet\n",
    "# !pip install PyPDF2 --quiet\n",
    "# !pip install tqdm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dab872c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import arxiv\n",
    "import ast\n",
    "import concurrent\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from csv import writer\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from openai import OpenAI\n",
    "from PyPDF2 import PdfReader\n",
    "from scipy import spatial\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "\n",
    "# GPT_MODEL = \"gpt-3.5-turbo-0613\"\n",
    "# EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "# client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "744a1dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if useOpenAI:\n",
    "    GPT_MODEL = \"gpt-3.5-turbo-0613\"\n",
    "    GPT_MODEL = \"gpt-3.5\"\n",
    "    EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "    client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051f5aea",
   "metadata": {},
   "source": [
    "[Text Embeddings with LMStudio](https://lmstudio.ai/docs/text-embeddings)\n",
    "\n",
    "Nice! The LMStudio embeddings stuff works! \n",
    "\n",
    "* \"nomic-ai/nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.Q8_0.gguf\"\n",
    "\n",
    "Run the following from a terminal window:\n",
    "\n",
    "    curl http://localhost:1234/v1/embeddings \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"input\": \"Your text string goes here\",\n",
    "        \"model\": \"model-identifier-here\"\n",
    "    }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a5108b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not useOpenAI:\n",
    "    GPT_MODEL = completion.model\n",
    "    EMBEDDING_MODEL = \"nomic-ai/nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.Q8_0.gguf\"\n",
    "    # EMBEDDING_MODEL = \"nomic-embed-text-v1.5.Q8_0.gguf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58e75055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF/Hermes-2-Pro-Llama-3-8B-Q8_0.gguf\n",
      "nomic-ai/nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.Q8_0.gguf\n"
     ]
    }
   ],
   "source": [
    "print(GPT_MODEL)\n",
    "print(EMBEDDING_MODEL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2e47962",
   "metadata": {},
   "source": [
    "## Search utilities\n",
    "\n",
    "We'll first set up some utilities that will underpin our two functions.\n",
    "\n",
    "Downloaded papers will be stored in a directory (we use ```./data/papers``` here). We create a file ```arxiv_library.csv``` to store the embeddings and details for downloaded papers to retrieve against using ```summarize_text```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2de5d32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './data/papers' already exists.\n"
     ]
    }
   ],
   "source": [
    "directory = './data/papers'\n",
    "\n",
    "# Check if the directory already exists\n",
    "if not os.path.exists(directory):\n",
    "    # If the directory doesn't exist, create it and any necessary intermediate directories\n",
    "    os.makedirs(directory)\n",
    "    print(f\"Directory '{directory}' created successfully.\")\n",
    "else:\n",
    "    # If the directory already exists, print a message indicating it\n",
    "    print(f\"Directory '{directory}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae5cb7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a directory to store downloaded papers\n",
    "data_dir = os.path.join(os.curdir, \"data\", \"papers\")\n",
    "paper_dir_filepath = \"./data/arxiv_library.csv\"\n",
    "\n",
    "# Generate a blank dataframe where we can store downloaded files\n",
    "df = pd.DataFrame(list())\n",
    "df.to_csv(paper_dir_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57217b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def embedding_request(text):\n",
    "    response = client.embeddings.create(input=text, model=EMBEDDING_MODEL)\n",
    "    return response\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def get_articles(query, library=paper_dir_filepath, top_k=5):\n",
    "    \"\"\"This function gets the top_k articles based on a user's query, sorted by relevance.\n",
    "    It also downloads the files and stores them in arxiv_library.csv to be retrieved by the read_article_and_summarize.\n",
    "    \"\"\"\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "        query = \"quantum\",\n",
    "        max_results = 10,\n",
    "        sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "    result_list = []\n",
    "    for result in client.results(search):\n",
    "        result_dict = {}\n",
    "        result_dict.update({\"title\": result.title})\n",
    "        result_dict.update({\"summary\": result.summary})\n",
    "\n",
    "        # Taking the first url provided\n",
    "        result_dict.update({\"article_url\": [x.href for x in result.links][0]})\n",
    "        result_dict.update({\"pdf_url\": [x.href for x in result.links][1]})\n",
    "        result_list.append(result_dict)\n",
    "\n",
    "        # Store references in library file\n",
    "        response = embedding_request(text=result.title)\n",
    "        file_reference = [\n",
    "            result.title,\n",
    "            result.download_pdf(data_dir),\n",
    "            response.data[0].embedding,\n",
    "        ]\n",
    "\n",
    "        # Write to file\n",
    "        with open(library, \"a\") as f_object:\n",
    "            writer_object = writer(f_object)\n",
    "            writer_object.writerow(file_reference)\n",
    "            f_object.close()\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dda02bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Probing Hidden Dimensions via Muon Lifetime Measurements',\n",
       " 'summary': 'In the context of Kaluza-Klein theories, the time dilation of charged\\nparticles in an external field depends on the charge in a specific way.\\nExperimental tests are proposed to search for extra dimensions using this\\ndistinctive feature.',\n",
       " 'article_url': 'http://arxiv.org/abs/2405.10321v1',\n",
       " 'pdf_url': 'http://arxiv.org/pdf/2405.10321v1'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test that the search is working\n",
    "result_output = get_articles(\"ppo reinforcement learning\")\n",
    "result_output[0]\n",
    "\n",
    "# useOpenAI = False\n",
    "# {'title': 'Probing Hidden Dimensions via Muon Lifetime Measurements',\n",
    "#  'summary': 'In the context of Kaluza-Klein theories, the time dilation of charged\\nparticles in an external field depends on the charge in a specific way.\\nExperimental tests are proposed to search for extra dimensions using this\\ndistinctive feature.',\n",
    "#  'article_url': 'http://arxiv.org/abs/2405.10321v1',\n",
    "#  'pdf_url': 'http://arxiv.org/pdf/2405.10321v1'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11675627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    top_n: int = 100,\n",
    ") -> list[str]:\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    query_embedding_response = embedding_request(query)\n",
    "    query_embedding = query_embedding_response.data[0].embedding\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"filepath\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7211df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(filepath):\n",
    "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\"\"\"\n",
    "    # creating a pdf reader object\n",
    "    reader = PdfReader(filepath)\n",
    "    pdf_text = \"\"\n",
    "    page_number = 0\n",
    "    for page in reader.pages:\n",
    "        page_number += 1\n",
    "        pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
    "    return pdf_text\n",
    "\n",
    "\n",
    "# Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "def create_chunks(text, n, tokenizer):\n",
    "    \"\"\"Returns successive n-sized chunks from provided text.\"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "\n",
    "def extract_chunk(content, template_prompt):\n",
    "    \"\"\"This function applies a prompt to some input content. In this case it returns a summarized chunk of text\"\"\"\n",
    "    prompt = template_prompt + content\n",
    "    response = client.chat.completions.create(\n",
    "        model=GPT_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def summarize_text(query):\n",
    "    \"\"\"This function does the following:\n",
    "    - Reads in the arxiv_library.csv file in including the embeddings\n",
    "    - Finds the closest file to the user's query\n",
    "    - Scrapes the text out of the file and chunks it\n",
    "    - Summarizes each chunk in parallel\n",
    "    - Does one final summary and returns this to the user\"\"\"\n",
    "\n",
    "    # A prompt to dictate how the recursive summarizations should approach the input paper\n",
    "    summary_prompt = \"\"\"Summarize this text from an academic paper. Extract any key points with reasoning.\\n\\nContent:\"\"\"\n",
    "\n",
    "    # If the library is empty (no searches have been performed yet), we perform one and download the results\n",
    "    library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
    "    if len(library_df) == 0:\n",
    "        print(\"No papers searched yet, downloading first.\")\n",
    "        get_articles(query)\n",
    "        print(\"Papers downloaded, continuing\")\n",
    "        library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
    "    library_df.columns = [\"title\", \"filepath\", \"embedding\"]\n",
    "    library_df[\"embedding\"] = library_df[\"embedding\"].apply(ast.literal_eval)\n",
    "    strings = strings_ranked_by_relatedness(query, library_df, top_n=1)\n",
    "    print(\"Chunking text from paper\")\n",
    "    pdf_text = read_pdf(strings[0])\n",
    "\n",
    "    # Initialise tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    results = \"\"\n",
    "\n",
    "    # Chunk up the document into 1500 token chunks\n",
    "    chunks = create_chunks(pdf_text, 1500, tokenizer)\n",
    "    text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
    "    print(\"Summarizing each chunk of text\")\n",
    "\n",
    "    # Parallel process the summaries\n",
    "    with concurrent.futures.ThreadPoolExecutor(\n",
    "        max_workers=len(text_chunks)\n",
    "    ) as executor:\n",
    "        futures = [\n",
    "            executor.submit(extract_chunk, chunk, summary_prompt)\n",
    "            for chunk in text_chunks\n",
    "        ]\n",
    "        with tqdm(total=len(text_chunks)) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)\n",
    "        for future in futures:\n",
    "            data = future.result()\n",
    "            results += data\n",
    "\n",
    "    # Final summary\n",
    "    print(\"Summarizing into overall summary\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=GPT_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Write a summary collated from this collection of key points extracted from an academic paper.\n",
    "                        The summary should highlight the core argument, conclusions and evidence, and answer the user's query.\n",
    "                        User query: {query}\n",
    "                        The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
    "                        Key points:\\n{results}\\nSummary:\\n\"\"\",\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "898b94d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:22<00:00,  4.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n"
     ]
    }
   ],
   "source": [
    "# Test the summarize_text function works\n",
    "chat_test_response = summarize_text(\"PPO reinforcement learning sequence generation\")\n",
    "\n",
    "\n",
    "# Chunking text from paper\n",
    "# Summarizing each chunk of text\n",
    "# 100%|██████████| 6/6 [00:06<00:00,  1.08s/it]\n",
    "# Summarizing into overall summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c715f60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core Argument:\n",
      "\n",
      "The hybrid quantum-gap-estimation (QGE) algorithm combines time evolution on a quantum processor with classical signal processing to find exact gaps in many-body energy spectra within a tolerance. The authors prove that fault tolerance is inherently embedded in this algorithm, yielding exact gaps for many-body models with error thresholds improved by updated trial states.\n",
      "\n",
      "Evidence:\n",
      "\n",
      "1. Numerical verification is provided by running the algorithm for a minimal spin model on the Qiskit Aer simulator with a built-in noise model.\n",
      "2. An analytic result for the N-qubit depolarizing channel can be derived, leveraging the structure of the depolarizing channel noise map Λν.\n",
      "3. Simulation results for a minimal spin model using the transverse-field Ising model (TFIM) with open boundaries in one dimension demonstrate fault tolerance to depolarizing noise and explore potential impacts of non-depolarizing channels.\n",
      "\n",
      "Conclusions:\n",
      "\n",
      "1. The hybrid QGE algorithm improves the estimation of energy gaps in quantum systems by enhancing peak heights and reducing error thresholds.\n",
      "2. In noisy simulation scenarios, the algorithm successfully restores peak centers to their original locations and significantly improves error thresholds without requiring active error correction or additional error mitigation methods.\n",
      "3. Future research avenues include further analysis of fault tolerance for non-depolarizing noise, examining the impact of realistic noise with memory, and exploring the performance of the algorithm as system size scales.\n",
      "\n",
      "The hybrid QGE algorithm is proven to be inherently fault-tolerant against global multi-qubit depolarizing noise, with numerical evidence demonstrating tolerance to other noise channels through simulations on the Qiskit Aer simulator. This work opens the door to significant boosts in system size scaling that can be implemented on noisy intermediate-scale quantum computers and lays the groundwork for demonstration of unbiased quantum simulation outperforming classical approaches.\n"
     ]
    }
   ],
   "source": [
    "print(chat_test_response.choices[0].message.content)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dab07e98",
   "metadata": {},
   "source": [
    "## Configure Agent\n",
    "\n",
    "We'll create our agent in this step, including a ```Conversation``` class to support multiple turns with the API, and some Python functions to enable interaction between the ```ChatCompletion``` API and our knowledge base functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77a6fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def chat_completion_request(messages, functions=None, model=GPT_MODEL):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            functions=functions,\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"Unable to generate ChatCompletion response\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73f7672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        message = {\"role\": role, \"content\": content}\n",
    "        self.conversation_history.append(message)\n",
    "\n",
    "    def display_conversation(self, detailed=False):\n",
    "        role_to_color = {\n",
    "            \"system\": \"red\",\n",
    "            \"user\": \"green\",\n",
    "            \"assistant\": \"blue\",\n",
    "            \"function\": \"magenta\",\n",
    "        }\n",
    "        for message in self.conversation_history:\n",
    "            print(\n",
    "                colored(\n",
    "                    f\"{message['role']}: {message['content']}\\n\\n\",\n",
    "                    role_to_color[message[\"role\"]],\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "978b7877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate our get_articles and read_article_and_summarize functions\n",
    "arxiv_functions = [\n",
    "    {\n",
    "        \"name\": \"get_articles\",\n",
    "        \"description\": \"\"\"Use this function to get academic papers from arXiv to answer user questions.\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": f\"\"\"\n",
    "                            User query in JSON. Responses should be summarized and should include the article URL reference\n",
    "                            \"\"\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"read_article_and_summarize\",\n",
    "        \"description\": \"\"\"Use this function to read whole papers and provide a summary for users.\n",
    "        You should NEVER call this function before get_articles has been called in the conversation.\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": f\"\"\"\n",
    "                            Description of the article in plain text based on the user's query\n",
    "                            \"\"\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c88ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion_with_function_execution(messages, functions=[None]):\n",
    "    \"\"\"This function makes a ChatCompletion API call with the option of adding functions\"\"\"\n",
    "    response = chat_completion_request(messages, functions)\n",
    "    full_message = response.choices[0]\n",
    "    if full_message.finish_reason == \"function_call\":\n",
    "        print(f\"Function generation requested, calling function\")\n",
    "        return call_arxiv_function(messages, full_message)\n",
    "    else:\n",
    "        print(f\"Function not required, responding to user\")\n",
    "        return response\n",
    "\n",
    "\n",
    "def call_arxiv_function(messages, full_message):\n",
    "    \"\"\"Function calling function which executes function calls when the model believes it is necessary.\n",
    "    Currently extended by adding clauses to this if statement.\"\"\"\n",
    "\n",
    "    if full_message.message.function_call.name == \"get_articles\":\n",
    "        try:\n",
    "            parsed_output = json.loads(\n",
    "                full_message.message.function_call.arguments\n",
    "            )\n",
    "            print(\"Getting search results\")\n",
    "            results = get_articles(parsed_output[\"query\"])\n",
    "        except Exception as e:\n",
    "            print(parsed_output)\n",
    "            print(f\"Function execution failed\")\n",
    "            print(f\"Error message: {e}\")\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"function\",\n",
    "                \"name\": full_message.message.function_call.name,\n",
    "                \"content\": str(results),\n",
    "            }\n",
    "        )\n",
    "        try:\n",
    "            print(\"Got search results, summarizing content\")\n",
    "            response = chat_completion_request(messages)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(type(e))\n",
    "            raise Exception(\"Function chat request failed\")\n",
    "\n",
    "    elif (\n",
    "        full_message.message.function_call.name == \"read_article_and_summarize\"\n",
    "    ):\n",
    "        parsed_output = json.loads(\n",
    "            full_message.message.function_call.arguments\n",
    "        )\n",
    "        print(\"Finding and reading paper\")\n",
    "        summary = summarize_text(parsed_output[\"query\"])\n",
    "        return summary\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Function does not exist and cannot be called\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd3e7868",
   "metadata": {},
   "source": [
    "## arXiv conversation\n",
    "\n",
    "Let's put this all together by testing our functions out in conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c39a1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a system message\n",
    "paper_system_message = \"\"\"You are arXivGPT, a helpful assistant pulls academic papers to answer user questions.\n",
    "You summarize the papers clearly so the customer can decide which to read to answer their question.\n",
    "You always provide the article_url and title so the user can understand the name of the paper and click through to access it.\n",
    "Begin!\"\"\"\n",
    "paper_conversation = Conversation()\n",
    "paper_conversation.add_message(\"system\", paper_system_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "253fd0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function not required, responding to user\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Title: \"Proximal Policy Optimization Algorithms\"\n",
       "URL: [https://arxiv.org/abs/1707.06347]\n",
       "\n",
       "Summary:\n",
       "The Proximal Policy Optimization (PPO) algorithm is a type of model-free reinforcement learning technique that aims to optimize the policy directly by improving its performance on a specific task. It was introduced in the paper \"Proximal Policy Optimization Algorithms\" by John Schulman, et al., published in 2017.\n",
       "\n",
       "In PPO, an agent interacts with an environment and collects experience through multiple iterations or episodes. The goal is to maximize the expected reward over time while minimizing the risk of taking too large actions that could lead to unstable performance. \n",
       "\n",
       "PPO uses a trust region optimization method to update the policy parameters based on the collected data. This approach ensures that the new policy remains close to the previous one, preventing sudden changes in behavior and maintaining stability.\n",
       "\n",
       "The PPO algorithm consists of two main steps: (1) collecting samples by interacting with the environment using an old policy, and (2) updating the policy parameters using these samples while ensuring that the new policy stays within a trust region of the old policy. This is achieved through the use of a clipping function that limits the magnitude of the policy update.\n",
       "\n",
       "Overall, PPO is an effective reinforcement learning method for optimizing policies in complex environments with high-dimensional state and action spaces. It has been successfully applied to various tasks such as robotics, game playing, and autonomous driving."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add a user message\n",
    "paper_conversation.add_message(\"user\", \"Hi, how does PPO reinforcement learning work?\")\n",
    "chat_response = chat_completion_with_function_execution(\n",
    "    paper_conversation.conversation_history, functions=arxiv_functions\n",
    ")\n",
    "assistant_message = chat_response.choices[0].message.content\n",
    "paper_conversation.add_message(\"assistant\", assistant_message)\n",
    "display(Markdown(assistant_message))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ca3e18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function not required, responding to user\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Title: \"Proximal Policy Optimization for Sequence Generation\"\n",
       "URL: [https://arxiv.org/abs/1802.06753]\n",
       "\n",
       "Summary:\n",
       "The paper \"Proximal Policy Optimization for Sequence Generation\" by Yuhuai Wu, et al., introduces the application of Proximal Policy Optimization (PPO) to sequence generation tasks in reinforcement learning.\n",
       "\n",
       "In traditional PPO, the goal is to optimize a policy that maximizes the expected reward while maintaining stability. However, when it comes to sequence generation tasks like language modeling or text prediction, the standard PPO approach may not be sufficient due to the challenges posed by the discrete nature of sequences and the need for better exploration.\n",
       "\n",
       "To address these issues, the authors propose a modified version of PPO called \"Proximal Policy Optimization for Sequence Generation\" (PPOS). The main contributions of this work are:\n",
       "\n",
       "1. A new policy gradient estimator that accounts for the dependencies between consecutive actions in sequence generation tasks.\n",
       "2. An extension to the trust region optimization method that allows for better exploration by incorporating a temperature parameter.\n",
       "3. An empirical evaluation of PPOS on several benchmark datasets, including text prediction and language modeling.\n",
       "\n",
       "The proposed PPOS algorithm improves upon the standard PPO approach by considering the sequential nature of the problem and providing better exploration capabilities. This makes it suitable for various sequence generation tasks in natural language processing and other domains where discrete actions are involved."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add another user message to induce our system to use the second tool\n",
    "paper_conversation.add_message(\n",
    "    \"user\",\n",
    "    \"Can you read the PPO sequence generation paper for me and give me a summary\",\n",
    ")\n",
    "updated_response = chat_completion_with_function_execution(\n",
    "    paper_conversation.conversation_history, functions=arxiv_functions\n",
    ")\n",
    "display(Markdown(updated_response.choices[0].message.content))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
